# Prior over reward functions

* humans have different preferences
* We may be able to specify a prior over these preferences
* We can train an agent to maximize each of these preferences, once conditioned on one.
* At test time, we can try to infer the human preferences