@misc{abdolmalekiDistributionalViewMultiObjective2020,
  title = {A {{Distributional View}} on {{Multi-Objective Policy Optimization}}},
  author = {Abdolmaleki, Abbas and Huang, Sandy H. and Hasenclever, Leonard and Neunert, Michael and Song, H. Francis and Zambelli, Martina and Martins, Murilo F. and Heess, Nicolas and Hadsell, Raia and Riedmiller, Martin},
  date = {2020-05-15},
  number = {arXiv:2005.07513},
  eprint = {2005.07513},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2005.07513},
  urldate = {2022-11-11},
  abstract = {Many real-world problems require trading off multiple competing objectives. However, these objectives are often in different units and/or scales, which can make it challenging for practitioners to express numerical preferences over objectives in their native units. In this paper we propose a novel algorithm for multi-objective reinforcement learning that enables setting desired preferences for objectives in a scale-invariant way. We propose to learn an action distribution for each objective, and we use supervised learning to fit a parametric policy to a combination of these distributions. We demonstrate the effectiveness of our approach on challenging high-dimensional real and simulated robotics tasks, and show that setting different preferences in our framework allows us to trace out the space of nondominated solutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/ZEA7RHY6/Abdolmaleki et al. - 2020 - A Distributional View on Multi-Objective Policy Op.pdf;/Users/alexandre.piche/Zotero/storage/IXKHC3AL/2005.html}
}

@misc{abdolmalekiDistributionalViewMultiObjective2020a,
  title = {A {{Distributional View}} on {{Multi-Objective Policy Optimization}}},
  author = {Abdolmaleki, Abbas and Huang, Sandy H. and Hasenclever, Leonard and Neunert, Michael and Song, H. Francis and Zambelli, Martina and Martins, Murilo F. and Heess, Nicolas and Hadsell, Raia and Riedmiller, Martin},
  date = {2020-05-15},
  number = {arXiv:2005.07513},
  eprint = {2005.07513},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.07513},
  url = {http://arxiv.org/abs/2005.07513},
  urldate = {2022-12-13},
  abstract = {Many real-world problems require trading off multiple competing objectives. However, these objectives are often in different units and/or scales, which can make it challenging for practitioners to express numerical preferences over objectives in their native units. In this paper we propose a novel algorithm for multi-objective reinforcement learning that enables setting desired preferences for objectives in a scale-invariant way. We propose to learn an action distribution for each objective, and we use supervised learning to fit a parametric policy to a combination of these distributions. We demonstrate the effectiveness of our approach on challenging high-dimensional real and simulated robotics tasks, and show that setting different preferences in our framework allows us to trace out the space of nondominated solutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/WL85Z7YC/Abdolmaleki et al. - 2020 - A Distributional View on Multi-Objective Policy Op.pdf;/Users/alexandre.piche/Zotero/storage/VKN4NFST/2005.html}
}

@misc{abramsonImprovingMultimodalInteractive2022,
  title = {Improving {{Multimodal Interactive Agents}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Abramson, Josh and Ahuja, Arun and Carnevale, Federico and Georgiev, Petko and Goldin, Alex and Hung, Alden and Landon, Jessica and Lhotka, Jirka and Lillicrap, Timothy and Muldal, Alistair and Powell, George and Santoro, Adam and Scully, Guy and Srivastava, Sanjana and von Glehn, Tamara and Wayne, Greg and Wong, Nathaniel and Yan, Chen and Zhu, Rui},
  options = {useprefix=true},
  date = {2022-11-21},
  number = {arXiv:2211.11602},
  eprint = {2211.11602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.11602},
  url = {http://arxiv.org/abs/2211.11602},
  urldate = {2022-12-29},
  abstract = {An important goal in artificial intelligence is to create agents that can both interact naturally with humans and learn from their feedback. Here we demonstrate how to use reinforcement learning from human feedback (RLHF) to improve upon simulated, embodied agents trained to a base level of competency with imitation learning. First, we collected data of humans interacting with agents in a simulated 3D world. We then asked annotators to record moments where they believed that agents either progressed toward or regressed from their human-instructed goal. Using this annotation data we leveraged a novel method - which we call "Inter-temporal Bradley-Terry" (IBT) modelling - to build a reward model that captures human judgments. Agents trained to optimise rewards delivered from IBT reward models improved with respect to all of our metrics, including subsequent human judgment during live interactions with agents. Altogether our results demonstrate how one can successfully leverage human judgments to improve agent behaviour, allowing us to use reinforcement learning in complex, embodied domains without programmatic reward functions. Videos of agent behaviour may be found at https://youtu.be/v\_Z9F2\_eKk4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/alexandre.piche/Zotero/storage/7ZJ3TQZJ/Abramson et al. - 2022 - Improving Multimodal Interactive Agents with Reinf.pdf;/Users/alexandre.piche/Zotero/storage/RC3QGL8F/2211.html}
}

@article{abramsonImprovingMultimodalInteractive2022a,
  title = {Improving {{Multimodal Interactive Agents}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Abramson, Josh and Ahuja, Arun and Carnevale, Federico and Georgiev, Petko and Goldin, Alex and Hung, Alden and Landon, Jessica and Lhotka, Jirka and Lillicrap, Timothy and Muldal, Alistair and Powell, George and Santoro, Adam and Scully, Guy and Srivastava, Sanjana and von Glehn, Tamara and Wayne, Greg and Wong, Nathaniel and Yan, Chen and Zhu, Rui},
  options = {useprefix=true},
  date = {2022-11-21},
  doi = {10.48550/arXiv.2211.11602},
  url = {https://arxiv.org/abs/2211.11602v1},
  urldate = {2022-12-29},
  abstract = {An important goal in artificial intelligence is to create agents that can both interact naturally with humans and learn from their feedback. Here we demonstrate how to use reinforcement learning from human feedback (RLHF) to improve upon simulated, embodied agents trained to a base level of competency with imitation learning. First, we collected data of humans interacting with agents in a simulated 3D world. We then asked annotators to record moments where they believed that agents either progressed toward or regressed from their human-instructed goal. Using this annotation data we leveraged a novel method - which we call "Inter-temporal Bradley-Terry" (IBT) modelling - to build a reward model that captures human judgments. Agents trained to optimise rewards delivered from IBT reward models improved with respect to all of our metrics, including subsequent human judgment during live interactions with agents. Altogether our results demonstrate how one can successfully leverage human judgments to improve agent behaviour, allowing us to use reinforcement learning in complex, embodied domains without programmatic reward functions. Videos of agent behaviour may be found at https://youtu.be/v\_Z9F2\_eKk4.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/NAZY5EGU/Abramson et al. - 2022 - Improving Multimodal Interactive Agents with Reinf.pdf}
}

@misc{adolphsCRINGELossLearning2022,
  title = {The {{CRINGE Loss}}: {{Learning}} What Language Not to Model},
  shorttitle = {The {{CRINGE Loss}}},
  author = {Adolphs, Leonard and Gao, Tianyu and Xu, Jing and Shuster, Kurt and Sukhbaatar, Sainbayar and Weston, Jason},
  date = {2022-11-10},
  number = {arXiv:2211.05826},
  eprint = {2211.05826},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.05826},
  url = {http://arxiv.org/abs/2211.05826},
  urldate = {2022-12-13},
  abstract = {Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data -- examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the CRINGE loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/YNYYZCSZ/Adolphs et al. - 2022 - The CRINGE Loss Learning what language not to mod.pdf;/Users/alexandre.piche/Zotero/storage/7AK59X27/2211.html}
}

@misc{ajayConditionalGenerativeModeling2022,
  title = {Is {{Conditional Generative Modeling}} All You Need for {{Decision-Making}}?},
  author = {Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua and Jaakkola, Tommi and Agrawal, Pulkit},
  date = {2022-12-06},
  number = {arXiv:2211.15657},
  eprint = {2211.15657},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.15657},
  url = {http://arxiv.org/abs/2211.15657},
  urldate = {2022-12-13},
  abstract = {Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/D2HN65IR/Ajay et al. - 2022 - Is Conditional Generative Modeling all you need fo.pdf;/Users/alexandre.piche/Zotero/storage/K7UH9ZA4/2211.html}
}

@misc{ajayConditionalGenerativeModeling2022a,
  title = {Is {{Conditional Generative Modeling}} All You Need for {{Decision-Making}}?},
  author = {Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua and Jaakkola, Tommi and Agrawal, Pulkit},
  date = {2022-12-06},
  number = {arXiv:2211.15657},
  eprint = {2211.15657},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.15657},
  url = {http://arxiv.org/abs/2211.15657},
  urldate = {2022-12-20},
  abstract = {Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/7Z624KVG/Ajay et al. - 2022 - Is Conditional Generative Modeling all you need fo.pdf;/Users/alexandre.piche/Zotero/storage/HXDZZVSD/2211.html}
}

@misc{akyurekWhatLearningAlgorithm2022,
  title = {What Learning Algorithm Is In-Context Learning? {{Investigations}} with Linear Models},
  shorttitle = {What Learning Algorithm Is In-Context Learning?},
  author = {Akyürek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  date = {2022-11-28},
  number = {arXiv:2211.15661},
  eprint = {2211.15661},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.15661},
  url = {http://arxiv.org/abs/2211.15661},
  urldate = {2022-12-13},
  abstract = {Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples \$(x, f(x))\$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at https://github.com/ekinakyurek/google-research/blob/master/incontext.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/PCCQ8683/Akyürek et al. - 2022 - What learning algorithm is in-context learning In.pdf;/Users/alexandre.piche/Zotero/storage/ZH27RDB6/2211.html}
}

@misc{andreasLanguageModelsAgent2022,
  title = {Language {{Models}} as {{Agent Models}}},
  author = {Andreas, Jacob},
  date = {2022-12-03},
  number = {arXiv:2212.01681},
  eprint = {2212.01681},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.01681},
  url = {http://arxiv.org/abs/2212.01681},
  urldate = {2022-12-13},
  abstract = {Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Multiagent Systems},
  file = {/Users/alexandre.piche/Zotero/storage/KRCRRTTA/Andreas - 2022 - Language Models as Agent Models.pdf;/Users/alexandre.piche/Zotero/storage/C7QUA56Z/2212.html}
}

@misc{anilExploringLengthGeneralization2022,
  title = {Exploring {{Length Generalization}} in {{Large Language Models}}},
  author = {Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  date = {2022-11-14},
  number = {arXiv:2207.04901},
  eprint = {2207.04901},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.04901},
  urldate = {2022-12-13},
  abstract = {The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models’ in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/7VHPWNIU/Anil et al. - 2022 - Exploring Length Generalization in Large Language .pdf}
}

@misc{baeIfInfluenceFunctions2022,
  title = {If {{Influence Functions}} Are the {{Answer}}, {{Then What}} Is the {{Question}}?},
  author = {Bae, Juhan and Ng, Nathan and Lo, Alston and Ghassemi, Marzyeh and Grosse, Roger},
  date = {2022-09-12},
  number = {arXiv:2209.05364},
  eprint = {2209.05364},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.05364},
  urldate = {2022-12-13},
  abstract = {Influence functions efficiently estimate the effect of removing a single training data point on a model’s learned parameters. While influence estimates align well with leave-one-out retraining for linear models, recent works have shown this alignment is often poor in neural networks. In this work, we investigate the specific factors that cause this discrepancy by decomposing it into five separate terms. We study the contributions of each term on a variety of architectures and datasets and how they vary with factors such as network width and training time. While practical influence function estimates may be a poor match to leave-one-out retraining for nonlinear networks, we show they are often a good approximation to a different object we term the proximal Bregman response function (PBRF). Since the PBRF can still be used to answer many of the questions motivating influence functions, such as identifying influential or mislabeled examples, our results suggest that current algorithms for influence function estimation give more informative results than previous error analyses would suggest.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/5DNXDWYG/Bae et al. - 2022 - If Influence Functions are the Answer, Then What i.pdf}
}

@misc{baiTrainingHelpfulHarmless2022,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  date = {2022-04-12},
  number = {arXiv:2204.05862},
  eprint = {2204.05862},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.05862},
  urldate = {2022-07-27},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/UJI5YEEC/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf;/Users/alexandre.piche/Zotero/storage/9X96A3ML/2204.html}
}

@misc{baiTrainingHelpfulHarmless2022a,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  date = {2022-04-12},
  number = {arXiv:2204.05862},
  eprint = {2204.05862},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.05862},
  urldate = {2022-10-27},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/PSBQLSI5/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf;/Users/alexandre.piche/Zotero/storage/A4GNT26C/2204.html}
}

@misc{bakerVideoPreTrainingVPT2022,
  title = {Video {{PreTraining}} ({{VPT}}): {{Learning}} to {{Act}} by {{Watching Unlabeled Online Videos}}},
  shorttitle = {Video {{PreTraining}} ({{VPT}})},
  author = {Baker, Bowen and Akkaya, Ilge and Zhokhov, Peter and Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and Houghton, Brandon and Sampedro, Raul and Clune, Jeff},
  date = {2022-06-23},
  number = {arXiv:2206.11795},
  eprint = {2206.11795},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.11795},
  url = {http://arxiv.org/abs/2206.11795},
  urldate = {2022-12-13},
  abstract = {Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/XKCDZWTC/Baker et al. - 2022 - Video PreTraining (VPT) Learning to Act by Watchi.pdf;/Users/alexandre.piche/Zotero/storage/IAYE6MRJ/2206.html}
}

@misc{bavarianEfficientTrainingLanguage2022,
  title = {Efficient {{Training}} of {{Language Models}} to {{Fill}} in the {{Middle}}},
  author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  date = {2022-07-28},
  number = {arXiv:2207.14255},
  eprint = {2207.14255},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.14255},
  urldate = {2022-08-03},
  abstract = {We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/JK89Q438/Bavarian et al. - 2022 - Efficient Training of Language Models to Fill in t.pdf;/Users/alexandre.piche/Zotero/storage/9FS9VVLY/2207.html}
}

@misc{bhatiaStatisticalComputationalTradeoffs2022,
  title = {Statistical and {{Computational Trade-offs}} in {{Variational Inference}}: {{A Case Study}} in {{Inferential Model Selection}}},
  shorttitle = {Statistical and {{Computational Trade-offs}} in {{Variational Inference}}},
  author = {Bhatia, Kush and Kuang, Nikki Lijing and Ma, Yi-An and Wang, Yixin},
  date = {2022-07-22},
  number = {arXiv:2207.11208},
  eprint = {2207.11208},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.11208},
  urldate = {2022-12-13},
  abstract = {Variational inference has recently emerged as a popular alternative to the classical Markov chain Monte Carlo (MCMC) in large-scale Bayesian inference. The core idea of variational inference is to trade statistical accuracy for computational e ciency. It aims to approximate the posterior, reducing computation costs but potentially compromising its statistical accuracy. In this work, we study this statistical and computational trade-o in variational inference via a case study in inferential model selection. Focusing on Gaussian inferential models (also known as variational approximating families) with diagonal plus low-rank precision matrices, we initiate a theoretical study of the trade-o s in two aspects, Bayesian posterior inference error and frequentist uncertainty quanti cation error. From the Bayesian posterior inference perspective, we characterize the error of the variational posterior relative to the exact posterior. We prove that, given a xed computation budget, a lower-rank inferential model produces variational posteriors with a higher statistical approximation error, but a lower computational error; it reduces variances in stochastic optimization and, in turn, accelerates convergence. From the frequentist uncertainty quanti cation perspective, we consider the precision matrix of the variational posterior as an uncertainty estimate. We nd that, relative to the true asymptotic precision, the variational approximation su ers from an additional statistical error originating from the sampling uncertainty of the data. Moreover, this statistical error becomes the dominant factor as the computation budget increases. As a consequence, for small datasets, the inferential model need not be full-rank to achieve optimal estimation error (even with unlimited computation budget). We nally demonstrate these statistical and computational trade-o s inference across empirical studies, corroborating the theoretical ndings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/QJCXZK2K/Bhatia et al. - 2022 - Statistical and Computational Trade-offs in Variat.pdf}
}

@inproceedings{bhattacharyyaEnergyBasedRerankingImproving2021,
  title = {Energy-{{Based Reranking}}: {{Improving Neural Machine Translation Using Energy-Based Models}}},
  shorttitle = {Energy-{{Based Reranking}}},
  author = {Bhattacharyya, Sumanta and Rooshenas, Amirmohammad and Naskar, Subhajit and Sun, Simeng and Iyyer, Mohit and McCallum, Andrew},
  date = {2021},
  eprint = {2009.13267},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {4528--4537},
  doi = {10.18653/v1/2021.acl-long.349},
  url = {http://arxiv.org/abs/2009.13267},
  urldate = {2022-12-16},
  abstract = {The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution -- there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a re-ranking algorithm based on the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences). Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT: +4 BLEU points on IWSLT'14 German-English, +3.0 BELU points on Sinhala-English, +1.2 BLEU on WMT'16 English-German tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/ENQFWBUN/Bhattacharyya et al. - 2021 - Energy-Based Reranking Improving Neural Machine Translation Using Energy-Based Models.pdf}
}

@misc{blondelEfficientModularImplicit2022,
  title = {Efficient and {{Modular Implicit Differentiation}}},
  author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and Llinares-López, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
  date = {2022-10-12},
  number = {arXiv:2105.15183},
  eprint = {2105.15183},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.15183},
  urldate = {2022-11-17},
  abstract = {Automatic differentiation (autodiff) has revolutionized machine learning. It allows to express complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization layers, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, so far, implicit differentiation remained difficult to use for practitioners, as it often required case-by-case tedious mathematical derivations and implementations. In this paper, we propose automatic implicit differentiation, an efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function \$F\$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of \$F\$ and the implicit function theorem to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many existing implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/ZFK5FNIV/Blondel et al. - 2022 - Efficient and Modular Implicit Differentiation.pdf;/Users/alexandre.piche/Zotero/storage/FKCV2UHX/2105.html}
}

@misc{blondelLearningEnergyNetworks2022,
  title = {Learning {{Energy Networks}} with {{Generalized Fenchel-Young Losses}}},
  author = {Blondel, Mathieu and Llinares-López, Felipe and Dadashi, Robert and Hussenot, Léonard and Geist, Matthieu},
  date = {2022-05-19},
  number = {arXiv:2205.09589},
  eprint = {2205.09589},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.09589},
  urldate = {2022-07-18},
  abstract = {Energy-based models, a.k.a. energy networks, perform inference by optimizing an energy function, typically parametrized by a neural network. This allows one to capture potentially complex relationships between inputs and outputs. To learn the parameters of the energy function, the solution to that optimization problem is typically fed into a loss function. The key challenge for training energy networks lies in computing loss gradients, as this typically requires argmin/argmax differentiation. In this paper, building upon a generalized notion of conjugate function, which replaces the usual bilinear pairing with a general energy function, we propose generalized Fenchel-Young losses, a natural loss construction for learning energy networks. Our losses enjoy many desirable properties and their gradients can be computed efficiently without argmin/argmax differentiation. We also prove the calibration of their excess risk in the case of linear-concave energies. We demonstrate our losses on multilabel classification and imitation learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/QQC3AJZA/Blondel et al. - 2022 - Learning Energy Networks with Generalized Fenchel-.pdf;/Users/alexandre.piche/Zotero/storage/LJ7SD4MD/2205.html}
}

@misc{bowmanMeasuringProgressScalable2022,
  title = {Measuring {{Progress}} on {{Scalable Oversight}} for {{Large Language Models}}},
  author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Lukosuite, Kamile and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Kaplan, Jared},
  date = {2022-11-04},
  number = {arXiv:2211.03540},
  eprint = {2211.03540},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2211.03540},
  urldate = {2022-11-08},
  abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on how to turn it into one that can be productively studied empirically. We first present an experimental design centered on choosing tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment following meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/alexandre.piche/Zotero/storage/KJVMNXYC/Bowman et al. - 2022 - Measuring Progress on Scalable Oversight for Large.pdf;/Users/alexandre.piche/Zotero/storage/XGCZ28IT/2211.html}
}

@misc{bowmanMeasuringProgressScalable2022a,
  title = {Measuring {{Progress}} on {{Scalable Oversight}} for {{Large Language Models}}},
  author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Lukošiūtė, Kamilė and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Kaplan, Jared},
  date = {2022-11-11},
  number = {arXiv:2211.03540},
  eprint = {2211.03540},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.03540},
  url = {http://arxiv.org/abs/2211.03540},
  urldate = {2022-12-13},
  abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/alexandre.piche/Zotero/storage/K89C9NJY/Bowman et al. - 2022 - Measuring Progress on Scalable Oversight for Large.pdf;/Users/alexandre.piche/Zotero/storage/NAM4WIMX/2211.html}
}

@article{branavanLearningWinReading2012,
  title = {Learning to {{Win}} by {{Reading Manuals}} in a {{Monte-Carlo Framework}}},
  author = {Branavan, S. R. K. and Silver, David and Barzilay, Regina},
  date = {2012-04-30},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {43},
  eprint = {1401.5390},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {661--704},
  issn = {1076-9757},
  doi = {10.1613/jair.3484},
  url = {http://arxiv.org/abs/1401.5390},
  urldate = {2022-08-31},
  abstract = {Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34\% absolute improvement and winning over 65\% of games when playing against the built-in AI of Civilization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/JPTAVGIH/Branavan et al. - 2012 - Learning to Win by Reading Manuals in a Monte-Carl.pdf;/Users/alexandre.piche/Zotero/storage/HZZ2SH9Q/1401.html}
}

@misc{brownExtrapolatingSuboptimalDemonstrations2019,
  title = {Extrapolating {{Beyond Suboptimal Demonstrations}} via {{Inverse Reinforcement Learning}} from {{Observations}}},
  author = {Brown, Daniel S. and Goo, Wonjoon and Nagarajan, Prabhat and Niekum, Scott},
  date = {2019-07-08},
  number = {arXiv:1904.06387},
  eprint = {1904.06387},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.06387},
  url = {http://arxiv.org/abs/1904.06387},
  urldate = {2022-12-19},
  abstract = {A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/X4UW9GTH/Brown et al. - 2019 - Extrapolating Beyond Suboptimal Demonstrations via.pdf;/Users/alexandre.piche/Zotero/storage/8JKNVR8B/1904.html}
}

@misc{brownExtrapolatingSuboptimalDemonstrations2019a,
  title = {Extrapolating {{Beyond Suboptimal Demonstrations}} via {{Inverse Reinforcement Learning}} from {{Observations}}},
  author = {Brown, Daniel S. and Goo, Wonjoon and Nagarajan, Prabhat and Niekum, Scott},
  date = {2019-07-08},
  number = {arXiv:1904.06387},
  eprint = {1904.06387},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.06387},
  url = {http://arxiv.org/abs/1904.06387},
  urldate = {2022-12-21},
  abstract = {A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/FVPHMAL3/Brown et al. - 2019 - Extrapolating Beyond Suboptimal Demonstrations via.pdf;/Users/alexandre.piche/Zotero/storage/3QXE569Z/1904.html}
}

@misc{brownSafeImitationLearning2020,
  title = {Safe {{Imitation Learning}} via {{Fast Bayesian Reward Inference}} from {{Preferences}}},
  author = {Brown, Daniel S. and Coleman, Russell and Srinivasan, Ravi and Niekum, Scott},
  date = {2020-12-17},
  number = {arXiv:2002.09089},
  eprint = {2002.09089},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.09089},
  url = {http://arxiv.org/abs/2002.09089},
  urldate = {2022-12-15},
  abstract = {Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/BZ7SZUUK/Brown et al. - 2020 - Safe Imitation Learning via Fast Bayesian Reward I.pdf;/Users/alexandre.piche/Zotero/storage/E32U6AW6/2002.html}
}

@article{brunetImplicationsModelIndeterminacy,
  title = {Implications of {{Model Indeterminacy}} for {{Explanations}} of {{Automated Decisions}}},
  author = {Brunet, Marc-Etienne and Anderson, Ashton and Zemel, Richard},
  abstract = {There has been a significant research effort focused on explaining predictive models, for example through post-hoc explainability and recourse methods. Most of the proposed techniques operate upon a single, fixed, predictive model. However, it is well-known that given a dataset and a predictive task, there may be a multiplicity of models that solve the problem (nearly) equally well. In this work, we investigate the implications of this kind of model indeterminacy on the post-hoc explanations of predictive models. We show how it can lead to explanatory multiplicity, and we explore the underlying drivers. We show how predictive multiplicity, and the related concept of epistemic uncertainty, are not reliable indicators of explanatory multiplicity. We further illustrate how a set of models showing very similar aggregate performance on a test dataset may show large variations in their local explanations, i.e., for a specific input. We explore these effects for Shapley value based explanations on three risk assessment datasets. Our results indicate that model indeterminacy may have a substantial impact on explanations in practice, leading to inconsistent and even contradicting explanations.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/ZFNUU8XR/Brunet et al. - Implications of Model Indeterminacy for Explanatio.pdf}
}

@inproceedings{brunetImplicationsModelIndeterminacy2022,
  title = {Implications of {{Model Indeterminacy}} for {{Explanations}} of {{Automated Decisions}}},
  author = {Brunet, Marc-Etienne and Anderson, Ashton and Zemel, Richard},
  date = {2022-10-31},
  url = {https://openreview.net/forum?id=LzbrVf-l0Xq},
  urldate = {2022-12-16},
  abstract = {There has been a significant research effort focused on explaining predictive models, for example through post-hoc explainability and recourse methods. Most of the proposed techniques operate upon a single, fixed, predictive model. However, it is well-known that given a dataset and a predictive task, there may be a multiplicity of models that solve the problem (nearly) equally well. In this work, we investigate the implications of this kind of model indeterminacy on the post-hoc explanations of predictive models. We show how it can lead to explanatory multiplicity, and we explore the underlying drivers. We show how predictive multiplicity, and the related concept of epistemic uncertainty, are not reliable indicators of explanatory multiplicity. We further illustrate how a set of models showing very similar aggregate performance on a test dataset may show large variations in their local explanations, i.e., for a specific input. We explore these effects for Shapley value based explanations on three risk assessment datasets. Our results indicate that model indeterminacy may have a substantial impact on explanations in practice, leading to inconsistent and even contradicting explanations.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/GFFNQBNT/Brunet et al. - 2022 - Implications of Model Indeterminacy for Explanatio.pdf}
}

@misc{burnsDiscoveringLatentKnowledge2022,
  title = {Discovering {{Latent Knowledge}} in {{Language Models Without Supervision}}},
  author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  date = {2022-12-07},
  number = {arXiv:2212.03827},
  eprint = {2212.03827},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2212.03827},
  urldate = {2022-12-13},
  abstract = {Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can’t detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 questionanswering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don’t have access to explicit ground truth labels.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/HMUVU9WE/Burns et al. - 2022 - Discovering Latent Knowledge in Language Models Wi.pdf}
}

@inproceedings{carrollEstimatingPenalizingInduced2022,
  title = {Estimating and {{Penalizing Induced Preference Shifts}} in {{Recommender Systems}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Carroll, Micah D. and Dragan, Anca and Russell, Stuart and Hadfield-Menell, Dylan},
  date = {2022-06-28},
  pages = {2686--2708},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/carroll22a.html},
  urldate = {2022-07-30},
  abstract = {The content that a recommender system (RS) shows to users influences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce specific internal states in users. Even more, systems trained via long-horizon optimization will have direct incentives to manipulate users, e.g. shift their preferences so they are easier to satisfy. We focus on induced preference shifts in users. We argue that \{–\} before deployment \{–\} system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical policies would influence user preferences if deployed \{–\} we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such influences are manipulative or otherwise unwanted \{–\} we use the notion of "safe shifts", that define a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed "safe". In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/33SGG9GL/Carroll et al. - 2022 - Estimating and Penalizing Induced Preference Shift.pdf}
}

@misc{carrollEstimatingPenalizingInduced2022a,
  title = {Estimating and {{Penalizing Induced Preference Shifts}} in {{Recommender Systems}}},
  author = {Carroll, Micah and Dragan, Anca and Russell, Stuart and Hadfield-Menell, Dylan},
  date = {2022-07-14},
  number = {arXiv:2204.11966},
  eprint = {2204.11966},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.11966},
  urldate = {2022-11-07},
  abstract = {The content that a recommender system (RS) shows to users influences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce specific internal states in users. Even more, systems trained via long-horizon optimization will have direct incentives to manipulate users: in this work, we focus on the incentive to shift user preferences so they are easier to satisfy. We argue that - before deployment - system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical algorithms would influence user preferences if deployed - we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such influences are manipulative or otherwise unwanted - we use the notion of "safe shifts", that define a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed "safe". In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/233QKI58/Carroll et al. - 2022 - Estimating and Penalizing Induced Preference Shift.pdf;/Users/alexandre.piche/Zotero/storage/CZIBEWC4/2204.html}
}

@online{CaseAligningNarrowly,
  title = {The Case for Aligning Narrowly Superhuman Models - {{AI Alignment Forum}}},
  url = {https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models},
  urldate = {2022-11-08},
  abstract = {I wrote this post to get people’s takes on a type of work that seems exciting to me personally; I’m not speaking for Open Phil as a whole. Institutionally, we are very uncertain whether to prioritize…},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/SZB4C4UH/the-case-for-aligning-narrowly-superhuman-models.html}
}

@misc{chakrabartyHelpMeWrite2022,
  title = {Help Me Write a Poem: {{Instruction Tuning}} as a {{Vehicle}} for {{Collaborative Poetry Writing}}},
  shorttitle = {Help Me Write a Poem},
  author = {Chakrabarty, Tuhin and Padmakumar, Vishakh and He, He},
  date = {2022-10-24},
  number = {arXiv:2210.13669},
  eprint = {2210.13669},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.13669},
  urldate = {2022-11-30},
  abstract = {Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of LLMs in the realm of computer-assisted creativity, we aim to study if LLMs can improve the quality of user-generated content through collaboration. We present CoPoet, a collaborative poetry writing system. In contrast to auto-completing a user's text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as Write a sentence about `love' or Write a sentence ending in `fly'. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive with publicly available LLMs trained on instructions (InstructGPT), but is also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from Monarchy to Climate change. Further, the collaboratively written poems are preferred by third-party evaluators over those written without the system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/TNBNWL3S/Chakrabarty et al. - 2022 - Help me write a poem Instruction Tuning as a Vehi.pdf;/Users/alexandre.piche/Zotero/storage/WURAVPII/2210.html}
}

@misc{chakrabartyHelpMeWrite2022a,
  title = {Help Me Write a Poem: {{Instruction Tuning}} as a {{Vehicle}} for {{Collaborative Poetry Writing}}},
  shorttitle = {Help Me Write a Poem},
  author = {Chakrabarty, Tuhin and Padmakumar, Vishakh and He, He},
  date = {2022-10-24},
  number = {arXiv:2210.13669},
  eprint = {2210.13669},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.13669},
  url = {http://arxiv.org/abs/2210.13669},
  urldate = {2022-12-13},
  abstract = {Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of LLMs in the realm of computer-assisted creativity, we aim to study if LLMs can improve the quality of user-generated content through collaboration. We present CoPoet, a collaborative poetry writing system. In contrast to auto-completing a user's text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as Write a sentence about `love' or Write a sentence ending in `fly'. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive with publicly available LLMs trained on instructions (InstructGPT), but is also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from Monarchy to Climate change. Further, the collaboratively written poems are preferred by third-party evaluators over those written without the system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/HUS2Y3IT/Chakrabarty et al. - 2022 - Help me write a poem Instruction Tuning as a Vehi.pdf;/Users/alexandre.piche/Zotero/storage/8ZSPFUBZ/2210.html}
}

@misc{chanDataDistributionalProperties2022,
  title = {Data {{Distributional Properties Drive Emergent In-Context Learning}} in {{Transformers}}},
  author = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, Jay and Hill, Felix},
  date = {2022-11-17},
  number = {arXiv:2205.05055},
  eprint = {2205.05055},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.05055},
  url = {http://arxiv.org/abs/2205.05055},
  urldate = {2022-12-13},
  abstract = {Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/QM9VVMFK/Chan et al. - 2022 - Data Distributional Properties Drive Emergent In-C.pdf;/Users/alexandre.piche/Zotero/storage/PHH4F9ZT/2205.html}
}

@misc{chenDecisionTransformerReinforcement2021,
  title = {Decision {{Transformer}}: {{Reinforcement Learning}} via {{Sequence Modeling}}},
  shorttitle = {Decision {{Transformer}}},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  date = {2021-06-24},
  number = {arXiv:2106.01345},
  eprint = {2106.01345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.01345},
  urldate = {2022-08-13},
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/3KQ9M5QR/Chen et al. - 2021 - Decision Transformer Reinforcement Learning via S.pdf;/Users/alexandre.piche/Zotero/storage/8T7MU24S/2106.html}
}

@misc{chenLearningUniversalHyperparameter2022,
  title = {Towards {{Learning Universal Hyperparameter Optimizers}} with {{Transformers}}},
  author = {Chen, Yutian and Song, Xingyou and Lee, Chansoo and Wang, Zi and Zhang, Qiuyi and Dohan, David and Kawakami, Kazuya and Kochanski, Greg and Doucet, Arnaud and Ranzato, Marc'aurelio and Perel, Sagi and de Freitas, Nando},
  options = {useprefix=true},
  date = {2022-05-26},
  number = {arXiv:2205.13320},
  eprint = {2205.13320},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.13320},
  urldate = {2022-07-28},
  abstract = {Meta-learning hyperparameter optimization (HPO) algorithms from prior experiments is a promising approach to improve optimization efficiency over objective functions from a similar distribution. However, existing methods are restricted to learning from experiments sharing the same set of hyperparameters. In this paper, we introduce the OptFormer, the first text-based Transformer HPO framework that provides a universal end-to-end interface for jointly learning policy and function prediction when trained on vast tuning data from the wild. Our extensive experiments demonstrate that the OptFormer can imitate at least 7 different HPO algorithms, which can be further improved via its function uncertainty estimates. Compared to a Gaussian Process, the OptFormer also learns a robust prior distribution for hyperparameter response functions, and can thereby provide more accurate and better calibrated predictions. This work paves the path to future extensions for training a Transformer-based model as a general HPO optimizer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/B74YXQY6/Chen et al. - 2022 - Towards Learning Universal Hyperparameter Optimize.pdf;/Users/alexandre.piche/Zotero/storage/CIJXJCMW/2205.html}
}

@misc{chowMixtureofExpertApproachRLbased2022,
  title = {A {{Mixture-of-Expert Approach}} to {{RL-based Dialogue Management}}},
  author = {Chow, Yinlam and Tulepbergenov, Aza and Nachum, Ofir and Ryu, MoonKyung and Ghavamzadeh, Mohammad and Boutilier, Craig},
  date = {2022-05-31},
  number = {arXiv:2206.00059},
  eprint = {2206.00059},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.00059},
  urldate = {2022-08-03},
  abstract = {Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge. We use reinforcement learning (RL) to develop a dialogue agent that avoids being short-sighted (outputting generic utterances) and maximizes overall user satisfaction. Most existing RL approaches to DM train the agent at the word-level, and thus, have to deal with a combinatorially complex action space even for a medium-size vocabulary. As a result, they struggle to produce a successful and engaging dialogue even if they are warm-started with a pre-trained LM. To address this issue, we develop a RL-based DM using a novel mixture of expert language model (MoE-LM) that consists of (i) a LM capable of learning diverse semantics for conversation histories, (ii) a number of \{\textbackslash em specialized\} LMs (or experts) capable of generating utterances corresponding to a particular attribute or personality, and (iii) a RL-based DM that performs dialogue planning with the utterances generated by the experts. Our MoE approach provides greater flexibility to generate sensible utterances with different intents and allows RL to focus on conversational-level DM. We compare it with SOTA baselines on open-domain dialogues and demonstrate its effectiveness both in terms of the diversity and sensibility of the generated utterances and the overall DM performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/5HGIFXUM/Chow et al. - 2022 - A Mixture-of-Expert Approach to RL-based Dialogue .pdf;/Users/alexandre.piche/Zotero/storage/U3AWZCYH/2206.html}
}

@misc{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  date = {2017-07-13},
  number = {arXiv:1706.03741},
  eprint = {1706.03741},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1706.03741},
  urldate = {2022-10-27},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/JYA3LJ45/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf;/Users/alexandre.piche/Zotero/storage/PXPMCVCP/1706.html}
}

@misc{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  date = {2022-10-21},
  number = {arXiv:2210.11416},
  eprint = {2210.11416},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.11416},
  urldate = {2022-11-07},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/LV22ZAGU/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf;/Users/alexandre.piche/Zotero/storage/G9S8P9P5/2210.html}
}

@misc{chungScalingInstructionFinetunedLanguage2022a,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  date = {2022-12-06},
  number = {arXiv:2210.11416},
  eprint = {2210.11416},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11416},
  url = {http://arxiv.org/abs/2210.11416},
  urldate = {2022-12-21},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/CZSASUZD/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf;/Users/alexandre.piche/Zotero/storage/BX3QSUXL/2210.html}
}

@misc{collobertNaturalLanguageProcessing2011,
  title = {Natural {{Language Processing}} (Almost) from {{Scratch}}},
  author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  date = {2011-03-02},
  number = {arXiv:1103.0398},
  eprint = {1103.0398},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1103.0398},
  urldate = {2022-12-13},
  abstract = {We propose a unified neural network architecture and learning algorithm that can be applied iv to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/WA4K3758/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf}
}

@online{Copilotexplorer,
  title = {Copilot-Explorer},
  url = {https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html},
  urldate = {2022-12-19},
  abstract = {Hacky repo to see what the Copilot extension sends to the server}
}

@online{CounterargumentsBasicAI2022,
  title = {Counterarguments to the Basic {{AI}} X-Risk Case},
  date = {2022-10-14T05:58:58-07:00},
  url = {https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/},
  urldate = {2022-10-31},
  abstract = {Sixteen weaknesses in the classic argument for AI risk.}
}

@inproceedings{damourFairnessNotStatic2020,
  title = {Fairness Is Not Static: Deeper Understanding of Long Term Fairness via Simulation Studies},
  shorttitle = {Fairness Is Not Static},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
  date = {2020-01-27},
  pages = {525--534},
  publisher = {{ACM}},
  location = {{Barcelona Spain}},
  doi = {10.1145/3351095.3372878},
  url = {https://dl.acm.org/doi/10.1145/3351095.3372878},
  urldate = {2022-11-22},
  abstract = {As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses �xed data sets. To address this structural di�culty in the �eld, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the longterm consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.},
  eventtitle = {{{FAT}}* '20: {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  isbn = {978-1-4503-6936-7},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/EELUC87N/D'Amour et al. - 2020 - Fairness is not static deeper understanding of lo.pdf}
}

@misc{dathathriPlugPlayLanguage2020,
  title = {Plug and {{Play Language Models}}: {{A Simple Approach}} to {{Controlled Text Generation}}},
  shorttitle = {Plug and {{Play Language Models}}},
  author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  date = {2020-03-03},
  number = {arXiv:1912.02164},
  eprint = {1912.02164},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.02164},
  url = {http://arxiv.org/abs/1912.02164},
  urldate = {2022-12-13},
  abstract = {Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/LXDEP8LX/Dathathri et al. - 2020 - Plug and Play Language Models A Simple Approach t.pdf;/Users/alexandre.piche/Zotero/storage/XQQS2SJM/1912.html}
}

@online{DemystifyingNoiseContrastive2022,
  title = {Demystifying {{Noise Contrastive Estimation}}},
  date = {2022-01-21T19:40:00+00:00},
  url = {http://jxmo.io/posts/nce},
  urldate = {2022-08-12},
  abstract = {Personal website for Jack Morris.},
  organization = {{Jack Morris}},
  file = {/Users/alexandre.piche/Zotero/storage/XQLKHPH7/nce.html}
}

@misc{dennisEmergentComplexityZeroshot2021,
  title = {Emergent {{Complexity}} and {{Zero-shot Transfer}} via {{Unsupervised Environment Design}}},
  author = {Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre and Russell, Stuart and Critch, Andrew and Levine, Sergey},
  date = {2021-02-03},
  number = {arXiv:2012.02096},
  eprint = {2012.02096},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2012.02096},
  urldate = {2022-12-13},
  abstract = {A wide range of reinforcement learning (RL) problems — including robustness, transfer learning, unsupervised RL, and emergent complexity — require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent’s learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent’s return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/alexandre.piche/Zotero/storage/BNUTTS8P/Dennis et al. - 2021 - Emergent Complexity and Zero-shot Transfer via Uns.pdf}
}

@misc{dielemanContinuousDiffusionCategorical2022,
  title = {Continuous Diffusion for Categorical Data},
  author = {Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H. and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and Hawthorne, Curtis and Leblond, Rémi and Grathwohl, Will and Adler, Jonas},
  date = {2022-12-06},
  number = {arXiv:2211.15089},
  eprint = {2211.15089},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.15089},
  url = {http://arxiv.org/abs/2211.15089},
  urldate = {2022-12-16},
  abstract = {Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/V2JTVLWB/Dieleman et al. - 2022 - Continuous diffusion for categorical data.pdf}
}

@misc{dielemanContinuousDiffusionCategorical2022a,
  title = {Continuous Diffusion for Categorical Data},
  author = {Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H. and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and Hawthorne, Curtis and Leblond, Rémi and Grathwohl, Will and Adler, Jonas},
  date = {2022-12-15},
  number = {arXiv:2211.15089},
  eprint = {2211.15089},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.15089},
  url = {http://arxiv.org/abs/2211.15089},
  urldate = {2022-12-17},
  abstract = {Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/KM8GH9G9/Dieleman et al. - 2022 - Continuous diffusion for categorical data.pdf;/Users/alexandre.piche/Zotero/storage/RBWQDF87/2211.html}
}

@misc{doucetScoreBasedDiffusionMeets2022,
  title = {Score-{{Based Diffusion}} Meets {{Annealed Importance Sampling}}},
  author = {Doucet, Arnaud and Grathwohl, Will and Matthews, Alexander G. D. G. and Strathmann, Heiko},
  date = {2022-10-24},
  number = {arXiv:2208.07698},
  eprint = {2208.07698},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2208.07698},
  url = {http://arxiv.org/abs/2208.07698},
  urldate = {2022-12-18},
  abstract = {More than twenty years after its introduction, Annealed Importance Sampling (AIS) remains one of the most effective methods for marginal likelihood estimation. It relies on a sequence of distributions interpolating between a tractable initial distribution and the target distribution of interest which we simulate from approximately using a non-homogeneous Markov chain. To obtain an importance sampling estimate of the marginal likelihood, AIS introduces an extended target distribution to reweight the Markov chain proposal. While much effort has been devoted to improving the proposal distribution used by AIS, an underappreciated issue is that AIS uses a convenient but suboptimal extended target distribution. We here leverage recent progress in score-based generative modeling (SGM) to approximate the optimal extended target distribution minimizing the variance of the marginal likelihood estimate for AIS proposals corresponding to the discretization of Langevin and Hamiltonian dynamics. We demonstrate these novel, differentiable, AIS procedures on a number of synthetic benchmark distributions and variational auto-encoders.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/U46BPND9/Doucet et al. - 2022 - Score-Based Diffusion meets Annealed Importance Sampling.pdf}
}

@misc{duLearningIterativeReasoning2022,
  title = {Learning {{Iterative Reasoning}} through {{Energy Minimization}}},
  author = {Du, Yilun and Li, Shuang and Tenenbaum, Joshua B. and Mordatch, Igor},
  date = {2022-06-30},
  number = {arXiv:2206.15448},
  eprint = {2206.15448},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.15448},
  urldate = {2022-07-18},
  abstract = {Deep learning has excelled on complex pattern recognition tasks such as image classification and object recognition. However, it struggles with tasks requiring nontrivial reasoning, such as algorithmic computation. Humans are able to solve such tasks through iterative reasoning -- spending more time thinking about harder tasks. Most existing neural networks, however, exhibit a fixed computational budget controlled by the neural network architecture, preventing additional computational processing on harder tasks. In this work, we present a new framework for iterative reasoning with neural networks. We train a neural network to parameterize an energy landscape over all outputs, and implement each step of the iterative reasoning as an energy minimization step to find a minimal energy solution. By formulating reasoning as an energy minimization problem, for harder problems that lead to more complex energy landscapes, we may then adjust our underlying computational budget by running a more complex optimization procedure. We empirically illustrate that our iterative reasoning approach can solve more accurate and generalizable algorithmic reasoning tasks in both graph and continuous domains. Finally, we illustrate that our approach can recursively solve algorithmic problems requiring nested reasoning},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/GNSI85PS/Du et al. - 2022 - Learning Iterative Reasoning through Energy Minimi.pdf;/Users/alexandre.piche/Zotero/storage/82WE25JQ/2206.html}
}

@misc{elazarMeasuringCausalEffects2022,
  title = {Measuring {{Causal Effects}} of {{Data Statistics}} on {{Language Model}}'s `{{Factual}}' {{Predictions}}},
  author = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Feder, Amir and Ravichander, Abhilasha and Mosbach, Marius and Belinkov, Yonatan and Schütze, Hinrich and Goldberg, Yoav},
  date = {2022-07-28},
  number = {arXiv:2207.14251},
  eprint = {2207.14251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.14251},
  urldate = {2022-08-03},
  abstract = {Large amounts of training data are one of the major reasons for the high performance of state-of-the-art NLP models. But what exactly in the training data causes a model to make a certain prediction? We seek to answer this question by providing a language for describing how training data influences predictions, through a causal framework. Importantly, our framework bypasses the need to retrain expensive models and allows us to estimate causal effects based on observational data alone. Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do influence the predictions of PLMs, suggesting that such models rely on shallow heuristics. Our causal framework and our results demonstrate the importance of studying datasets and the benefits of causality for understanding NLP models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/XTFCWTNX/Elazar et al. - 2022 - Measuring Causal Effects of Data Statistics on Lan.pdf;/Users/alexandre.piche/Zotero/storage/23GDH3HR/2207.html}
}

@misc{emmonsRvSWhatEssential2022,
  title = {{{RvS}}: {{What}} Is {{Essential}} for {{Offline RL}} via {{Supervised Learning}}?},
  shorttitle = {{{RvS}}},
  author = {Emmons, Scott and Eysenbach, Benjamin and Kostrikov, Ilya and Levine, Sergey},
  date = {2022-05-10},
  number = {arXiv:2112.10751},
  eprint = {2112.10751},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.10751},
  urldate = {2022-08-07},
  abstract = {Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin "RvS learning"). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{emmonsRvSWhatEssential2022a,
  title = {{{RvS}}: {{What}} Is {{Essential}} for {{Offline RL}} via {{Supervised Learning}}?},
  shorttitle = {{{RvS}}},
  author = {Emmons, Scott and Eysenbach, Benjamin and Kostrikov, Ilya and Levine, Sergey},
  date = {2022-05-10},
  number = {arXiv:2112.10751},
  eprint = {2112.10751},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.10751},
  urldate = {2022-08-07},
  abstract = {Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin "RvS learning"). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{emmonsRvSWhatEssential2022b,
  title = {{{RvS}}: {{What}} Is {{Essential}} for {{Offline RL}} via {{Supervised Learning}}?},
  shorttitle = {{{RvS}}},
  author = {Emmons, Scott and Eysenbach, Benjamin and Kostrikov, Ilya and Levine, Sergey},
  date = {2022-05-10},
  number = {arXiv:2112.10751},
  eprint = {2112.10751},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.10751},
  urldate = {2022-08-07},
  abstract = {Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin "RvS learning"). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/KB2TU49G/Emmons et al. - 2022 - RvS What is Essential for Offline RL via Supervis.pdf;/Users/alexandre.piche/Zotero/storage/ZYQ5ZYQ7/2112.html}
}

@misc{ethayarajhUnderstandingDatasetDifficulty2022,
  title = {Understanding {{Dataset Difficulty}} with \$\textbackslash mathcal\{\vphantom\}{{V}}\vphantom\{\}\$-{{Usable Information}}},
  author = {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  date = {2022-06-14},
  number = {arXiv:2110.08420},
  eprint = {2110.08420},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.08420},
  urldate = {2022-12-13},
  abstract = {Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty—w.r.t. a model V—as the lack of V-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for V. We further introduce pointwise V-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, V-usable information and PVI also permit the converse: for a given model V, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/T4NXWF5H/Ethayarajh et al. - 2022 - Understanding Dataset Difficulty with $mathcal V .pdf}
}

@misc{eysenbachRewritingHistoryInverse2020,
  title = {Rewriting {{History}} with {{Inverse RL}}: {{Hindsight Inference}} for {{Policy Improvement}}},
  shorttitle = {Rewriting {{History}} with {{Inverse RL}}},
  author = {Eysenbach, Benjamin and Geng, Xinyang and Levine, Sergey and Salakhutdinov, Ruslan},
  date = {2020-02-25},
  number = {arXiv:2002.11089},
  eprint = {2002.11089},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2002.11089},
  urldate = {2022-08-11},
  abstract = {Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/VKPBIX6H/Eysenbach et al. - 2020 - Rewriting History with Inverse RL Hindsight Infer.pdf;/Users/alexandre.piche/Zotero/storage/2NMVJJ9T/2002.html}
}

@misc{faccioGeneralPolicyEvaluation2022,
  title = {General {{Policy Evaluation}} and {{Improvement}} by {{Learning}} to {{Identify Few But Crucial States}}},
  author = {Faccio, Francesco and Ramesh, Aditya and Herrmann, Vincent and Harb, Jean and Schmidhuber, Jürgen},
  date = {2022-07-04},
  number = {arXiv:2207.01566},
  eprint = {2207.01566},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.01566},
  urldate = {2022-08-04},
  abstract = {Learning to evaluate and improve policies is a core problem of Reinforcement Learning (RL). Traditional RL algorithms learn a value function defined for a single policy. A recently explored competitive alternative is to learn a single value function for many policies. Here we combine the actor-critic architecture of Parameter-Based Value Functions and the policy embedding of Policy Evaluation Networks to learn a single value function for evaluating (and thus helping to improve) any policy represented by a deep neural network (NN). The method yields competitive experimental results. In continuous control problems with infinitely many states, our value function minimizes its prediction error by simultaneously learning a small set of `probing states' and a mapping from actions produced in probing states to the policy's return. The method extracts crucial abstract knowledge about the environment in form of very few states sufficient to fully specify the behavior of many policies. A policy improves solely by changing actions in probing states, following the gradient of the value function's predictions. Surprisingly, it is possible to clone the behavior of a near-optimal policy in Swimmer-v3 and Hopper-v3 environments only by knowing how to act in 3 and 5 such learned states, respectively. Remarkably, our value function trained to evaluate NN policies is also invariant to changes of the policy architecture: we show that it allows for zero-shot learning of linear policies competitive with the best policy seen during training. Our code is public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/7Y2S276F/Faccio et al. - 2022 - General Policy Evaluation and Improvement by Learn.pdf;/Users/alexandre.piche/Zotero/storage/BRYDA5LS/2207.html}
}

@misc{faccioGoalConditionedGeneratorsDeep2022,
  title = {Goal-{{Conditioned Generators}} of {{Deep Policies}}},
  author = {Faccio, Francesco and Herrmann, Vincent and Ramesh, Aditya and Kirsch, Louis and Schmidhuber, Jürgen},
  date = {2022-07-04},
  number = {arXiv:2207.01570},
  eprint = {2207.01570},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.01570},
  urldate = {2022-08-04},
  abstract = {Goal-conditioned Reinforcement Learning (RL) aims at learning optimal policies, given goals encoded in special command inputs. Here we study goal-conditioned neural nets (NNs) that learn to generate deep NN policies in form of context-specific weight matrices, similar to Fast Weight Programmers and other methods from the 1990s. Using context commands of the form "generate a policy that achieves a desired expected return," our NN generators combine powerful exploration of parameter space with generalization across commands to iteratively find better and better policies. A form of weight-sharing HyperNetworks and policy embeddings scales our method to generate deep NNs. Experiments show how a single learned policy generator can produce policies that achieve any return seen during training. Finally, we evaluate our algorithm on a set of continuous control tasks where it exhibits competitive performance. Our code is public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/928TQQPM/Faccio et al. - 2022 - Goal-Conditioned Generators of Deep Policies.pdf;/Users/alexandre.piche/Zotero/storage/KT4SWLTG/2207.html}
}

@misc{faulknerSamplingAlgorithmsStatistical2022,
  title = {Sampling Algorithms in Statistical Physics: A Guide for Statistics and Machine Learning},
  shorttitle = {Sampling Algorithms in Statistical Physics},
  author = {Faulkner, Michael F. and Livingstone, Samuel},
  date = {2022-08-09},
  number = {arXiv:2208.04751},
  eprint = {2208.04751},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.04751},
  urldate = {2022-12-13},
  abstract = {We discuss several algorithms for sampling from unnormalized probability distributions in statistical physics, but using the language of statistics and machine learning. We provide a self-contained introduction to some key ideas and concepts of the field, before discussing three well-known problems: phase transitions in the Ising model, the melting transition on a two-dimensional plane and simulation of an all-atom model for liquid water. We review the classical Metropolis, Glauber and molecular dynamics sampling algorithms before discussing several more recent approaches, including cluster algorithms, novel variations of hybrid Monte Carlo and Langevin dynamics and piece-wise deterministic processes such as event chain Monte Carlo. We highlight cross-over with statistics and machine learning throughout and present some results on event chain Monte Carlo and sampling from the Ising model using tools from the statistics literature. We provide a simulation study on the Ising and XY models, with reproducible code freely available online, and following this we discuss several open areas for interaction between the disciplines that have not yet been explored and suggest avenues for doing so.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Statistics - Computation},
  file = {/Users/alexandre.piche/Zotero/storage/YHK7QCWB/Faulkner and Livingstone - 2022 - Sampling algorithms in statistical physics a guid.pdf}
}

@misc{federCausalInferenceNatural2022,
  title = {Causal {{Inference}} in {{Natural Language Processing}}: {{Estimation}}, {{Prediction}}, {{Interpretation}} and {{Beyond}}},
  shorttitle = {Causal {{Inference}} in {{Natural Language Processing}}},
  author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  date = {2022-07-30},
  number = {arXiv:2109.00725},
  eprint = {2109.00725},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.00725},
  urldate = {2022-09-30},
  abstract = {A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/6YNCN9CZ/Feder et al. - 2022 - Causal Inference in Natural Language Processing E.pdf;/Users/alexandre.piche/Zotero/storage/L2YFYIJ9/2109.html}
}

@misc{federCausalInferenceNatural2022a,
  title = {Causal {{Inference}} in {{Natural Language Processing}}: {{Estimation}}, {{Prediction}}, {{Interpretation}} and {{Beyond}}},
  shorttitle = {Causal {{Inference}} in {{Natural Language Processing}}},
  author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  date = {2022-07-30},
  number = {arXiv:2109.00725},
  eprint = {2109.00725},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.00725},
  urldate = {2022-12-13},
  abstract = {A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/MLXVZGMP/Feder et al. - 2022 - Causal Inference in Natural Language Processing E.pdf}
}

@misc{finnConnectionGenerativeAdversarial2016,
  title = {A {{Connection}} between {{Generative Adversarial Networks}}, {{Inverse Reinforcement Learning}}, and {{Energy-Based Models}}},
  author = {Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  date = {2016-11-25},
  number = {arXiv:1611.03852},
  eprint = {1611.03852},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.03852},
  url = {http://arxiv.org/abs/1611.03852},
  urldate = {2022-12-13},
  abstract = {Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/YD575ZVL/Finn et al. - 2016 - A Connection between Generative Adversarial Networ.pdf;/Users/alexandre.piche/Zotero/storage/IJP2VIMC/1611.html}
}

@misc{florenceImplicitBehavioralCloning2021,
  title = {Implicit {{Behavioral Cloning}}},
  author = {Florence, Pete and Lynch, Corey and Zeng, Andy and Ramirez, Oscar and Wahid, Ayzaan and Downs, Laura and Wong, Adrian and Lee, Johnny and Mordatch, Igor and Tompson, Jonathan},
  date = {2021-08-31},
  number = {arXiv:2109.00137},
  eprint = {2109.00137},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.00137},
  urldate = {2022-07-27},
  abstract = {We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models. We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. On robotic policy learning tasks we show that implicit behavioral cloning policies with energy-based models (EBM) often outperform common explicit (Mean Square Error, or Mixture Density) behavioral cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. We find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the D4RL benchmark suite, despite using no reward information. In the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/JU7737RJ/Florence et al. - 2021 - Implicit Behavioral Cloning.pdf;/Users/alexandre.piche/Zotero/storage/BTVFHVNR/2109.html}
}

@misc{florenceImplicitBehavioralCloning2021a,
  title = {Implicit {{Behavioral Cloning}}},
  author = {Florence, Pete and Lynch, Corey and Zeng, Andy and Ramirez, Oscar and Wahid, Ayzaan and Downs, Laura and Wong, Adrian and Lee, Johnny and Mordatch, Igor and Tompson, Jonathan},
  date = {2021-08-31},
  number = {arXiv:2109.00137},
  eprint = {2109.00137},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.00137},
  urldate = {2022-08-07},
  abstract = {We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models. We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. On robotic policy learning tasks we show that implicit behavioral cloning policies with energy-based models (EBM) often outperform common explicit (Mean Square Error, or Mixture Density) behavioral cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. We find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the D4RL benchmark suite, despite using no reward information. In the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/MQLU7ZNJ/Florence et al. - 2021 - Implicit Behavioral Cloning.pdf;/Users/alexandre.piche/Zotero/storage/M7T7CXLH/2109.html}
}

@misc{franklinRecognisingImportancePreference2022,
  title = {Recognising the Importance of Preference Change: {{A}} Call for a Coordinated Multidisciplinary Research Effort in the Age of {{AI}}},
  shorttitle = {Recognising the Importance of Preference Change},
  author = {Franklin, Matija and Ashton, Hal and Gorman, Rebecca and Armstrong, Stuart},
  date = {2022-03-30},
  number = {arXiv:2203.10525},
  eprint = {2203.10525},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.10525},
  urldate = {2022-11-07},
  abstract = {As artificial intelligence becomes more powerful and a ubiquitous presence in daily life, it is imperative to understand and manage the impact of AI systems on our lives and decisions. Modern ML systems often change user behavior (e.g. personalized recommender systems learn user preferences to deliver recommendations that change online behavior). An externality of behavior change is preference change. This article argues for the establishment of a multidisciplinary endeavor focused on understanding how AI systems change preference: Preference Science. We operationalize preference to incorporate concepts from various disciplines, outlining the importance of meta-preferences and preference-change preferences, and proposing a preliminary framework for how preferences change. We draw a distinction between preference change, permissible preference change, and outright preference manipulation. A diversity of disciplines contribute unique insights to this framework.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/alexandre.piche/Zotero/storage/FYDFCV2W/Franklin et al. - 2022 - Recognising the importance of preference change A.pdf;/Users/alexandre.piche/Zotero/storage/H9IUUXFM/2203.html}
}

@misc{ganguliRedTeamingLanguage2022,
  title = {Red {{Teaming Language Models}} to {{Reduce Harms}}: {{Methods}}, {{Scaling Behaviors}}, and {{Lessons Learned}}},
  shorttitle = {Red {{Teaming Language Models}} to {{Reduce Harms}}},
  author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and El-Showk, Sheer and Fort, Stanislav and Hatfield-Dodds, Zac and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and Tran-Johnson, Eli and Amodei, Dario and Brown, Tom and Joseph, Nicholas and McCandlish, Sam and Olah, Chris and Kaplan, Jared and Clark, Jack},
  date = {2022-11-22},
  number = {arXiv:2209.07858},
  eprint = {2209.07858},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.07858},
  url = {http://arxiv.org/abs/2209.07858},
  urldate = {2022-12-13},
  abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/alexandre.piche/Zotero/storage/K84BKZCM/Ganguli et al. - 2022 - Red Teaming Language Models to Reduce Harms Metho.pdf;/Users/alexandre.piche/Zotero/storage/JNTY9T3B/2209.html}
}

@misc{gaoScalingLawsReward2022,
  title = {Scaling {{Laws}} for {{Reward Model Overoptimization}}},
  author = {Gao, Leo and Schulman, John and Hilton, Jacob},
  date = {2022-10-19},
  number = {arXiv:2210.10760},
  eprint = {2210.10760},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.10760},
  urldate = {2022-10-27},
  abstract = {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed "gold-standard" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-\$n\$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/D6F7SGRP/Gao et al. - 2022 - Scaling Laws for Reward Model Overoptimization.pdf;/Users/alexandre.piche/Zotero/storage/2JXRXPNA/2210.html}
}

@misc{gargWhatCanTransformers2022,
  title = {What {{Can Transformers Learn In-Context}}? {{A Case Study}} of {{Simple Function Classes}}},
  shorttitle = {What {{Can Transformers Learn In-Context}}?},
  author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  date = {2022-08-01},
  number = {arXiv:2208.01066},
  eprint = {2208.01066},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.01066},
  urldate = {2022-12-13},
  abstract = {In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/M8CA29YT/Garg et al. - 2022 - What Can Transformers Learn In-Context A Case Stu.pdf}
}

@misc{geipingCrammingTrainingLanguage2022,
  title = {Cramming: {{Training}} a {{Language Model}} on a {{Single GPU}} in {{One Day}}},
  shorttitle = {Cramming},
  author = {Geiping, Jonas and Goldstein, Tom},
  date = {2022-12-28},
  number = {arXiv:2212.14034},
  eprint = {2212.14034},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.14034},
  url = {http://arxiv.org/abs/2212.14034},
  urldate = {2022-12-29},
  abstract = {Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/Z5HE9PPD/Geiping and Goldstein - 2022 - Cramming Training a Language Model on a Single GPU in One Day.pdf}
}

@misc{ghoshOfflineRLPolicies2022,
  title = {Offline {{RL Policies Should}} Be {{Trained}} to Be {{Adaptive}}},
  author = {Ghosh, Dibya and Ajay, Anurag and Agrawal, Pulkit and Levine, Sergey},
  date = {2022-07-05},
  number = {arXiv:2207.02200},
  eprint = {2207.02200},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.02200},
  urldate = {2022-07-18},
  abstract = {Offline RL algorithms must account for the fact that the dataset they are provided may leave many facets of the environment unknown. The most common way to approach this challenge is to employ pessimistic or conservative methods, which avoid behaviors that are too dissimilar from those in the training dataset. However, relying exclusively on conservatism has drawbacks: performance is sensitive to the exact degree of conservatism, and conservative objectives can recover highly suboptimal policies. In this work, we propose that offline RL methods should instead be adaptive in the presence of uncertainty. We show that acting optimally in offline RL in a Bayesian sense involves solving an implicit POMDP. As a result, optimal policies for offline RL must be adaptive, depending not just on the current state but rather all the transitions seen so far during evaluation.We present a model-free algorithm for approximating this optimal adaptive policy, and demonstrate the efficacy of learning such adaptive policies in offline RL benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/F6THD3SU/Ghosh et al. - 2022 - Offline RL Policies Should be Trained to be Adapti.pdf;/Users/alexandre.piche/Zotero/storage/VNFGDEJI/2207.html}
}

@misc{ghoshRepresentationsStableOffPolicy2020,
  title = {Representations for {{Stable Off-Policy Reinforcement Learning}}},
  author = {Ghosh, Dibya and Bellemare, Marc G.},
  date = {2020-10-02},
  number = {arXiv:2007.05520},
  eprint = {2007.05520},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2007.05520},
  urldate = {2022-08-07},
  abstract = {Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical TD algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/337F8ELM/Ghosh and Bellemare - 2020 - Representations for Stable Off-Policy Reinforcemen.pdf;/Users/alexandre.piche/Zotero/storage/8XIQ2Z2V/2007.html}
}

@misc{ghoshWhyGeneralizationRL2021,
  title = {Why {{Generalization}} in {{RL}} Is {{Difficult}}: {{Epistemic POMDPs}} and {{Implicit Partial Observability}}},
  shorttitle = {Why {{Generalization}} in {{RL}} Is {{Difficult}}},
  author = {Ghosh, Dibya and Rahme, Jad and Kumar, Aviral and Zhang, Amy and Adams, Ryan P. and Levine, Sergey},
  date = {2021-07-13},
  number = {arXiv:2107.06277},
  eprint = {2107.06277},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2107.06277},
  urldate = {2022-07-18},
  abstract = {Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/D2PFZWNX/Ghosh et al. - 2021 - Why Generalization in RL is Difficult Epistemic P.pdf;/Users/alexandre.piche/Zotero/storage/KK2YXVJS/2107.html}
}

@misc{glaeseImprovingAlignmentDialogue2022,
  title = {Improving Alignment of Dialogue Agents via Targeted Human Judgements},
  author = {Glaese, Amelia and McAleese, Nat and Trębacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and Campbell-Gillingham, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez and Green, Richard and Mokrá, Soňa and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
  date = {2022-09-28},
  number = {arXiv:2209.14375},
  eprint = {2209.14375},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.14375},
  urldate = {2022-10-27},
  abstract = {We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78\% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8\% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/NS5MQS95/Glaese et al. - 2022 - Improving alignment of dialogue agents via targete.pdf;/Users/alexandre.piche/Zotero/storage/XVVFRV8U/2209.html}
}

@misc{guoEfficientSoftQLearning2022,
  title = {Efficient ({{Soft}}) {{Q-Learning}} for {{Text Generation}} with {{Limited Good Data}}},
  author = {Guo, Han and Tan, Bowen and Liu, Zhengzhong and Xing, Eric P. and Hu, Zhiting},
  date = {2022-10-22},
  number = {arXiv:2106.07704},
  eprint = {2106.07704},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.07704},
  url = {http://arxiv.org/abs/2106.07704},
  urldate = {2022-12-13},
  abstract = {Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/7PNWVRB6/Guo et al. - 2022 - Efficient (Soft) Q-Learning for Text Generation wi.pdf;/Users/alexandre.piche/Zotero/storage/MRUB8J2A/2106.html}
}

@misc{guptaInstructDialImprovingZero2022,
  title = {{{InstructDial}}: {{Improving Zero}} and {{Few-shot Generalization}} in {{Dialogue}} through {{Instruction Tuning}}},
  shorttitle = {{{InstructDial}}},
  author = {Gupta, Prakhar and Jiao, Cathy and Yeh, Yi-Ting and Mehri, Shikib and Eskenazi, Maxine and Bigham, Jeffrey P.},
  date = {2022-10-26},
  number = {arXiv:2205.12673},
  eprint = {2205.12673},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.12673},
  url = {http://arxiv.org/abs/2205.12673},
  urldate = {2022-12-13},
  abstract = {Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/8ZC78M68/Gupta et al. - 2022 - InstructDial Improving Zero and Few-shot Generali.pdf;/Users/alexandre.piche/Zotero/storage/CCE8QPKD/2205.html}
}

@inproceedings{gutmannNoisecontrastiveEstimationNew2010,
  title = {Noise-Contrastive Estimation: {{A}} New Estimation Principle for Unnormalized Statistical Models},
  shorttitle = {Noise-Contrastive Estimation},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gutmann, Michael and Hyvärinen, Aapo},
  date = {2010-03-31},
  pages = {297--304},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/gutmann10a.html},
  urldate = {2022-08-14},
  abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/CAJZJE2L/Gutmann and Hyvärinen - 2010 - Noise-contrastive estimation A new estimation pri.pdf}
}

@misc{guyerWillMyRobot2022,
  title = {Will {{My Robot Achieve My Goals}}? {{Predicting}} the {{Probability}} That an {{MDP Policy Reaches}} a {{User-Specified Behavior Target}}},
  shorttitle = {Will {{My Robot Achieve My Goals}}?},
  author = {Guyer, Alexander and Dietterich, Thomas G.},
  date = {2022-11-29},
  number = {arXiv:2211.16462},
  eprint = {2211.16462},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2211.16462},
  url = {http://arxiv.org/abs/2211.16462},
  urldate = {2022-12-17},
  abstract = {As an autonomous system performs a task, it should maintain a calibrated estimate of the probability that it will achieve the user's goal. If that probability falls below some desired level, it should alert the user so that appropriate interventions can be made. This paper considers settings where the user's goal is specified as a target interval for a real-valued performance summary, such as the cumulative reward, measured at a fixed horizon \$H\$. At each time \$t \textbackslash in \textbackslash\{0, \textbackslash ldots, H-1\textbackslash\}\$, our method produces a calibrated estimate of the probability that the final cumulative reward will fall within a user-specified target interval \$[y\^-,y\^+].\$ Using this estimate, the autonomous system can raise an alarm if the probability drops below a specified threshold. We compute the probability estimates by inverting conformal prediction. Our starting point is the Conformalized Quantile Regression (CQR) method of Romano et al., which applies split-conformal prediction to the results of quantile regression. CQR is not invertible, but by using the conditional cumulative distribution function (CDF) as the non-conformity measure, we show how to obtain an invertible modification that we call \textbackslash textbf\{P\}robability-space \textbackslash textbf\{C\}onformalized \textbackslash textbf\{Q\}uantile \textbackslash textbf\{R\}egression (PCQR). Like CQR, PCQR produces well-calibrated conditional prediction intervals with finite-sample marginal guarantees. By inverting PCQR, we obtain marginal guarantees for the probability that the cumulative reward of an autonomous system will fall within an arbitrary user-specified target intervals. Experiments on two domains confirm that these probabilities are well-calibrated.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/alexandre.piche/Zotero/storage/XD7F5YWP/Guyer and Dietterich - 2022 - Will My Robot Achieve My Goals Predicting the Probability that an MDP Policy Reaches a User-Specified Behavior Target.pdf}
}

@misc{haoStructuredPromptingScaling2022,
  title = {Structured {{Prompting}}: {{Scaling In-Context Learning}} to 1,000 {{Examples}}},
  shorttitle = {Structured {{Prompting}}},
  author = {Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},
  date = {2022-12-13},
  number = {arXiv:2212.06713},
  eprint = {2212.06713},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.06713},
  url = {http://arxiv.org/abs/2212.06713},
  urldate = {2022-12-19},
  abstract = {Large language models have exhibited intriguing in-context learning capability, achieving promising zero- and few-shot performance without updating the parameters. However, conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Specifically, demonstration examples are separately encoded with well-designed position embeddings, and then they are jointly attended by the test example using a rescaled attention mechanism. So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at https://aka.ms/structured-prompting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/LBI9XZ6J/Hao et al. - 2022 - Structured Prompting Scaling In-Context Learning to 1,000 Examples.pdf}
}

@misc{hejnaFewShotPreferenceLearning2022,
  title = {Few-{{Shot Preference Learning}} for {{Human-in-the-Loop RL}}},
  author = {Hejna, Joey and Sadigh, Dorsa},
  date = {2022-12-06},
  number = {arXiv:2212.03363},
  eprint = {2212.03363},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.03363},
  url = {http://arxiv.org/abs/2212.03363},
  urldate = {2022-12-13},
  abstract = {While reinforcement learning (RL) has become a more popular approach for robotics, designing sufficiently informative reward functions for complex tasks has proven to be extremely difficult due their inability to capture human intent and policy exploitation. Preference based RL algorithms seek to overcome these challenges by directly learning reward functions from human feedback. Unfortunately, prior work either requires an unreasonable number of queries implausible for any human to answer or overly restricts the class of reward functions to guarantee the elicitation of the most informative queries, resulting in models that are insufficiently expressive for realistic robotics tasks. Contrary to most works that focus on query selection to \textbackslash emph\{minimize\} the amount of data required for learning reward functions, we take an opposite approach: \textbackslash emph\{expanding\} the pool of available data by viewing human-in-the-loop RL through the more flexible lens of multi-task learning. Motivated by the success of meta-learning, we pre-train preference models on prior task data and quickly adapt them for new tasks using only a handful of queries. Empirically, we reduce the amount of online feedback needed to train manipulation policies in Meta-World by 20\$\textbackslash times\$, and demonstrate the effectiveness of our method on a real Franka Panda Robot. Moreover, this reduction in query-complexity allows us to train robot policies from actual human users. Videos of our results and code can be found at https://sites.google.com/view/few-shot-preference-rl/home.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/IIKT8A2E/Hejna and Sadigh - 2022 - Few-Shot Preference Learning for Human-in-the-Loop.pdf;/Users/alexandre.piche/Zotero/storage/STZ27NEW/2212.html}
}

@inproceedings{hoevenManyFacesExponential2018,
  title = {The {{Many Faces}} of {{Exponential Weights}} in {{Online Learning}}},
  booktitle = {Proceedings of the 31st  {{Conference On Learning Theory}}},
  author = {Hoeven, Dirk and Erven, Tim and Kotłowski, Wojciech},
  date = {2018-07-03},
  pages = {2067--2092},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v75/hoeven18a.html},
  urldate = {2022-11-24},
  abstract = {A standard introduction to online learning might place Online Gradient Descent at its center and then proceed to develop generalizations and extensions like Online Mirror Descent and second-order methods. Here we explore the alternative approach of putting Exponential Weights (EW) first. We show that many standard methods and their regret bounds then follow as a special case by plugging in suitable surrogate losses and playing the EW posterior mean. For instance, we easily recover Online Gradient Descent by using EW with a Gaussian prior on linearized losses, and, more generally, all instances of Online Mirror Descent based on regular Bregman divergences also correspond to EW with a prior that depends on the mirror map. Furthermore, appropriate quadratic surrogate losses naturally give rise to Online Gradient Descent for strongly convex losses and to Online Newton Step. We further interpret several recent adaptive methods (iProd, Squint, and a variation of Coin Betting for experts) as a series of closely related reductions to exp-concave surrogate losses that are then handled by Exponential Weights. Finally, a benefit of our EW interpretation is that it opens up the possibility of sampling from the EW posterior distribution instead of playing the mean. As already observed by Bubeck and Eldan, this recovers the best-known rate in Online Bandit Linear Optimization.},
  eventtitle = {Conference {{On Learning Theory}}},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/UU4WTD6J/Hoeven et al. - 2018 - The Many Faces of Exponential Weights in Online Le.pdf}
}

@misc{hongSensitivityRewardInference2022,
  title = {On the {{Sensitivity}} of {{Reward Inference}} to {{Misspecified Human Models}}},
  author = {Hong, Joey and Bhatia, Kush and Dragan, Anca},
  date = {2022-12-09},
  number = {arXiv:2212.04717},
  eprint = {2212.04717},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.04717},
  url = {http://arxiv.org/abs/2212.04717},
  urldate = {2022-12-19},
  abstract = {Inferring reward functions from human behavior is at the center of value alignment - aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases in behavior that lead to arbitrarily large errors in the inferred reward. However, and arguably more importantly, we are also able to identify reasonable assumptions under which the reward inference error can be bounded linearly in the error in the human model. Finally, we verify our theoretical insights in discrete and continuous control tasks with simulated and human data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/YDDXXTQB/Hong et al. - 2022 - On the Sensitivity of Reward Inference to Misspeci.pdf;/Users/alexandre.piche/Zotero/storage/J7D6CNIW/2212.html}
}

@misc{hoqueThriftyDAggerBudgetAwareNovelty2021,
  title = {{{ThriftyDAgger}}: {{Budget-Aware Novelty}} and {{Risk Gating}} for {{Interactive Imitation Learning}}},
  shorttitle = {{{ThriftyDAgger}}},
  author = {Hoque, Ryan and Balakrishna, Ashwin and Novoseller, Ellen and Wilcox, Albert and Brown, Daniel S. and Goldberg, Ken},
  date = {2021-09-16},
  number = {arXiv:2109.08273},
  eprint = {2109.08273},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.08273},
  urldate = {2022-12-09},
  abstract = {Effective robot learning often requires online human feedback and interventions that can cost significant human time, giving rise to the central challenge in interactive imitation learning: is it possible to control the timing and length of interventions to both facilitate learning and limit burden on the human supervisor? This paper presents ThriftyDAgger, an algorithm for actively querying a human supervisor given a desired budget of human interventions. ThriftyDAgger uses a learned switching policy to solicit interventions only at states that are sufficiently (1) novel, where the robot policy has no reference behavior to imitate, or (2) risky, where the robot has low confidence in task completion. To detect the latter, we introduce a novel metric for estimating risk under the current robot policy. Experiments in simulation and on a physical cable routing experiment suggest that ThriftyDAgger's intervention criteria balances task performance and supervisor burden more effectively than prior algorithms. ThriftyDAgger can also be applied at execution time, where it achieves a 100\% success rate on both the simulation and physical tasks. A user study (N=10) in which users control a three-robot fleet while also performing a concentration task suggests that ThriftyDAgger increases human and robot performance by 58\% and 80\% respectively compared to the next best algorithm while reducing supervisor burden.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/PKBTQ3TS/Hoque et al. - 2021 - ThriftyDAgger Budget-Aware Novelty and Risk Gatin.pdf;/Users/alexandre.piche/Zotero/storage/XX6RMZ72/2109.html}
}

@online{HowCanWe2022,
  title = {How {{Can We Make Robotics More}} like {{Generative Modeling}}?},
  date = {2022-07-23T00:00:00+00:00},
  url = {https://evjang.com/2022/07/23/robotics-generative.html},
  urldate = {2022-08-13},
  abstract = {I recently gave a talk (YouTube) at the RSS’22 L-DOD workshop. Here’s a lightly edited transcript and slides of the talk in blog form.},
  langid = {english},
  organization = {{Eric Jang}},
  file = {/Users/alexandre.piche/Zotero/storage/MMJKR7B9/robotics-generative.html}
}

@inproceedings{huangConstrainedMultiObjectiveReinforcement2022,
  title = {A {{Constrained Multi-Objective Reinforcement Learning Framework}}},
  booktitle = {Proceedings of the 5th {{Conference}} on {{Robot Learning}}},
  author = {Huang, Sandy and Abdolmaleki, Abbas and Vezzani, Giulia and Brakel, Philemon and Mankowitz, Daniel J. and Neunert, Michael and Bohez, Steven and Tassa, Yuval and Heess, Nicolas and Riedmiller, Martin and Hadsell, Raia},
  date = {2022-01-11},
  pages = {883--893},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v164/huang22a.html},
  urldate = {2022-12-13},
  abstract = {Many real-world problems, especially in robotics, require that reinforcement learning (RL) agents learn policies that not only maximize an environment reward, but also satisfy constraints. We propose a high-level framework for solving such problems, that treats the environment reward and costs as separate objectives, and learns what preference over objectives the policy should optimize for in order to meet the constraints. We call this Learning Preferences and Policies in Parallel (LP3). By making different choices for how to learn the preference and how to optimize for the policy given the preference, we can obtain existing approaches (e.g., Lagrangian relaxation) and derive novel approaches that lead to better performance. One of these is an algorithm that learns a set of constraint-satisfying policies, useful for when we do not know the exact constraint a priori.},
  eventtitle = {Conference on {{Robot Learning}}},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/6R9RK4NY/Huang et al. - 2022 - A Constrained Multi-Objective Reinforcement Learni.pdf}
}

@misc{huangVariationalPerspectiveDiffusionBased2021,
  title = {A {{Variational Perspective}} on {{Diffusion-Based Generative Models}} and {{Score Matching}}},
  author = {Huang, Chin-Wei and Lim, Jae Hyun and Courville, Aaron},
  date = {2021-09-29},
  number = {arXiv:2106.02808},
  eprint = {2106.02808},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.02808},
  urldate = {2022-12-11},
  abstract = {Discrete-time diffusion-based generative models and score matching methods have shown promising results in modeling high-dimensional image data. Recently, Song et al. (2021) show that diffusion processes that transform data into noise can be reversed via learning the score function, i.e. the gradient of the log-density of the perturbed data. They propose to plug the learned score function into an inverse formula to define a generative diffusion process. Despite the empirical success, a theoretical underpinning of this procedure is still lacking. In this work, we approach the (continuous-time) generative diffusion directly and derive a variational framework for likelihood estimation, which includes continuous-time normalizing flows as a special case, and can be seen as an infinitely deep variational autoencoder. Under this framework, we show that minimizing the score-matching loss is equivalent to maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed by Song et al. (2021), bridging the theoretical gap.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/Z2DNWGHW/Huang et al. - 2021 - A Variational Perspective on Diffusion-Based Gener.pdf;/Users/alexandre.piche/Zotero/storage/ABMVU5X6/2106.html}
}

@misc{humphreysDatadrivenApproachLearning2022,
  title = {A Data-Driven Approach for Learning to Control Computers},
  author = {Humphreys, Peter C. and Raposo, David and Pohlen, Toby and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Goldin, Alex and Santoro, Adam and Lillicrap, Timothy},
  date = {2022-02-16},
  number = {arXiv:2202.08137},
  eprint = {2202.08137},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.08137},
  urldate = {2022-08-04},
  abstract = {It would be useful for machines to use computers as humans do so that they can aid us in everyday tasks. This is a setting in which there is also the potential to leverage large-scale expert demonstrations and human judgements of interactive behaviour, which are two ingredients that have driven much recent success in AI. Here we investigate the setting of computer control using keyboard and mouse, with goals specified via natural language. Instead of focusing on hand-designed curricula and specialized action spaces, we focus on developing a scalable method centered on reinforcement learning combined with behavioural priors informed by actual human-computer interactions. We achieve state-of-the-art and human-level mean performance across all tasks within the MiniWob++ benchmark, a challenging suite of computer control problems, and find strong evidence of cross-task transfer. These results demonstrate the usefulness of a unified human-agent interface when training machines to use computers. Altogether our results suggest a formula for achieving competency beyond MiniWob++ and towards controlling computers, in general, as a human would.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/KSW5XAE3/Humphreys et al. - 2022 - A data-driven approach for learning to control com.pdf;/Users/alexandre.piche/Zotero/storage/ALBT9E39/2202.html}
}

@misc{huszarHowNotTrain2015,
  title = {How (Not) to {{Train}} Your {{Generative Model}}: {{Scheduled Sampling}}, {{Likelihood}}, {{Adversary}}?},
  shorttitle = {How (Not) to {{Train}} Your {{Generative Model}}},
  author = {Huszár, Ferenc},
  date = {2015-11-16},
  number = {arXiv:1511.05101},
  eprint = {1511.05101},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.05101},
  url = {http://arxiv.org/abs/1511.05101},
  urldate = {2022-12-13},
  abstract = {Modern applications and progress in deep learning research have created renewed interest for generative models of text and of images. However, even today it is unclear what objective functions one should use to train and evaluate these models. In this paper we present two contributions. Firstly, we present a critique of scheduled sampling, a state-of-the-art training method that contributed to the winning entry to the MSCOCO image captioning benchmark in 2015. Here we show that despite this impressive empirical performance, the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm. Secondly, we revisit the problems that scheduled sampling was meant to address, and present an alternative interpretation. We argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples. We go on to derive an ideal objective function to use in this situation instead. We introduce a generalisation of adversarial training, and show how such method can interpolate between maximum likelihood training and our ideal training objective. To our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/S3E47V8T/Huszár - 2015 - How (not) to Train your Generative Model Schedule.pdf;/Users/alexandre.piche/Zotero/storage/JMICC4JU/1511.html}
}

@article{icarteRewardMachinesExploiting2022,
  title = {Reward {{Machines}}: {{Exploiting Reward Function Structure}} in {{Reinforcement Learning}}},
  shorttitle = {Reward {{Machines}}},
  author = {Icarte, Rodrigo Toro and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
  date = {2022-01-11},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {73},
  eprint = {2010.03950},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {173--208},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12440},
  url = {http://arxiv.org/abs/2010.03950},
  urldate = {2022-12-21},
  abstract = {Reinforcement learning (RL) methods usually treat reward functions as black boxes. As such, these methods must extensively interact with the environment in order to discover rewards and optimal policies. In most RL applications, however, users have to program the reward function and, hence, there is the opportunity to make the reward function visible -- to show the reward function's code to the RL agent so it can exploit the function's internal structure to learn optimal policies in a more sample efficient manner. In this paper, we show how to accomplish this idea in two steps. First, we propose reward machines, a type of finite state machine that supports the specification of reward functions while exposing reward function structure. We then describe different methodologies to exploit this structure to support learning, including automated reward shaping, task decomposition, and counterfactual reasoning with off-policy learning. Experiments on tabular and continuous domains, across different tasks and RL agents, show the benefits of exploiting reward structure with respect to sample efficiency and the quality of resultant policies. Finally, by virtue of being a form of finite state machine, reward machines have the expressive power of a regular language and as such support loops, sequences and conditionals, as well as the expression of temporally extended properties typical of linear temporal logic and non-Markovian reward specification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/EHZK4ALC/Icarte et al. - 2022 - Reward Machines Exploiting Reward Function Structure in Reinforcement Learning.pdf}
}

@misc{imaniImprovingRegressionPerformance2018,
  title = {Improving {{Regression Performance}} with {{Distributional Losses}}},
  author = {Imani, Ehsan and White, Martha},
  date = {2018-06-12},
  number = {arXiv:1806.04613},
  eprint = {1806.04613},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.04613},
  url = {http://arxiv.org/abs/1806.04613},
  urldate = {2022-12-13},
  abstract = {There is growing evidence that converting targets to soft targets in supervised learning can provide considerable gains in performance. Much of this work has considered classification, converting hard zero-one values to soft labels---such as by adding label noise, incorporating label ambiguity or using distillation. In parallel, there is some evidence from a regression setting in reinforcement learning that learning distributions can improve performance. In this work, we investigate the reasons for this improvement, in a regression setting. We introduce a novel distributional regression loss, and similarly find it significantly improves prediction accuracy. We investigate several common hypotheses, around reducing overfitting and improved representations. We instead find evidence for an alternative hypothesis: this loss is easier to optimize, with better behaved gradients, resulting in improved generalization. We provide theoretical support for this alternative hypothesis, by characterizing the norm of the gradients of this loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/KWJU6E52/Imani and White - 2018 - Improving Regression Performance with Distribution.pdf;/Users/alexandre.piche/Zotero/storage/VW4JPQFD/1806.html}
}

@misc{irieDualFormNeural2022,
  title = {The {{Dual Form}} of {{Neural Networks Revisited}}: {{Connecting Test Time Predictions}} to {{Training Patterns}} via {{Spotlights}} of {{Attention}}},
  shorttitle = {The {{Dual Form}} of {{Neural Networks Revisited}}},
  author = {Irie, Kazuki and Csordás, Róbert and Schmidhuber, Jürgen},
  date = {2022-06-17},
  number = {arXiv:2202.05798},
  eprint = {2202.05798},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.05798},
  urldate = {2022-08-04},
  abstract = {Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/R74F5Q46/Irie et al. - 2022 - The Dual Form of Neural Networks Revisited Connec.pdf;/Users/alexandre.piche/Zotero/storage/46YM9XJR/2202.html}
}

@misc{izacardAtlasFewshotLearning2022,
  title = {Atlas: {{Few-shot Learning}} with {{Retrieval Augmented Language Models}}},
  shorttitle = {Atlas},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  date = {2022-11-16},
  number = {arXiv:2208.03299},
  eprint = {2208.03299},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.03299},
  urldate = {2022-12-13},
  abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/SPZVGW27/Izacard et al. - 2022 - Atlas Few-shot Learning with Retrieval Augmented .pdf}
}

@misc{jabbariFairnessReinforcementLearning2017,
  title = {Fairness in {{Reinforcement Learning}}},
  author = {Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Roth, Aaron},
  date = {2017-08-05},
  number = {arXiv:1611.03071},
  eprint = {1611.03071},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1611.03071},
  urldate = {2022-12-13},
  abstract = {We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/INRFYKBV/Jabbari et al. - 2017 - Fairness in Reinforcement Learning.pdf}
}

@misc{jainDataBasedPerspectiveTransfer2022,
  title = {A {{Data-Based Perspective}} on {{Transfer Learning}}},
  author = {Jain, Saachi and Salman, Hadi and Khaddaj, Alaa and Wong, Eric and Park, Sung Min and Madry, Aleksander},
  date = {2022-07-12},
  number = {arXiv:2207.05739},
  eprint = {2207.05739},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.05739},
  urldate = {2022-07-19},
  abstract = {It is commonly believed that in transfer learning including more pre-training data translates into better performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we take a closer look at the role of the source dataset's composition in transfer learning and present a framework for probing its impact on downstream performance. Our framework gives rise to new capabilities such as pinpointing transfer learning brittleness as well as detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset. In particular, we demonstrate that removing detrimental datapoints identified by our framework improves transfer learning performance from ImageNet on a variety of target tasks. Code is available at https://github.com/MadryLab/data-transfer},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/MYKZ2YS9/Jain et al. - 2022 - A Data-Based Perspective on Transfer Learning.pdf;/Users/alexandre.piche/Zotero/storage/CSAETLVK/2207.html}
}

@misc{jeonRewardrationalImplicitChoice2020,
  title = {Reward-Rational (Implicit) Choice: {{A}} Unifying Formalism for Reward Learning},
  shorttitle = {Reward-Rational (Implicit) Choice},
  author = {Jeon, Hong Jun and Milli, Smitha and Dragan, Anca D.},
  date = {2020-12-11},
  number = {arXiv:2002.04833},
  eprint = {2002.04833},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2002.04833},
  urldate = {2022-11-11},
  abstract = {It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We’ve gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key observation is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. We use this formalism to survey prior work through a unifying lens, and discuss its potential use as a recipe for interpreting new sources of information that are yet to be uncovered.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/GLAVUS44/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf}
}

@misc{jiangGeneralIntelligenceRequires2022,
  title = {General {{Intelligence Requires Rethinking Exploration}}},
  author = {Jiang, Minqi and Rocktäschel, Tim and Grefenstette, Edward},
  date = {2022-11-14},
  number = {arXiv:2211.07819},
  eprint = {2211.07819},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2211.07819},
  urldate = {2022-11-17},
  abstract = {We are at the cusp of a transition from "learning from data" to "learning what data to learn from" as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train our models to how to effectively acquire and use task-relevant data. This problem, which we frame as exploration, is a universal aspect of learning in open-ended domains, such as the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of generalized exploration to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration serves as a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/WK72EART/Jiang et al. - 2022 - General Intelligence Requires Rethinking Explorati.pdf;/Users/alexandre.piche/Zotero/storage/Q9VERLU9/2211.html}
}

@misc{jiangGeneralIntelligenceRequires2022a,
  title = {General {{Intelligence Requires Rethinking Exploration}}},
  author = {Jiang, Minqi and Rocktäschel, Tim and Grefenstette, Edward},
  date = {2022-11-14},
  number = {arXiv:2211.07819},
  eprint = {2211.07819},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2211.07819},
  urldate = {2022-11-17},
  abstract = {We are at the cusp of a transition from "learning from data" to "learning what data to learn from" as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train our models to how to effectively acquire and use task-relevant data. This problem, which we frame as exploration, is a universal aspect of learning in open-ended domains, such as the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of generalized exploration to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration serves as a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@online{JustAskGeneralization2021,
  title = {Just {{Ask}} for {{Generalization}}},
  date = {2021-10-23T00:00:00+00:00},
  url = {https://evjang.com/2021/10/23/generalization.html},
  urldate = {2022-08-13},
  abstract = {This blog post outlines a key engineering principle I’ve come to believe strongly in for building general AI systems with deep learning. This principle guides my present-day research tastes and day-to-day design choices in building large-scale, general-purpose ML systems. Discoveries around Neural Scaling Laws, unsupervised pretraining on Internet-scale datasets, and other work on Foundation Models have pointed to a simple yet exciting narrative for making progress in Machine Learning: Large amounts of diverse data are more important to generalization than clever model biases. If you believe (1), then how much your model generalizes is directly proportional to how fast you can push diverse data into a sufficiently high-capacity model. To that end, Deep Neural nets trained with supervised learning are excellent data sponges - they can memorize vast amounts of data and can do this quickly by training with batch sizes in the tens of thousands. Modern architectures like ResNets and Transformers seem to have no trouble absorbing increasingly large datasets when trained via supervised learning. When a model has minimized training loss (a.k.a empirical risk), it can be said to have “memorized” the training set. Classically one would think that minimizing training loss to zero is shortly followed by overfitting, but overparameterized deep networks seem to generalize well even in this regime. Here is an illustration of the “double descent” phenomena from Patterns, Predictions, and Actions, which illustrates that in some problems, overparameterized models can continue to reduce test error (risk) even as training loss is fully minimized. A recent ICLR workshop paper investigates this phenomenon on synthetic datasets, showing that if you train long enough in this zero-training-loss regime, the model can suddenly have an epiphany and generalize much later on (the authors call this “Grokking”). Furthermore, the paper also presents evidence that increasing training data actually decreases the amount of optimization required to generalize. It’s as my colleague Chelsea Finn once told me: “Memorization is the first step towards generalization!” State-of-then-art neural networks trained this way can do really impressive things. Here is a DALL-E model that, when prompted with “A banana performing stand-up comedy”, draws the following picture: Here is another DALL-E output, prompted with “an illstration of a baby panda with headphones staring at its reflection in a mirror”. Note that there are no such images of “pandas looking into mirrors” or “banana comedians” in the training data (I think), so these results suggest that the DALL-E model has learned to interpret distinct concepts from text, render the corresponding visual parts in an image and have them interact with each other somewhat coherently. The ability to “just ask” language-conditioned deep learning models for what you want has led to “prompt engineering” as a viable space for improving our ML models. Here is a Tweet discussing how priming a VQGAN + CLIP model with the words “Unreal Engine” leads to drastically higher-quality images. What if we could extend this principle - just asking generalization - to other challenging problems that have eluded analytical algorithmic improvements? Reinforcement Learning: Not a Great Data Sponge In contrast to supervised learning, reinforcement learning algorithms are much less computationally efficient when it comes to absorbing vast quantities of diverse data needed for generalization. To see why this is the case, let’s consider a thought experiment where we train a general-purpose robot to do millions of tasks in unstructured environments. The standard Markov Decision Process is set up as follows: a policy is represented as a state-conditioned distribution over actions, \textbackslash (p(a \textbackslash vert s)\textbackslash ), and the environment as consisting of a reward function \textbackslash (r(s\_t, a\_t)\textbackslash ) and transition dynamics \textbackslash (p(s\_\{t+1\} \textbackslash vert s\_t, a\_t)\textbackslash ). Initial states and task objectives are encoded in the initial state \textbackslash (s\_0\textbackslash ), which is sampled from a distribution \textbackslash (p(s\_0)\textbackslash ). The goal is to maximize the sum of rewards across the episode, averaged across different starting states sampled from \textbackslash (p(s\_0)\textbackslash ): [\textbackslash DeclareMathOperator\{\textbackslash argmax\}\{arg\textbackslash,max\} \textbackslash DeclareMathOperator\{\textbackslash argmin\}\{arg\textbackslash,min\} \textbackslash text\{Solve\}\textasciitilde\textbackslash theta\^*\textbackslash{} = \textbackslash argmax\_\textbackslash theta\textasciitilde R(\textbackslash theta)] [\textbackslash text\{where\}\textasciitilde R(\textbackslash theta)=E\_\{p(s\_0)\}[\textbackslash sum\_\{t=1\}\^\{T\}\{r(s\_t, a\_t)\}]\textasciitilde\textbackslash text\{and\}\textasciitilde a\_t \textbackslash sim p\_\textbackslash theta(\textbackslash cdot s\_t)\textasciitilde\textbackslash text\{and\}\textasciitilde s\_\{t+1\} \textbackslash sim p(\textbackslash cdot s\_t, a\_t)\textasciitilde\textbackslash text\{and\}\textasciitilde s\_0 \textbackslash sim p(s\_0)] Let’s assume the existence of some optimal policy which we call \textbackslash (p\^\textbackslash star(a \textbackslash vert s)\textbackslash ), that achieves the maximum reward \textbackslash (\textbackslash max\_\textbackslash theta R(\textbackslash theta)\textbackslash ). “Supremum” would be more accurate, but I use the \textbackslash (\textbackslash max\textbackslash ) operator for notational simplicity. We want to bring our model, \textbackslash (p\_\textbackslash theta(a \textbackslash vert s)\textbackslash ), as close as possible to \textbackslash (p\^\textbackslash star(a \textbackslash vert s)\textbackslash ). If we had access to the optimal policy \textbackslash (p\^\textbackslash star(a \textbackslash vert s)\textbackslash ) as an oracle, we could simply query the oracle action and use it like a supervised learning label. We could then train a feedforward policy that maps the states to the oracle actions, and benefit from all the nice properties that supervised learning methods enjoy: stable training, large batches, diverse offline datasets, no need to interact with the environment. while not converged: batch\_states = replay\_buffer.sample(batch\_size) oracle\_actions = [oracle\_policy.sample\_action(s) for s in batch\_states] model.fit(batch\_states, oracle\_actions) However, in reinforcement learning we often don’t have an expert policy to query, so we must improve the policy from its own collected experience. To do this, estimating the gradient that takes the model policy closer to the optimal policy requires evaluating the average episodic return of the current policy in the environment, and then estimating a gradient of that return with respect to parameters. If you treat the environment returns as a black-box with respect to some parameter \textbackslash (\textbackslash theta\textbackslash ) you can use the log-derivative trick to estimate its gradients: [\textbackslash nabla\_\textbackslash theta E\_\{p(\textbackslash theta)\} [R(\textbackslash theta)] = \textbackslash int\_\textbackslash Theta d\textbackslash theta \textbackslash nabla\_\textbackslash theta p(\textbackslash theta) R(\textbackslash theta) \textbackslash{} = \textbackslash int\_\textbackslash Theta d\textbackslash theta p(\textbackslash theta) \textbackslash nabla\_\textbackslash theta \textbackslash log p(\textbackslash theta) R(\textbackslash theta) = E\_\{p(\textbackslash theta)\} [\textbackslash nabla\_\textbackslash theta \textbackslash log p(\textbackslash theta) R(\textbackslash theta)]] This gradient estimator contains two expectations that we need to numerically approximate. First is computing \textbackslash (R(\textbackslash theta)\textbackslash ) itself, which is an expectation over starting states \textbackslash (p(s\_0)\textbackslash ). In my previous blog post I mentioned that accurate evaluation of a Binomial variable (e.g. the success rate of a robot on a single task) could require thousands of trials in order to achieve statistical certainty within a couple percent. For our hypothetical generalist robot, \textbackslash (p(s\_0)\textbackslash ) could encompass millions of unique tasks and scenarios, which makes accurate evaluation prohibitively expensive. The second expectation is encountered in the estimation of the policy gradient, over \textbackslash (p(\textbackslash theta)\textbackslash ). Some algorithms like CMA-ES draw samples directly from the policy parameter distribution \textbackslash (p(\textbackslash theta)\textbackslash ), while other RL algorithms like PPO sample from the policy distribution \textbackslash (p\_\textbackslash theta(a\textbackslash vert s)\textbackslash ) and use the backpropagation rule to compute the gradient of the return with respect to the parameters: \textbackslash (\textbackslash frac\{\textbackslash partial R\}\{\textbackslash partial \textbackslash theta\} = \textbackslash frac\{\textbackslash partial R\}\{\textbackslash partial \textbackslash mu\_a\} \textbackslash cdot \textbackslash frac\{\textbackslash partial \textbackslash mu\_a\}\{\textbackslash partial \textbackslash theta\}\textbackslash ). The latter is typically preferred because the search space on action parameters is thought to be smaller than the search space on policy parameters (and therefore requires fewer environment interactions to estimate a gradient for). If supervised behavior cloning on a single oracle label \textbackslash (a \textbackslash sim p\^\textbackslash star(a\textbackslash vert s)\textbackslash ) gives you some gradient vector \textbackslash (g\^\textbackslash star\textbackslash ), estimating the same gradient vector \textbackslash (\textbackslash bar\{g\} \textbackslash approx g\^\textbackslash star\textbackslash ) with reinforcement learning requires something on the order of \textbackslash (O(H(s\_0) \textbackslash cdot H(a))\textbackslash ) times as many episode rollouts to get a comparably low-variance estimate. This is a hand-wavy estimate that assumes that there is a multiplicative factor of the entropy of the initial state distribution \textbackslash (O(H(s\_0))\textbackslash ) for estimating \textbackslash (R(\textbackslash theta)\textbackslash ) and a multiplicative factor of the entropy of the action distribution \textbackslash (O(H(a))\textbackslash ) for estimating \textbackslash (\textbackslash nabla\_\textbackslash theta R(\textbackslash theta)\textbackslash ) itself. Consequently, online reinforcement learning on sparse rewards and diverse, possibly multi-task environments require enormous numbers of rollouts to estimate returns and their gradients accurately. You have to pay this cost on every minibatch update! When the environment requires handling a wide variety of scenarios and demands generalization to unseen situations, it further increases the number of minibatch elements needed. The OpenAI DOTA team found that having millions of examples in their minibatch was required to bring down gradient noise to an acceptable level. This intuitively makes sense: if your objective \textbackslash (R(\textbackslash theta)\textbackslash ) has a minimum minibatch size needed to generalize well across many \textbackslash (s\_0\textbackslash ) without excessive catastrophic forgetting, then switching from supervised learning to online reinforcement learning will probably require a larger batch size by some multiplicative factor. What about Offline RL? What about offline RL methods like Deep Q-Learning on large datasets of \textbackslash ((S,A,R,S)\textbackslash ) transitions? These methods work by bootstrapping, where the target values that we regress value functions to are computed using a copy of the same network’s best action-value estimate on the next state. The appeal of these offline reinforcement learning methods is that you can get optimal policies from diverse, off-policy data without having to interact with the environment. Modified versions of Q-learning like CQL work even better on offline datasets, and have shown promise on smaller-scale simulated control environments. Unfortunately, bootstrapping does not mix well with generalization. It is folk knowledge that the deadly triad of function approximation, bootstrapping, and off-policy data make training unstable. I think this problem will only get worse as we scale up models and expect to train them on increasingly general tasks. This work shows that repeated bootstrapping iteratively decreases the capacity of the neural network. If you believe the claim that overparameterization of deep neural networks is key to generalization, then it would appear that for the same neural net architecture, offline RL is not quite as “data absorbent” as supervised learning. In practice, even algorithms like CQL are still challenging to scale and debug on larger, real-world datasets; colleagues of mine tried several variations of AWAC and CQL on large-scale robotics problems and found them to be trickier to get them to work than naive methods like Behavior Cloning. Instead of going through all this trouble, what if we lean into what deep nets excel at - sponging up data quickly with supervised learning and generalizing to massive datasets? Can we accomplish what RL sets out to do using the tools of generalization, rather than direct optimization? Learn the Distribution instead of the Optimum What if we make generalization the first-class citizen in algorithmic design, and tailor everything else in service of it? What if we could simply learn all the policies with supervised learning, and “just ask nicely” for the best one? Consider the recent work on Decision Transformer (DT), whereby instead of modeling a single policy and iteratively improving it with reinforcement learning, the authors simply use supervised learning coupled with a sequential model to predict trajectories of many different policies. The model is conditioned with the Return-to-Go so that it may predict actions consistent with a policy that would achieve those returns. The DT simply models all policies - good and bad - with supervised learning, and then use the magic of deep learning generalization to infer from the expert-conditioned policy. This phenomenon has been observed and developed in several prior and concurrent works, such as Reward-Conditioned Policies, Upside Down Reinforcement Learning and Reinforcement Learning as One Big Sequence Modeling Problem. The AlphaStar team also found that conditioning a model on human player skill level (e.g. future units they ended up build order, MMR, ELO scores) and using it to imitate all player data was superior to only imitating expert-level build orders. This technique is also commonly used in the Autonomous Vehicle space to model both good drivers and bad drivers jointly, even though the autonomous policy is only ever deployed to imitate good driving behavior. Hindsight Language Relabeling At a high level, DTs condition the supervised learning objective on some high level description \textbackslash (g\textbackslash ) that partitions what the policy will do in the future based on that value of \textbackslash (g\textbackslash ). The return-to-go is an especially salient quantity for a reinforcement learning task, but you can also express the future outcomes via a goal state or StarCraft build order or even a natural language description of what was accomplished. In Language Conditioned Imitation Learning over Unstructured Data, the authors pair arbitrary trajectories with post-hoc natural language descriptions, and then train a model to clone those behaviors conditioned on language. At test time, they simply “ask” the policy to do a novel task in a zero-shot manner. The nice thing about these techniques is that they are indispensable for reaching sparse goals on RL tasks like Ant-Maze. This lends support to the claim that generalization and inference across goal-conditioning can do far better than brute force search for a single sparse goal in a long-horizon task. Language is a particularly nice choice for conditioning because it can be used to partition a trajectory not just on skill level, but also by task, by how much the policy explores, how “animal-like” it is, and any other observations a human might make about the trajectory. Clauses can be composed ad-hoc without developing a formal grammar for all outcomes that the robot might accomplish. Language is an ideal “fuzzy” representation for the diversity of real-world outcomes and behaviors, which will become increasingly important as we want to partition increasingly diverse datasets. Generalizing From Imperfect Demonstrations A recent work I am quite inspired is D-REX, which tackles the problem of inferring the environment’s reward function from the demonstrations of a suboptimal policy. Classically, one requires making an assumption that the demonstrator is the optimal policy, from which you can use off-policy algorithms (e.g. Q-learning) to estimate the value function. Offline value estimation with deep neural nets can suffer from poor generalization to state-action pairs not in the demonstrator trajectory, and thus requires careful algorithmic tuning to make sure that the value function converges. An algorithm with poor convergence properties makes the propsects of minimizing training loss - and therefore generalization - tenuous. D-REX proposes a really clever trick to get around not having any reward labels at all, even when the demonstrator is suboptimal: Given a suboptimal policy \textbackslash (\textbackslash pi\_\textbackslash theta\textbackslash ), generate trajectory rollouts \textbackslash (\textbackslash tau\_1, \textbackslash tau\_2, ... \textbackslash tau\_N\textbackslash ) by having the policy interact with the environment. On each rollout, add variable amounts of noise \textbackslash (\textbackslash epsilon\textbackslash ) to its actions. Assume that adding noise to a suboptimal policy makes it even more suboptimal, i.e. \textbackslash (R(\textbackslash tau) \textbackslash geq R(\textbackslash tau + \textbackslash epsilon)\textbackslash ). Train a ranking model \textbackslash (f\_\textbackslash theta(\textbackslash tau\_i, \textbackslash tau\_j)\textbackslash ) to predict which of two trajectories \textbackslash (\textbackslash tau\_i, \textbackslash tau\_j\textbackslash ) has a higher return. The ranking model magically extrapolates to trajectories that are better than what \textbackslash (\textbackslash pi\_\textbackslash theta\textbackslash ) can generate, even though the ranking model has never been trained on trajectories better than \textbackslash (\textbackslash pi\_\textbackslash theta\textbackslash ) itself. I like this approach because ranking models are stable to train (they are just classifiers), and this method is able to achieve better-than-demonstrator behavior not through the explicit construction of the Bellman inequality or implicit planning through a learned model, but rather via extrapolation on a family of perturbations. Do You Even Need RL to Improve from Experience? In the above sections I’ve described how you can “generalize and infer” to get around exploration and even inverse reinforcement learning from sparse rewards. But what about “improving from a policy’s own experience, tabular rasa”? This is the main reason why people put up with the pain of implementing RL algorithms. Can we replace this with supervised learning algorithms and a bit of generalization as well? The goal of RL is to go from the current set of parameters \textbackslash (\textbackslash theta\^\{n\}\textbackslash ) and some collected policy experience \textbackslash (\textbackslash tau\textbackslash ) to a new set of parameters \textbackslash (\textbackslash theta\^\{n+1\}\textbackslash ) that achieves a higher episode return. Instead of using a “proper” RL algorithm to update the agent, could we just learn this mapping \textbackslash (f: (\textbackslash theta\^\{n\}, \textbackslash tau) \textbackslash to \textbackslash theta\^\{n+1\}\textbackslash ) via supervised deep learning? This idea is sometimes referred to as “meta-reinforcement learning”, because it involves learning a better reinforcement learning function than off-the-shelf RL algorithms. My colleagues and I applied this idea to a project where we trained a neural network to predict “improved policy behavior” from a video of a lesser policy’s experience. I could imagine this idea being combined with ranking and trajectory augmentation ideas from D-REX to further generalize the “policy improvement behavior”. Even if we never train on optimal policy trajectories, perhaps sufficient data augmentation can also lead to a general improvement operator that extrapolates to the optimal policy regime of parameters. People often conflate this policy improvement behavior with “reinforcement learning algorithms” like DQN and PPO, but behavior is distinct from implementation. The “policy improvement operator” \textbackslash (f: (\textbackslash theta\^\{n\}, \textbackslash tau) \textbackslash to \textbackslash theta\^\{n+1\}\textbackslash ) can be learned via your choice of reinforcement learning or supervised learning, but is deployed in a RL-like manner for interacting with the environment. The “Just-Ask-Generalization” Recipe Here is a table summarizing the previously mentioned RL problems, and comparing how each of them can be tackled with a “generalize-and-infer” approach instead of direct optimization. Goal “Direct Optimization” Approach “Generalize + Inference” Approach Reinforcement Learning with Sparse Rewards Find \textbackslash (p\^\textbackslash star(a\_t\textbackslash vert s\_t)\textbackslash ) s.t. \textbackslash (R\_t=1\textbackslash ), brute force exploration DT: Learn \textbackslash (p(a\_t\textbackslash vert s\_t,R\_t)\textbackslash ) from many policies, infer \textbackslash (p(a\_t\textbackslash vert s\_t, R\_t=1)\textbackslash ). H.E.R - Infer tasks for which gathered trajectories are optimal, then learn \textbackslash (p(\textbackslash text\{trajectory\}\textbackslash vert \textbackslash text\{task\})\textbackslash ). Then infer optimal trajectory for desired task. Learn a Reward Function from Suboptimal Trajectories Offline Inverse RL D-REX: Trajectory augmentation + Extrapolate to better trajectories. Improve the policy from experience Q-Learning, Policy Gradient Watch-Try-Learn: Learn \textbackslash (p(\textbackslash theta\^\{n+1\} \textbackslash vert \textbackslash theta\^n , \textbackslash tau, \textbackslash text\{task\})\textbackslash ) Fine-tune a simulated policy in a real-world environment Sample-efficient RL fine-tuning Domain Randomization: train on a distribution of simulators, and the policy “infers which world” it is in at test time. The high-level recipe is simple. If you want to find the solution \textbackslash (y\_i\textbackslash ) for a problem \textbackslash (x\_i\textbackslash ), consider setting up a dataset of paired problems and solutions \textbackslash ((x\_1, y\_1), ..., (x\_N, y\_N)\textbackslash ) and then training a deep network \textbackslash (y = f\_\textbackslash theta(x)\textbackslash ) that “simply maps your problems to solutions”. Then substitute your desired \textbackslash (x\_i\textbackslash ) and have the deep network infer the solution \textbackslash (y\_i\textbackslash ) via generalization. “Problem” is meant in the most abstract of terms and can refer to a RL environment, a dataset, or even a single example. “Solutions” could be represented as the optimal parameters of a policy or a neural network, or a single prediction. Techniques like goal relabeling help generate post-hoc problems from solutions, but building such a dataset can also be achieved via data augmentation techniques. At its core, we are transforming a difficult optimization problem into an inference problem, and training a supervised learning model on a distribution of problems for which it’s comparatively cheap to obtain solutions. To summarize the recommendations in a three-step recipe: Choose a method capable of minimizing training loss on massive datasets, i.e. supervised learning with maximum likelihood. This will facilitate scaling to complex, diverse datasets and getting the most generalization mileage out of your compute budget. If you want to learn \textbackslash (p(y\textbackslash vert x, \textbackslash text\{task\}=g\^\textbackslash star)\textbackslash ) for some prediction task \textbackslash (g\^\textbackslash star\textbackslash ), try learning \textbackslash (p(y\textbackslash vert x, \textbackslash text\{task\})\textbackslash ) for many related but different tasks \textbackslash (g \textbackslash sim p(g), g \textbackslash neq g\^\textbackslash star\textbackslash ) Then at test time just condition on \textbackslash (g\^\textbackslash star\textbackslash ). Formulate conditioning variables that help partition the data distribution while still admitting generalization on held-out samples from \textbackslash (p(g)\textbackslash ). Natural language encoding is a good choice. The insight that we can cast optimization problems into inference problems is not new. For example, the SGD optimizer can be cast as approximate Bayesian inference and so can optimal control via AICO. These works present a theoretical justification as to why inference can be a suitable replacement for optimization, since the problems and algorithms can be translated back and forth. I’m suggesting something slightly different here. Instead of casting a sequential decision making problem into an equivalent sequential inference problem, we construct the “meta-problem”: a distribution of similar problems for which it’s easy to obtain the solutions. We then solve the meta-problem with supervised learning by mapping problems directly to solutions. Don’t overthink it, just train the deep net in the simplest way possible and ask it for generalization! Perhaps in the near future we will be able to prompt-engineer such language-conditioned models with the hint “Generalize to unseen …”. Just ask for … Consciousness? How far can we stretch the principle of “generalize-and-infer” as an alternative to direct optimization? Here is a “recipe for consciousness” which would probably be better pondered over some strong drinks: Train a language-conditioned multi-policy model \textbackslash (p\_\textbackslash theta(a\textbackslash vert s, g)\textbackslash ) (implemented via a Decision Transformer or equivalent) to imitate a variety of policies \textbackslash (\textbackslash pi\_1, ..., \textbackslash pi\_N\textbackslash ) conditioned on natural language descriptions \textbackslash (g\textbackslash ) of those agents. At test time, some default policy \textbackslash (p(a\textbackslash vert s, g=\textbackslash text\{Behave as myself\})\textbackslash ) interacts with another agent \textbackslash (\textbackslash pi\_\textbackslash text\{test\}\textbackslash ) for a number of steps, after which we instruct the model to “behave as if you were \textbackslash (\textbackslash pi\_\textbackslash text\{test\}\textbackslash ).” The model would require a sort of “meta-cognition of others” capability, since it would have to infer what policy \textbackslash (\textbackslash pi\_\textbackslash text\{test\}\textbackslash ) would do in a particular situation. We make a copy of the multi-policy model \textbackslash (p\_\textbackslash phi \textbackslash sim p\_\textbackslash theta\textbackslash ), and embed multiple test-time iterations of step (1) within a single episode, with dozens of agents. Two of these agents are initially conditioned as \textbackslash (p\_\textbackslash theta(a\textbackslash vert s, g=\textbackslash text\{Behave as myself\})\textbackslash ) and \textbackslash (p\_\textbackslash phi(a\textbackslash vert s, g=\textbackslash text\{Behave as myself\})\textbackslash ). This generates episodes where some agents imitate other agents, and all agents observe this behavior. Then we ask \textbackslash (p\_\textbackslash phi\textbackslash ) to emit actions with the conditioning context “behave as if you were \textbackslash (\textbackslash pi\_\textbackslash theta\textbackslash ) pretending to be you”. This would require \textbackslash (\textbackslash pi\_\textbackslash phi\textbackslash ) to model \textbackslash (\textbackslash pi\_\textbackslash theta\textbackslash )’s imitation capabilities, as well as what information \textbackslash (\textbackslash pi\_\textbackslash theta\textbackslash ) knows about \textbackslash (\textbackslash pi\_\textbackslash phi\textbackslash ), on the fly. Researchers like Jürgen Schmidhuber have previously discussed how dynamics models (aka World Models) of embodied agents are already “conscious”, because successful modeling the dynamics of the environment around oneself necessitates a representation of the self as an embodied participant in the environment. While I think that “self-representation” is a necessity in planning and dynamics prediction problems, I think the framework is too vacuous to be of use in reproducing a convincing imitation of consciousness. After all, any planning algorithm that represents “the self” explicitly within each imagined trajectory rollout would be conscious under this definition. An A* maze-planner would satisfy this definition of consciousness. What I’m proposing is implementing a “more convincing” form of consciousness, not based on a “necessary representation of the self for planning”, but rather an understanding of the self that can be transmitted through language and behavior unrelated to any particular objective. For instance, the model needs to not only understand not only how a given policy regards itself, but how a variety of other policies might interpret the behavior of a that policy, much like funhouse mirrors that distort one’s reflection. The hypothesis is that through demonstrating this understanding of “distorted self-reflection”, the policy will learn to recognize itself and model the internal motivations and beliefs of other agents in agent-agent interactions. There are some important implementation details that I haven’t fleshed out yet, but at high level, I do think that supervised learning and natural language conditioning with enormous agent-interaction datasets are sufficiently powerful tools to learn interesting behaviors. Imbuing agents with some kind of meta-cogition ability of the self and other agents is an important step towards a convincing imitation of consciousness. Acknowledgements Thanks to Daniel Freeman, David Ha, Karol Hausman, Irwan Bello, Igor Mordatch, and Vincent Vanhoucke for feedback and discussion on earlier drafts of this work. References Generalization and scaling: Scaling Laws for Neural Language Models Self-supervised Pretraining of Visual Features in the Wild On the Opportunities and Risks of Foundation Models Understanding deep learning requires rethinking generalization A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes Patterns, Predictions, Actions: Generalization Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets DALL·E: Creating Images from Text RL challenges: Robots Must Be Ephemeralized An Empirical Model of Large-Batch Training Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning Deep Reinforcement Learning and the Deadly Triad Conservative Q-Learning AW-Opt: Learning Robotic Skills with Imitation andReinforcement at Scale Hindsight Imitation Decision Transformer: Reinforcement Learning via Sequence Modeling Reward-Conditioned Policies Upside Down Reinforcement Learning Reinforcement Learning as One Big Sequence Modeling Problem Grandmaster level in Starcraft II via multi-agent reinforcement learning Hindsight Experience Replay Learning Latent Plans from Play Replacing RL with Supervised Learning Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards Distribution Augmentation for Generative Modeling Stochastic Gradient Descent as Approximate Bayesian Inference Robot Trajectory Optimization using Approximate Inference Q/A Igor Mordatch supplied interesting questions and comments in reviewing this blog post. I have paraphrased his questions here and added responses in this section. 1. You discussed Supervised Learning and Reinforcement Learning. What do you think about Unsupervised Learning and “The Cake Analogy”? I consider unsupervised learning to be simply supervised learning for a different task, with comparable gradient variance, since targets are not usually noisly estimated beyond augmentation. Maximum likelihood estimation and contrastive algorithms like InfoNCE seem to be both useful for facilitating generalization in large models. 2. For the first difficulty of RL (evaluating success), aren’t there parallels to current generative models too? Success evaluation is hard for language models, as evidenced by dissatisfaction with BLEU scores and difficulty of evaluating likelihoods with non-likelihood based generative image models. There are parallels to likelihood-free generative models which require extensive compute for either training or sampling or likelihood evaluation. In practice, however, I think the burdens of evaluation are not directly comparable, since the computational expense of marginalization over observations for such models is dwarfed by the marginalization of success rate estimation in RL. In RL, you have to roll out the environment over O(coin flips) x O(initial state distribution) x O(action distribution) in order to get a low-variance policy gradient for “improved success across all states and tasks”. O(coin flips) is O(1000) samples for local improvement of a couple percent with statistical certainty, wheras I think that typically the marginalization costs of implicit likelihood tends to be cheaper with tricks like Langevin sampling O(minibatch=32). Also, the backprop passes used in Langevin dynamics are usually cheaper than running full environment simulations with a forward pass of the neural net on every step. 3. One of the findings of current language model work is that proxy objectives for what you really want are good enough. Simple next-token prediction induces generalization. But alignment to what you really want is still a hard problem in large model field and we don’t have good answers there yet (and ironically many attempts so far relied on incorporation of RL algorithms). Alignment objectives may lack a per-example surrogate loss. But under the “generalize-then-infer” school of thought, I would simply recommend learning \textbackslash (p(y\textbackslash vert x, \textbackslash text\{alignment objective\})\textbackslash ) with max likelihood over numerous hindsight alignment objectives, and then simply condition on the desired alignment object at test time. One could obtain a distribution of alignment descriptions by simply running the model live, and then hindsight labeling with the corresponding alignment realized by the model. Then we simply invoke this meme by Connor Leahy: Just asking the AI to be nice sounds flippant, but after seeing DALL-E and other large-scale multi-modal models that seem to generalize better as they get bigger, I think we should take these simple, borderline-naive ideas more seriously. 4. For the second difficulty of RL (gradient estimation), we know that for settings where you can backprop through environment dynamics to get exact policy gradient, doing so often leads to worse results. This reminds me of an old FB comment by Yann Lecun that a better way to estimate Hessian-vector products with ReLU activations is to use a stochastic estimator rather than computing the analytical hessian, since the 2nd-order curvature of ReLU is 0 and what you actually want is the Hessian-vector product of the smoothed version of the function. If you need to relax the dynamics or use an unbiased stochastic estimator to train through a differentiable simulator, then I think you’re back to where you’re starting with expensive evaluation, since presumably you need many rollouts to smooth out the simulator function and reduce variance. However, maybe the number of samples you need to estimate a smoothed policy gradient is a reasonable tradeoff here and this is a nice way to obtain gradients. 5. Why hasn’t something as simple as what you propose (generalize-then-infer) been done already? Some researchers out there are probably pursuing this already. My guess is that the research community tends to reward narratives that increase intellectual complexity and argue that “we need better algorithms”. People pay lip service to “simple ideas” but few are willing to truly pursue simplicity to its limit and simply scale up existing ideas. Another reason would be that researchers often don’t take generalization for granted, so it’s often quicker to think about adding explicit inductive biases rather than thinking about generalization as a first-class citizen and then tailoring all other design decisions in support of it. 6. How does your consciousness proposal relate to ideas from Schmidhuber’s “consciousness in world models” ideas, Friston’s Free Energy Principle, and Hawkin’s “memory of thoughts”? I consider Schmidhuber and Friston’s unified theories as more or less stating “optimal control requires good future prediction and future prediction with me in it requires self-representation”. If we draw an analogy to next-word prediction in large language models, maybe optimizing next state prediction perfectly is sufficient for subsuming all consciousness-type behaviors like theory-of-mind and the funhouse self-reflections I mentioned above. However, this would require an environment where predicting such dynamics accurately has an outsized impact on observation likelihoods. One critique I have about Schmidhuber and Friston’s frameworks is that they are too general, and can be universally applied to sea slugs and humans. If a certain environmental complexity is needed for future prediction to give rise to something humans would accept as conscious, then the main challenge is declaring what the minimum complexity would be. Hawkin’s “consciousness as memory of perception” seems to be more related to the subjective qualia aspect of consciousness rather than theory of mind. Note that most people do not consider a program that concatenates numpy arrays to be capable of “experiencing qualia” in the way humans do. Perhaps what is missing is the meta-cognition aspect - the policy needs to exhibit behaviors suggesting that it contemplates the fact that it experiences things. Again, this requires a carefully designed environment that demands such meta-cognition behavior. I think this could emerge from training for the theory-of-mind imitation problems I described above, since the agent would need to access a consistent representation about how it perceives things and transform it through a variety of “other agent’s lenses”. The flexibility of being able to project one’s own representation of sensory observations through one’s representation of other agents’ sensory capabilities is what would convince me that the agent understands that it can do sufficient meta-cognition about qualia. 7. Your formulation of consciousness only concerns itself with theory-of-mind behavior. What about attention behavior? See the second paragraph of the response to \#6. Update 20211025: Updated with a paraphrased question from Alexander Terenin 8. In Rich Sutton’s Bitter Lesson Essay, he argues that search and learning are both important. Do you really think that search can be completely replaced by a learned approach? I agree that having a bit of light search in your program can be immensely helpful to learning and overall performance. It’s a bit of a chicken/egg though. Does AlphaGo work because MCTS uses a learned value function to make search tractable? Or does the policy distillation only work because of search? I’m suggesting that when search becomes too hard (most RL tasks), it’s time to use more learning. You’re still doing search when performing supervised learning - you just get a lot more gradient signal per flop of computation.},
  langid = {english},
  organization = {{Eric Jang}},
  file = {/Users/alexandre.piche/Zotero/storage/XBXVP8CV/Just Ask for Generalization _ Eric Jang.pdf;/Users/alexandre.piche/Zotero/storage/K677VW7L/generalization.html}
}

@misc{kaddourCausalMachineLearning2022,
  title = {Causal {{Machine Learning}}: {{A Survey}} and {{Open Problems}}},
  shorttitle = {Causal {{Machine Learning}}},
  author = {Kaddour, Jean and Lynch, Aengus and Liu, Qi and Kusner, Matt J. and Silva, Ricardo},
  date = {2022-07-21},
  number = {arXiv:2206.15475},
  eprint = {2206.15475},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.15475},
  urldate = {2022-07-28},
  abstract = {Causal Machine Learning (CausalML) is an umbrella term for machine learning methods that formalize the data-generation process as a structural causal model (SCM). This perspective enables us to reason about the effects of changes to this process (interventions) and what would have happened in hindsight (counterfactuals). We categorize work in CausalML into five groups according to the problems they address: (1) causal supervised learning, (2) causal generative modeling, (3) causal explanations, (4) causal fairness, and (5) causal reinforcement learning. We systematically compare the methods in each category and point out open problems. Further, we review data-modality-specific applications in computer vision, natural language processing, and graph representation learning. Finally, we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field, including recommendations for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Methodology},
  file = {/Users/alexandre.piche/Zotero/storage/GX5VSE2M/Kaddour et al. - 2022 - Causal Machine Learning A Survey and Open Problem.pdf;/Users/alexandre.piche/Zotero/storage/G6U8WBUM/2206.html}
}

@misc{karrasElucidatingDesignSpace2022,
  title = {Elucidating the {{Design Space}} of {{Diffusion-Based Generative Models}}},
  author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  date = {2022-10-11},
  number = {arXiv:2206.00364},
  eprint = {2206.00364},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.00364},
  url = {http://arxiv.org/abs/2206.00364},
  urldate = {2022-12-13},
  abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/BGFDHILL/Karras et al. - 2022 - Elucidating the Design Space of Diffusion-Based Ge.pdf;/Users/alexandre.piche/Zotero/storage/EZPE3DYU/2206.html}
}

@misc{korbakRLKLPenalties2022,
  title = {{{RL}} with {{KL}} Penalties Is Better Viewed as {{Bayesian}} Inference},
  author = {Korbak, Tomasz and Perez, Ethan and Buckley, Christopher L.},
  date = {2022-10-21},
  number = {arXiv:2205.11275},
  eprint = {2205.11275},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.11275},
  urldate = {2022-11-23},
  abstract = {Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/WQ4YVVEU/Korbak et al. - 2022 - RL with KL penalties is better viewed as Bayesian .pdf;/Users/alexandre.piche/Zotero/storage/2S8I755S/2205.html}
}

@misc{krishnaDisagreementProblemExplainable2022,
  title = {The {{Disagreement Problem}} in {{Explainable Machine Learning}}: {{A Practitioner}}'s {{Perspective}}},
  shorttitle = {The {{Disagreement Problem}} in {{Explainable Machine Learning}}},
  author = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
  date = {2022-02-08},
  number = {arXiv:2202.01602},
  eprint = {2202.01602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.01602},
  urldate = {2022-12-13},
  abstract = {As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations (feature attributions) generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular post hoc explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Worse yet, there do not seem to be any principled, well-established approaches that machine learning practitioners employ to resolve these disagreements, which in turn implies that they may be relying on misleading explanations to make critical decisions such as which models to deploy in the real world. Our findings underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/XXGBWQYX/Krishna et al. - 2022 - The Disagreement Problem in Explainable Machine Le.pdf}
}

@misc{krishnamoorthyGenerativePretrainingBlackBox2022,
  title = {Generative {{Pretraining}} for {{Black-Box Optimization}}},
  author = {Krishnamoorthy, Siddarth and Mashkaria, Satvik Mehul and Grover, Aditya},
  date = {2022-06-21},
  number = {arXiv:2206.10786},
  eprint = {2206.10786},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.10786},
  urldate = {2022-07-28},
  abstract = {Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose Black-box Optimization Transformer (BOOMER), a generative framework for pretraining black-box optimizers using offline datasets. In BOOMER, we train an autoregressive model to imitate trajectory runs of implicit black-box function optimizers. Since these trajectories are unavailable by default, we develop a simple randomized heuristic to synthesize trajectories by sorting random points from offline data. We show theoretically that this heuristic induces trajectories that mimic transitions from diverse low-fidelity (exploration) to high-fidelity (exploitation) samples. Further, we introduce mechanisms to control the rate at which a trajectory transitions from exploration to exploitation, and use it to generalize outside the offline data at test-time. Empirically, we instantiate BOOMER using a casually masked Transformer and evaluate it on Design-Bench, where we rank the best on average, outperforming state-of-the-art baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/8CIHAMTP/Krishnamoorthy et al. - 2022 - Generative Pretraining for Black-Box Optimization.pdf;/Users/alexandre.piche/Zotero/storage/YIMPNL75/2206.html}
}

@misc{krishnaRankGenImprovingText2022,
  title = {{{RankGen}}: {{Improving Text Generation}} with {{Large Ranking Models}}},
  shorttitle = {{{RankGen}}},
  author = {Krishna, Kalpesh and Chang, Yapei and Wieting, John and Iyyer, Mohit},
  date = {2022-05-19},
  number = {arXiv:2205.09726},
  eprint = {2205.09726},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.09726},
  urldate = {2022-08-03},
  abstract = {Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues, we present RankGen, an encoder model (1.2B parameters) that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and, which discourage topically-similar but irrelevant generations; (2) sequences generated from a large language model conditioned on the prefix, which discourage repetition and hallucination. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5\% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We open source our model checkpoints, code, and human preferences with detailed explanations for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{krishnaRankGenImprovingText2022a,
  title = {{{RankGen}}: {{Improving Text Generation}} with {{Large Ranking Models}}},
  shorttitle = {{{RankGen}}},
  author = {Krishna, Kalpesh and Chang, Yapei and Wieting, John and Iyyer, Mohit},
  date = {2022-05-19},
  number = {arXiv:2205.09726},
  eprint = {2205.09726},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.09726},
  urldate = {2022-08-03},
  abstract = {Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues, we present RankGen, an encoder model (1.2B parameters) that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and, which discourage topically-similar but irrelevant generations; (2) sequences generated from a large language model conditioned on the prefix, which discourage repetition and hallucination. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5\% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We open source our model checkpoints, code, and human preferences with detailed explanations for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{kumarImplicitUnderParameterizationInhibits2021,
  title = {Implicit {{Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning}}},
  author = {Kumar, Aviral and Agarwal, Rishabh and Ghosh, Dibya and Levine, Sergey},
  date = {2021-10-24},
  number = {arXiv:2010.14498},
  eprint = {2010.14498},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.14498},
  urldate = {2022-08-07},
  abstract = {We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions, approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by previous instances of the value network, more gradient updates decrease the expressivity of the current value network. We characterize this loss of expressivity via a drop in the rank of the learned value network features, and show that this typically corresponds to a performance drop. We demonstrate this phenomenon on Atari and Gym benchmarks, in both offline and online RL settings. We formally analyze this phenomenon and show that it results from a pathological interaction between bootstrapping and gradient-based optimization. We further show that mitigating implicit under-parameterization by controlling rank collapse can improve performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/Z9Y4F889/Kumar et al. - 2021 - Implicit Under-Parameterization Inhibits Data-Effi.pdf;/Users/alexandre.piche/Zotero/storage/HDE2MDEB/2010.html}
}

@misc{kumarOfflineQLearningDiverse2022,
  title = {Offline {{Q-Learning}} on {{Diverse Multi-Task Data Both Scales And Generalizes}}},
  author = {Kumar, Aviral and Agarwal, Rishabh and Geng, Xinyang and Tucker, George and Levine, Sergey},
  date = {2022-11-28},
  number = {arXiv:2211.15144},
  eprint = {2211.15144},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.15144},
  url = {http://arxiv.org/abs/2211.15144},
  urldate = {2022-12-13},
  abstract = {The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: ResNets, cross-entropy based distributional backups, and feature normalization, offline Q-learning algorithms exhibit strong performance that scales with model capacity. Using multi-task Atari as a testbed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly suboptimal dataset (51\% human-level performance). Compared to return-conditioned supervised approaches, offline Q-learning scales similarly with model capacity and has better performance, especially when the dataset is suboptimal. Finally, we show that offline Q-learning with a diverse dataset is sufficient to learn powerful representations that facilitate rapid transfer to novel games and fast online learning on new variations of a training game, improving over existing state-of-the-art representation learning approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/KWXZ3836/Kumar et al. - 2022 - Offline Q-Learning on Diverse Multi-Task Data Both.pdf;/Users/alexandre.piche/Zotero/storage/3VWRBXLE/2211.html}
}

@misc{kumarUsingNaturalLanguage2022,
  title = {Using {{Natural Language}} and {{Program Abstractions}} to {{Instill Human Inductive Biases}} in {{Machines}}},
  author = {Kumar, Sreejan and Correa, Carlos G. and Dasgupta, Ishita and Marjieh, Raja and Hu, Michael Y. and Hawkins, Robert D. and Daw, Nathaniel D. and Cohen, Jonathan D. and Narasimhan, Karthik and Griffiths, Thomas L.},
  date = {2022-10-13},
  number = {arXiv:2205.11558},
  eprint = {2205.11558},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11558},
  url = {http://arxiv.org/abs/2205.11558},
  urldate = {2022-12-13},
  abstract = {Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more human-like inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/alexandre.piche/Zotero/storage/3VTR8VRR/Kumar et al. - 2022 - Using Natural Language and Program Abstractions to.pdf;/Users/alexandre.piche/Zotero/storage/YDUISNU4/2205.html}
}

@misc{kurinDefenseUnitaryScalarization2022,
  title = {In {{Defense}} of the {{Unitary Scalarization}} for {{Deep Multi-Task Learning}}},
  author = {Kurin, Vitaly and De Palma, Alessandro and Kostrikov, Ilya and Whiteson, Shimon and Kumar, M. Pawan},
  date = {2022-10-12},
  number = {arXiv:2201.04122},
  eprint = {2201.04122},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.04122},
  urldate = {2022-12-13},
  abstract = {Recent multi-task learning research argues against unitary scalarization, where training simply minimizes the sum of the task losses. Several ad-hoc multi-task optimization algorithms have instead been proposed, inspired by various hypotheses about what makes multi-task settings difficult. The majority of these optimizers require per-task gradients, and introduce significant memory, runtime, and implementation overhead. We present a theoretical analysis suggesting that many specialized multi-task optimizers can be interpreted as forms of regularization. Moreover, we show that, when coupled with standard regularization and stabilization techniques from single-task learning, unitary scalarization matches or improves upon the performance of complex multi-task optimizers in both supervised and reinforcement learning settings. We believe our results call for a critical reevaluation of recent research in the area.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/35XRDQ3D/Kurin et al. - 2022 - In Defense of the Unitary Scalarization for Deep M.pdf}
}

@misc{lachauxUnsupervisedTranslationProgramming2020,
  title = {Unsupervised {{Translation}} of {{Programming Languages}}},
  author = {Lachaux, Marie-Anne and Roziere, Baptiste and Chanussot, Lowik and Lample, Guillaume},
  date = {2020-09-22},
  number = {arXiv:2006.03511},
  eprint = {2006.03511},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2006.03511},
  urldate = {2022-09-01},
  abstract = {A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Programming Languages},
  file = {/Users/alexandre.piche/Zotero/storage/2G24EYYF/Lachaux et al. - 2020 - Unsupervised Translation of Programming Languages.pdf;/Users/alexandre.piche/Zotero/storage/8DR7C547/2006.html}
}

@article{leblondSEARNNTRAININGRNNS2018,
  title = {{{SEARNN}}: {{TRAINING RNNS WITH GLOBAL-LOCAL LOSSES}}},
  author = {Leblond, Rémi and Alayrac, Jean-Baptiste and Osokin, Anton and Lacoste-Julien, Simon},
  date = {2018},
  pages = {16},
  abstract = {We propose SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired by the “learning to search” (L2S) approach to structured prediction. RNNs have been widely successful in structured prediction applications such as machine translation or parsing, and are commonly trained using maximum likelihood estimation (MLE). Unfortunately, this training loss is not always an appropriate surrogate for the test error: by only maximizing the ground truth probability, it fails to exploit the wealth of information offered by structured losses. Further, it introduces discrepancies between training and predicting (such as exposure bias) that may hurt test performance. Instead, SEARNN leverages test-alike search space exploration to introduce global-local losses that are closer to the test error. We first demonstrate improved performance over MLE on two different tasks: OCR and spelling correction. Then, we propose a subsampling strategy to enable SEARNN to scale to large vocabulary sizes. This allows us to validate the benefits of our approach on a machine translation task.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/L42STSY9/Leblond et al. - 2018 - SEARNN TRAINING RNNS WITH GLOBAL-LOCAL LOSSES.pdf}
}

@misc{leeBPrefBenchmarkingPreferenceBased2021,
  title = {B-{{Pref}}: {{Benchmarking Preference-Based Reinforcement Learning}}},
  shorttitle = {B-{{Pref}}},
  author = {Lee, Kimin and Smith, Laura and Dragan, Anca and Abbeel, Pieter},
  date = {2021-11-04},
  number = {arXiv:2111.03026},
  eprint = {2111.03026},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.03026},
  urldate = {2022-07-30},
  abstract = {Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/FLVL9EF3/Lee et al. - 2021 - B-Pref Benchmarking Preference-Based Reinforcemen.pdf;/Users/alexandre.piche/Zotero/storage/98JYUWUM/2111.html}
}

@inproceedings{leeGeneralizedLeverageScore2020,
  title = {Generalized {{Leverage Score Sampling}} for {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Jason D and Shen, Ruoqi and Song, Zhao and Wang, Mengdi and Yu, zheng},
  date = {2020},
  volume = {33},
  pages = {10775--10787},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/7a22c0c0a4515485e31f95fd372050c9-Abstract.html},
  urldate = {2022-08-04},
  abstract = {Leverage score sampling is a powerful technique that originates from theoretical computer science, which can be used to speed up a large number of fundamental questions, e.g. linear regression, linear programming, semi-definite programming, cutting plane method, graph sparsification, maximum matching and max-flow. Recently, it has been shown that leverage score sampling helps to accelerate kernel methods [Avron, Kapralov, Musco, Musco, Velingker and Zandieh 17]. In this work, we generalize the results in [Avron, Kapralov, Musco, Musco, Velingker and Zandieh 17] to a broader class of kernels. We further bring the leverage score sampling into the field of deep learning theory.  1. We show the connection between the initialization for neural network training and approximating the neural tangent kernel with random features. 2. We prove the equivalence between regularized neural network and neural tangent kernel ridge regression under the initialization of both classical random Gaussian and leverage score sampling.},
  file = {/Users/alexandre.piche/Zotero/storage/5JNFFGXQ/Lee et al. - 2020 - Generalized Leverage Score Sampling for Neural Net.pdf}
}

@misc{leePEBBLEFeedbackEfficientInteractive2021,
  title = {{{PEBBLE}}: {{Feedback-Efficient Interactive Reinforcement Learning}} via {{Relabeling Experience}} and {{Unsupervised Pre-training}}},
  shorttitle = {{{PEBBLE}}},
  author = {Lee, Kimin and Smith, Laura and Abbeel, Pieter},
  date = {2021-06-09},
  number = {arXiv:2106.05091},
  eprint = {2106.05091},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.05091},
  urldate = {2022-11-11},
  abstract = {Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher’s preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent’s past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/S9YWI3FM/Lee et al. - 2021 - PEBBLE Feedback-Efficient Interactive Reinforceme.pdf}
}

@misc{leePIQTOptPredictiveInformation2022,
  title = {{{PI-QT-Opt}}: {{Predictive Information Improves Multi-Task Robotic Reinforcement Learning}} at {{Scale}}},
  shorttitle = {{{PI-QT-Opt}}},
  author = {Lee, Kuang-Huei and Xiao, Ted and Li, Adrian and Wohlhart, Paul and Fischer, Ian and Lu, Yao},
  date = {2022-11-24},
  number = {arXiv:2210.08217},
  eprint = {2210.08217},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  doi = {10.48550/arXiv.2210.08217},
  url = {http://arxiv.org/abs/2210.08217},
  urldate = {2022-12-17},
  abstract = {The predictive information, the mutual information between the past and future, has been shown to be a useful representation learning auxiliary loss for training reinforcement learning agents, as the ability to model what will happen next is critical to success on many control tasks. While existing studies are largely restricted to training specialist agents on single-task settings in simulation, in this work, we study modeling the predictive information for robotic agents and its importance for general-purpose agents that are trained to master a large repertoire of diverse skills from large amounts of data. Specifically, we introduce Predictive Information QT-Opt (PI-QT-Opt), a QT-Opt agent augmented with an auxiliary loss that learns representations of the predictive information to solve up to 297 vision-based robot manipulation tasks in simulation and the real world with a single set of parameters. We demonstrate that modeling the predictive information significantly improves success rates on the training tasks and leads to better zero-shot transfer to unseen novel tasks. Finally, we evaluate PI-QT-Opt on real robots, achieving substantial and consistent improvement over QT-Opt in multiple experimental settings of varying environments, skills, and multi-task configurations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/4RQ9D62J/Lee et al. - 2022 - PI-QT-Opt Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale.pdf}
}

@misc{leibovichLearningControlIterative2022,
  title = {Learning {{Control}} by {{Iterative Inversion}}},
  author = {Leibovich, Gal and Jacob, Guy and Avner, Or and Novik, Gal and Tamar, Aviv},
  date = {2022-11-03},
  number = {arXiv:2211.01724},
  eprint = {2211.01724},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01724},
  url = {http://arxiv.org/abs/2211.01724},
  urldate = {2022-12-13},
  abstract = {We formulate learning for control as an \$\textbackslash textit\{inverse problem\}\$ -- inverting a dynamical system to give the actions which yield desired behavior. The key challenge in this formulation is a \$\textbackslash textit\{distribution shift\}\$ -- the learning agent only observes the forward mapping (its actions' consequences) on trajectories that it can execute, yet must learn the inverse mapping for inputs-outputs that correspond to a different, desired behavior. We propose a general recipe for inverse problems with a distribution shift that we term \$\textbackslash textit\{iterative inversion\}\$ -- learn the inverse mapping under the current input distribution (policy), then use it on the desired output samples to obtain new inputs, and repeat. As we show, iterative inversion can converge to the desired inverse mapping, but under rather strict conditions on the mapping itself. We next apply iterative inversion to learn control. Our input is a set of demonstrations of desired behavior, given as video embeddings of trajectories, and our method iteratively learns to imitate trajectories generated by the current policy, perturbed by random exploration noise. We find that constantly adding the demonstrated trajectory embeddings \$\textbackslash textit\{as input\}\$ to the policy when generating trajectories to imitate, a-la iterative inversion, steers the learning towards the desired trajectory distribution. To the best of our knowledge, this is the first exploration of learning control from the viewpoint of inverse problems, and our main advantage is simplicity -- we do not require rewards, and only employ supervised learning, which easily scales to state-of-the-art trajectory embedding techniques and policy representations. With a VQ-VAE embedding, and a transformer-based policy, we demonstrate non-trivial continuous control on several tasks. We also report improved performance on imitating diverse behaviors compared to reward based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/AJEF6EQK/Leibovich et al. - 2022 - Learning Control by Iterative Inversion.pdf;/Users/alexandre.piche/Zotero/storage/T4CEWRDC/2211.html}
}

@misc{leikeScalableAgentAlignment2018,
  title = {Scalable Agent Alignment via Reward Modeling: A Research Direction},
  shorttitle = {Scalable Agent Alignment via Reward Modeling},
  author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  date = {2018-11-19},
  number = {arXiv:1811.07871},
  eprint = {1811.07871},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1811.07871},
  urldate = {2022-11-14},
  abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/PZ4S47FB/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf;/Users/alexandre.piche/Zotero/storage/9Z9UYBAF/1811.html}
}

@misc{leikeScalableAgentAlignment2018a,
  title = {Scalable Agent Alignment via Reward Modeling: A Research Direction},
  shorttitle = {Scalable Agent Alignment via Reward Modeling},
  author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  date = {2018-11-19},
  number = {arXiv:1811.07871},
  eprint = {1811.07871},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1811.07871},
  urldate = {2022-12-02},
  abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/XGLY4ANW/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf;/Users/alexandre.piche/Zotero/storage/8P7WVHT5/1811.html}
}

@misc{leitaoHumanAICollaborationDecisionMaking2022,
  title = {Human-{{AI Collaboration}} in {{Decision-Making}}: {{Beyond Learning}} to {{Defer}}},
  shorttitle = {Human-{{AI Collaboration}} in {{Decision-Making}}},
  author = {Leitão, Diogo and Saleiro, Pedro and Figueiredo, Mário A. T. and Bizarro, Pedro},
  date = {2022-07-13},
  number = {arXiv:2206.13202},
  eprint = {2206.13202},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.13202},
  url = {http://arxiv.org/abs/2206.13202},
  urldate = {2022-12-19},
  abstract = {Human-AI collaboration (HAIC) in decision-making aims to create synergistic teaming between human decision-makers and AI systems. Learning to defer (L2D) has been presented as a promising framework to determine who among humans and AI should make which decisions in order to optimize the performance and fairness of the combined system. Nevertheless, L2D entails several often unfeasible requirements, such as the availability of predictions from humans for every instance or ground-truth labels that are independent from said humans. Furthermore, neither L2D nor alternative approaches tackle fundamental issues of deploying HAIC systems in real-world settings, such as capacity management or dealing with dynamic environments. In this paper, we aim to identify and review these and other limitations, pointing to where opportunities for future research in HAIC may lie.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/T2CECTPD/Leitão et al. - 2022 - Human-AI Collaboration in Decision-Making Beyond .pdf;/Users/alexandre.piche/Zotero/storage/ZHF3BNZ8/2206.html}
}

@misc{liangHolisticEvaluationLanguage2022,
  title = {Holistic {{Evaluation}} of {{Language Models}}},
  author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  date = {2022-11-16},
  number = {arXiv:2211.09110},
  eprint = {2211.09110},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.09110},
  url = {http://arxiv.org/abs/2211.09110},
  urldate = {2022-12-13},
  abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/T3524YKB/Liang et al. - 2022 - Holistic Evaluation of Language Models.pdf;/Users/alexandre.piche/Zotero/storage/KRNCNK32/2211.html}
}

@misc{liangRewardUncertaintyExploration2022,
  title = {Reward {{Uncertainty}} for {{Exploration}} in {{Preference-based Reinforcement Learning}}},
  author = {Liang, Xinran and Shu, Katherine and Lee, Kimin and Abbeel, Pieter},
  date = {2022-05-24},
  number = {arXiv:2205.12401},
  eprint = {2205.12401},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.12401},
  urldate = {2022-10-27},
  abstract = {Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/BM654SWS/Liang et al. - 2022 - Reward Uncertainty for Exploration in Preference-b.pdf;/Users/alexandre.piche/Zotero/storage/IAB7MKWB/2205.html}
}

@misc{liangRewardUncertaintyExploration2022a,
  title = {Reward {{Uncertainty}} for {{Exploration}} in {{Preference-based Reinforcement Learning}}},
  author = {Liang, Xinran and Shu, Katherine and Lee, Kimin and Abbeel, Pieter},
  date = {2022-05-24},
  number = {arXiv:2205.12401},
  eprint = {2205.12401},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2205.12401},
  url = {http://arxiv.org/abs/2205.12401},
  urldate = {2022-12-14},
  abstract = {Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/X4Y7MZPD/Liang et al. - 2022 - Reward Uncertainty for Exploration in Preference-based Reinforcement Learning.pdf}
}

@misc{liuCharacterAwareModelsImprove2022,
  title = {Character-{{Aware Models Improve Visual Text Rendering}}},
  author = {Liu, Rosanne and Garrette, Dan and Saharia, Chitwan and Chan, William and Roberts, Adam and Narang, Sharan and Blok, Irina and Mical, R. J. and Norouzi, Mohammad and Constant, Noah},
  date = {2022-12-20},
  number = {arXiv:2212.10562},
  eprint = {2212.10562},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.10562},
  url = {http://arxiv.org/abs/2212.10562},
  urldate = {2022-12-27},
  abstract = {Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word's visual makeup as a series of glyphs. To quantify the extent of this effect, we conduct a series of controlled experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell). Transferring these learnings onto the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark). Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/alexandre.piche/Zotero/storage/MWGG6PMZ/Liu et al. - 2022 - Character-Aware Models Improve Visual Text Rendering.pdf}
}

@misc{liuCompositionalVisualGeneration2022,
  title = {Compositional {{Visual Generation}} with {{Composable Diffusion Models}}},
  author = {Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B.},
  date = {2022-07-27},
  number = {arXiv:2206.01714},
  eprint = {2206.01714},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.01714},
  urldate = {2022-12-08},
  abstract = {Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/DFW2PARI/Liu et al. - 2022 - Compositional Visual Generation with Composable Di.pdf;/Users/alexandre.piche/Zotero/storage/9WSMVG9M/2206.html}
}

@misc{liuDelayedImpactFair2018,
  title = {Delayed {{Impact}} of {{Fair Machine Learning}}},
  author = {Liu, Lydia T. and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
  date = {2018-04-07},
  number = {arXiv:1803.04383},
  eprint = {1803.04383},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1803.04383},
  urldate = {2022-11-22},
  abstract = {Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/QKH8T4D9/Liu et al. - 2018 - Delayed Impact of Fair Machine Learning.pdf;/Users/alexandre.piche/Zotero/storage/ATHXU3YD/1803.html}
}

@misc{liuInstructionFollowingAgentsJointly2022,
  title = {Instruction-{{Following Agents}} with {{Jointly Pre-Trained Vision-Language Models}}},
  author = {Liu, Hao and Lee, Lisa and Lee, Kimin and Abbeel, Pieter},
  date = {2022-10-24},
  number = {arXiv:2210.13431},
  eprint = {2210.13431},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.13431},
  url = {http://arxiv.org/abs/2210.13431},
  urldate = {2022-12-14},
  abstract = {Humans are excellent at understanding language and vision to accomplish a wide range of tasks. In contrast, creating general instruction-following embodied agents remains a difficult challenge. Prior work that uses pure language-only models lack visual grounding, making it difficult to connect language instructions with visual observations. On the other hand, methods that use pre-trained vision-language models typically come with divided language and visual representations, requiring designing specialized network architecture to fuse them together. We propose a simple yet effective model for robots to solve instruction-following tasks in vision-based environments. Our \textbackslash ours method consists of a multimodal transformer that encodes visual observations and language instructions, and a policy transformer that predicts actions based on encoded representations. The multimodal transformer is pre-trained on millions of image-text pairs and natural language text, thereby producing generic cross-modal representations of observations and instructions. The policy transformer keeps track of the full history of observations and actions, and predicts actions autoregressively. We show that this unified transformer model outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings. Our model also shows better model scalability and generalization ability than prior work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/35F3GHSA/Liu et al. - 2022 - Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models.pdf}
}

@misc{liWhatMakesConvolutional2022,
  title = {What {{Makes Convolutional Models Great}} on {{Long Sequence Modeling}}?},
  author = {Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta},
  date = {2022-10-17},
  number = {arXiv:2210.09298},
  eprint = {2210.09298},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.09298},
  url = {http://arxiv.org/abs/2210.09298},
  urldate = {2022-12-13},
  abstract = {Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependency efficiently. Attention overcomes this problem by aggregating global information but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. [2021] proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. S4 can model much longer sequences than Transformers and achieve significant gains over SoTA on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes. As a result, S4 is less intuitive and hard to use. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution (SGConv). SGConv exhibits strong empirical performance over several tasks: 1) With faster speed, SGConv surpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging SGConv into standard language and vision models, it shows the potential to improve both efficiency and performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/VB8PCTL4/Li et al. - 2022 - What Makes Convolutional Models Great on Long Sequ.pdf;/Users/alexandre.piche/Zotero/storage/WYX9DZXN/2210.html}
}

@article{luImitationNotEnough,
  title = {Imitation {{Is Not Enough}}: {{Robustifying Imitation}} with {{Reinforcement Learning}} for {{Challenging Driving Scenarios}}},
  author = {Lu, Yiren and Fu, Justin and Tucker, George and Pan, Xinlei and Bronstein, Eli and Roelofs, Becca and Sapp, Benjamin and White, Brandyn and Faust, Aleksandra and Whiteson, Shimon and Anguelov, Dragomir and Levine, Sergey},
  pages = {12},
  abstract = {Imitation learning (IL) is a simple and powerful way to use high-quality human driving data, which can be collected at scale, to identify driving preferences and produce human-like behavior. However, policies based on imitation learning alone often fail to sufficiently account for safety and reliability concerns. In this paper, we show how imitation learning combined with reinforcement learning using simple rewards can substantially improve the safety and reliability of driving policies over those learned from imitation alone. In particular, we use a combination of imitation and reinforcement learning to train a policy on over 100k miles of urban driving data, and measure its effectiveness in test scenarios grouped by different levels of collision risk. To our knowledge, this is the first application of a combined imitation and reinforcement learning approach in autonomous driving that utilizes large amounts of real-world human driving data.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/2QVI3AKT/Lu et al. - Imitation Is Not Enough Robustifying Imitation wi.pdf}
}

@misc{luImprovingGeneralizationPretrained2022,
  title = {Improving {{Generalization}} of {{Pre-trained Language Models}} via {{Stochastic Weight Averaging}}},
  author = {Lu, Peng and Kobyzev, Ivan and Rezagholizadeh, Mehdi and Rashid, Ahmad and Ghodsi, Ali and Langlais, Philippe},
  date = {2022-12-12},
  number = {arXiv:2212.05956},
  eprint = {2212.05956},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.05956},
  url = {http://arxiv.org/abs/2212.05956},
  urldate = {2022-12-18},
  abstract = {Knowledge Distillation (KD) is a commonly used technique for improving the generalization of compact Pre-trained Language Models (PLMs) on downstream tasks. However, such methods impose the additional burden of training a separate teacher model for every new dataset. Alternatively, one may directly work on the improvement of the optimization procedure of the compact model toward better generalization. Recent works observe that the flatness of the local minimum correlates well with better generalization. In this work, we adapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a flatter minimum, to fine-tuning PLMs. We conduct extensive experiments on various NLP tasks (text classification, question answering, and generation) and different model architectures and demonstrate that our adaptation improves the generalization without extra computation cost. Moreover, we observe that this simple optimization technique is able to outperform the state-of-the-art KD methods for compact models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/3N6KSXMX/Lu et al. - 2022 - Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging.pdf}
}

@misc{luoControllingCommercialCooling2022,
  title = {Controlling {{Commercial Cooling Systems Using Reinforcement Learning}}},
  author = {Luo, Jerry and Paduraru, Cosmin and Voicu, Octavian and Chervonyi, Yuri and Munns, Scott and Li, Jerry and Qian, Crystal and Dutta, Praneet and Davis, Jared Quincy and Wu, Ningjia and Yang, Xingwei and Chang, Chu-Ming and Li, Ted and Rose, Rob and Fan, Mingyan and Nakhost, Hootan and Liu, Tinglin and Kirkman, Brian and Altamura, Frank and Cline, Lee and Tonker, Patrick and Gouker, Joel and Uden, Dave and Bryan, Warren Buddy and Law, Jason and Fatiha, Deeni and Satra, Neil and Rothenberg, Juliet and Carlin, Molly and Tallapaka, Satish and Witherspoon, Sims and Parish, David and Dolan, Peter and Zhao, Chenyu and Mankowitz, Daniel J.},
  date = {2022-11-11},
  number = {arXiv:2211.07357},
  eprint = {2211.07357},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.07357},
  url = {http://arxiv.org/abs/2211.07357},
  urldate = {2022-12-13},
  abstract = {This paper is a technical overview of DeepMind and Google's recent work on reinforcement learning for controlling commercial cooling systems. Building on expertise that began with cooling Google's data centers more efficiently, we recently conducted live experiments on two real-world facilities in partnership with Trane Technologies, a building management system provider. These live experiments had a variety of challenges in areas such as evaluation, learning from offline data, and constraint satisfaction. Our paper describes these challenges in the hope that awareness of them will benefit future applied RL work. We also describe the way we adapted our RL system to deal with these challenges, resulting in energy savings of approximately 9\% and 13\% respectively at the two live experiment sites.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/alexandre.piche/Zotero/storage/G3WQQIDG/Luo et al. - 2022 - Controlling Commercial Cooling Systems Using Reinf.pdf;/Users/alexandre.piche/Zotero/storage/VHA4TUWJ/2211.html}
}

@misc{luQuarkControllableText2022,
  title = {Quark: {{Controllable Text Generation}} with {{Reinforced Unlearning}}},
  shorttitle = {Quark},
  author = {Lu, Ximing and Welleck, Sean and Hessel, Jack and Jiang, Liwei and Qin, Lianhui and West, Peter and Ammanabrolu, Prithviraj and Choi, Yejin},
  date = {2022-11-16},
  number = {arXiv:2205.13636},
  eprint = {2205.13636},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.13636},
  url = {http://arxiv.org/abs/2205.13636},
  urldate = {2022-12-13},
  abstract = {Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/6GR3ZAE9/Lu et al. - 2022 - Quark Controllable Text Generation with Reinforce.pdf;/Users/alexandre.piche/Zotero/storage/NFVM5DQV/2205.html}
}

@misc{mazumderDataPerfBenchmarksDataCentric2022,
  title = {{{DataPerf}}: {{Benchmarks}} for {{Data-Centric AI Development}}},
  shorttitle = {{{DataPerf}}},
  author = {Mazumder, Mark and Banbury, Colby and Yao, Xiaozhe and Karlaš, Bojan and Rojas, William Gaviria and Diamos, Sudnya and Diamos, Greg and He, Lynn and Kiela, Douwe and Jurado, David and Kanter, David and Mosquera, Rafael and Ciro, Juan and Aroyo, Lora and Acun, Bilge and Eyuboglu, Sabri and Ghorbani, Amirata and Goodman, Emmett and Kane, Tariq and Kirkpatrick, Christine R. and Kuo, Tzu-Sheng and Mueller, Jonas and Thrush, Tristan and Vanschoren, Joaquin and Warren, Margaret and Williams, Adina and Yeung, Serena and Ardalani, Newsha and Paritosh, Praveen and Zhang, Ce and Zou, James and Wu, Carole-Jean and Coleman, Cody and Ng, Andrew and Mattson, Peter and Reddi, Vijay Janapa},
  date = {2022-07-20},
  number = {arXiv:2207.10062},
  eprint = {2207.10062},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.10062},
  urldate = {2022-12-13},
  abstract = {Machine learning (ML) research has generally focused on models, while the most prominent datasets have been employed for everyday ML tasks without regard for the breadth, difficulty, and faithfulness of these datasets to the underlying problem. Neglecting the fundamental importance of datasets has caused major problems involving data cascades in real-world applications and saturation of dataset-driven criteria for model quality, hindering research growth. To solve this problem, we present DataPerf, a benchmark package for evaluating ML datasets and datasetworking algorithms. We intend it to enable the “data ratchet,” in which training sets will aid in evaluating test sets on the same problems, and vice versa. Such a feedback-driven strategy will generate a virtuous loop that will accelerate development of data-centric AI. The MLCommons Association will maintain DataPerf.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/PYICLAPN/Mazumder et al. - 2022 - DataPerf Benchmarks for Data-Centric AI Developme.pdf}
}

@misc{mcmilinSelectionBiasInduced2022,
  title = {Selection {{Bias Induced Spurious Correlations}} in {{Large Language Models}}},
  author = {McMilin, Emily},
  date = {2022-07-18},
  number = {arXiv:2207.08982},
  eprint = {2207.08982},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.08982},
  urldate = {2022-12-13},
  abstract = {In this work we show how large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias. To demonstrate the effect, we developed a masked gender task that can be applied to BERT-family models to reveal spurious correlations between predicted gender pronouns and a variety of seemingly gender-neutral variables like date and location, on pre-trained (unmodified) BERT and RoBERTa large models. Finally, we provide an online demo, inviting readers to experiment further.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/X9T97KUR/McMilin - 2022 - Selection Bias Induced Spurious Correlations in La.pdf}
}

@article{mehtaUnifiedLearningDemonstrations,
  title = {Unified {{Learning}} from {{Demonstrations}}, {{Corrections}}, and {{Preferences}} during {{Physical Human-Robot Interaction}}},
  author = {Mehta, Shaunak A and Losey, Dylan P},
  pages = {21},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/G8DCYT37/Mehta and Losey - Unified Learning from Demonstrations, Corrections,.pdf}
}

@misc{mindermannPrioritizedTrainingPoints2022,
  title = {Prioritized {{Training}} on {{Points}} That Are {{Learnable}}, {{Worth Learning}}, and {{Not Yet Learnt}}},
  author = {Mindermann, Sören and Brauner, Jan and Razzak, Muhammed and Sharma, Mrinank and Kirsch, Andreas and Xu, Winnie and Höltgen, Benedikt and Gomez, Aidan N. and Morisot, Adrien and Farquhar, Sebastian and Gal, Yarin},
  date = {2022-06-16},
  number = {arXiv:2206.07137},
  eprint = {2206.07137},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.07137},
  urldate = {2022-08-05},
  abstract = {Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2\% higher final accuracy than uniform data shuffling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/ZPDCSRAU/Mindermann et al. - 2022 - Prioritized Training on Points that are Learnable,.pdf;/Users/alexandre.piche/Zotero/storage/7HL3SKAT/2206.html}
}

@misc{muImprovingIntrinsicExploration2022,
  title = {Improving {{Intrinsic Exploration}} with {{Language Abstractions}}},
  author = {Mu, Jesse and Zhong, Victor and Raileanu, Roberta and Jiang, Minqi and Goodman, Noah and Rocktäschel, Tim and Grefenstette, Edward},
  date = {2022-11-21},
  number = {arXiv:2202.08938},
  eprint = {2202.08938},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.08938},
  urldate = {2022-12-07},
  abstract = {Reinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 47-85\% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/DJFMS3KU/Mu et al. - 2022 - Improving Intrinsic Exploration with Language Abst.pdf;/Users/alexandre.piche/Zotero/storage/I4N7WGZ2/2202.html}
}

@misc{muImprovingIntrinsicExploration2022a,
  title = {Improving {{Intrinsic Exploration}} with {{Language Abstractions}}},
  author = {Mu, Jesse and Zhong, Victor and Raileanu, Roberta and Jiang, Minqi and Goodman, Noah and Rocktäschel, Tim and Grefenstette, Edward},
  date = {2022-11-21},
  number = {arXiv:2202.08938},
  eprint = {2202.08938},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.08938},
  url = {http://arxiv.org/abs/2202.08938},
  urldate = {2022-12-13},
  abstract = {Reinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 47-85\% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/AQNC5ZXY/Mu et al. - 2022 - Improving Intrinsic Exploration with Language Abst.pdf;/Users/alexandre.piche/Zotero/storage/ID844UEF/2202.html}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  date = {2022-06-01},
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.09332},
  url = {http://arxiv.org/abs/2112.09332},
  urldate = {2022-07-29},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/SGMGHV95/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf;/Users/alexandre.piche/Zotero/storage/UIRW4SXM/2112.html}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022a,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  date = {2022-06-01},
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.09332},
  urldate = {2022-10-27},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/6XQTD7CN/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf;/Users/alexandre.piche/Zotero/storage/ZBE6PLW4/2112.html}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022b,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  date = {2022-06-01},
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.09332},
  urldate = {2022-10-31},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/4TVEEJH6/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf;/Users/alexandre.piche/Zotero/storage/9BASEFXZ/2112.html}
}

@misc{ngoAlignmentProblemDeep2022,
  title = {The Alignment Problem from a Deep Learning Perspective},
  author = {Ngo, Richard and Chan, Lawrence and Mindermann, Sören},
  date = {2022-12-14},
  number = {arXiv:2209.00626},
  eprint = {2209.00626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.00626},
  url = {http://arxiv.org/abs/2209.00626},
  urldate = {2022-12-17},
  abstract = {Within the coming decades, artificial general intelligence (AGI) may surpass human capabilities at a wide range of important tasks. We outline a case for expecting that, without substantial effort to prevent it, AGIs could learn to pursue goals which are very undesirable (in other words, misaligned) from a human perspective. We argue that AGIs trained in similar ways as today's most capable models could learn to act deceptively to receive higher reward; learn internally-represented goals which generalize beyond their training distributions; and pursue those goals using power-seeking strategies. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing these problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/HPQM72EI/Ngo et al. - 2022 - The alignment problem from a deep learning perspective.pdf}
}

@misc{ortegaShakingFoundationsDelusions2021,
  title = {Shaking the Foundations: Delusions in Sequence Models for Interaction and Control},
  shorttitle = {Shaking the Foundations},
  author = {Ortega, Pedro A. and Kunesch, Markus and Delétang, Grégoire and Genewein, Tim and Grau-Moya, Jordi and Veness, Joel and Buchli, Jonas and Degrave, Jonas and Piot, Bilal and Perolat, Julien and Everitt, Tom and Tallec, Corentin and Parisotto, Emilio and Erez, Tom and Chen, Yutian and Reed, Scott and Hutter, Marcus and de Freitas, Nando and Legg, Shane},
  options = {useprefix=true},
  date = {2021-10-20},
  number = {arXiv:2110.10819},
  eprint = {2110.10819},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.10819},
  urldate = {2022-08-04},
  abstract = {The recent phenomenal success of language models has reinvigorated machine learning research, and large sequence models such as transformers are being applied to a variety of domains. One important problem class that has remained relatively elusive however is purposeful adaptive behavior. Currently there is a common perception that sequence models "lack the understanding of the cause and effect of their actions" leading them to draw incorrect inferences due to auto-suggestive delusions. In this report we explain where this mismatch originates, and show that it can be resolved by treating actions as causal interventions. Finally, we show that in supervised learning, one can teach a system to condition or intervene on data by training with factual and counterfactual error signals respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/H99U9UUG/Ortega et al. - 2021 - Shaking the foundations delusions in sequence mod.pdf;/Users/alexandre.piche/Zotero/storage/MUPLS54T/2110.html}
}

@misc{osbandFineTuningLanguageModels2022,
  title = {Fine-{{Tuning Language Models}} via {{Epistemic Neural Networks}}},
  author = {Osband, Ian and Asghari, Seyed Mohammad and Van Roy, Benjamin and McAleese, Nat and Aslanides, John and Irving, Geoffrey},
  date = {2022-11-02},
  number = {arXiv:2211.01568},
  eprint = {2211.01568},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01568},
  url = {http://arxiv.org/abs/2211.01568},
  urldate = {2022-12-13},
  abstract = {Large language models are now part of a powerful new paradigm in machine learning. These models learn a wide range of capabilities from training on large unsupervised text corpora. In many applications, these capabilities are then fine-tuned through additional training on specialized data to improve performance in that setting. In this paper, we augment these models with an epinet: a small additional network architecture that helps to estimate model uncertainty and form an epistemic neural network (ENN). ENNs are neural networks that can know what they don't know. We show that, using an epinet to prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same performance while using 2x less data. We also investigate performance in synthetic neural network generative models designed to build understanding. In each setting, using an epinet outperforms heuristic active learning schemes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/JUDDJIGP/Osband et al. - 2022 - Fine-Tuning Language Models via Epistemic Neural N.pdf;/Users/alexandre.piche/Zotero/storage/K3J9WBVW/2211.html}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2022-07-29},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/F9QRE2RV/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf;/Users/alexandre.piche/Zotero/storage/K9W3XWSS/2203.html}
}

@misc{ouyangTrainingLanguageModels2022a,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2022-10-27},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/668CKJKV/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf;/Users/alexandre.piche/Zotero/storage/2LTX5JEN/2203.html}
}

@misc{pangRewardGamingConditional2022,
  title = {Reward {{Gaming}} in {{Conditional Text Generation}}},
  author = {Pang, Richard Yuanzhe and Padmakumar, Vishakh and Sellam, Thibault and Parikh, Ankur P. and He, He},
  date = {2022-11-16},
  number = {arXiv:2211.08714},
  eprint = {2211.08714},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2211.08714},
  urldate = {2022-11-18},
  abstract = {To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this short discussion piece, we would like to highlight reward gaming in the NLG community using concrete conditional text generation examples and discuss potential fixes and areas for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/Z2XLC4LC/Pang et al. - 2022 - Reward Gaming in Conditional Text Generation.pdf;/Users/alexandre.piche/Zotero/storage/DJDSL6F6/2211.html}
}

@misc{pangRewardGamingConditional2022a,
  title = {Reward {{Gaming}} in {{Conditional Text Generation}}},
  author = {Pang, Richard Yuanzhe and Padmakumar, Vishakh and Sellam, Thibault and Parikh, Ankur P. and He, He},
  date = {2022-11-16},
  number = {arXiv:2211.08714},
  eprint = {2211.08714},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.08714},
  url = {http://arxiv.org/abs/2211.08714},
  urldate = {2022-12-13},
  abstract = {To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this short discussion piece, we would like to highlight reward gaming in the NLG community using concrete conditional text generation examples and discuss potential fixes and areas for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/L7QEFGDI/Pang et al. - 2022 - Reward Gaming in Conditional Text Generation.pdf;/Users/alexandre.piche/Zotero/storage/SVWELZHB/2211.html}
}

@misc{peerImprovingTrainabilityDeep2022,
  title = {Improving the {{Trainability}} of {{Deep Neural Networks}} through {{Layerwise Batch-Entropy Regularization}}},
  author = {Peer, David and Keulen, Bart and Stabinger, Sebastian and Piater, Justus and Rodríguez-Sánchez, Antonio},
  date = {2022-08-01},
  number = {arXiv:2208.01134},
  eprint = {2208.01134},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.01134},
  url = {http://arxiv.org/abs/2208.01134},
  urldate = {2022-12-13},
  abstract = {Training deep neural networks is a very demanding task, especially challenging is how to adapt architectures to improve the performance of trained models. We can find that sometimes, shallow networks generalize better than deep networks, and the addition of more layers results in higher training and test errors. The deep residual learning framework addresses this degradation problem by adding skip connections to several neural network layers. It would at first seem counter-intuitive that such skip connections are needed to train deep networks successfully as the expressivity of a network would grow exponentially with depth. In this paper, we first analyze the flow of information through neural networks. We introduce and evaluate the batch-entropy which quantifies the flow of information through each layer of a neural network. We prove empirically and theoretically that a positive batch-entropy is required for gradient descent-based training approaches to optimize a given loss function successfully. Based on those insights, we introduce batch-entropy regularization to enable gradient descent-based training algorithms to optimize the flow of information through each hidden layer individually. With batch-entropy regularization, gradient descent optimizers can transform untrainable networks into trainable networks. We show empirically that we can therefore train a "vanilla" fully connected network and convolutional neural network -- no skip connections, batch normalization, dropout, or any other architectural tweak -- with 500 layers by simply adding the batch-entropy regularization term to the loss function. The effect of batch-entropy regularization is not only evaluated on vanilla neural networks, but also on residual networks, autoencoders, and also transformer models over a wide range of computer vision as well as natural language processing tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/QEZUDH6X/Peer et al. - 2022 - Improving the Trainability of Deep Neural Networks.pdf;/Users/alexandre.piche/Zotero/storage/3UIFE5PE/2208.html}
}

@misc{pengInherentlyExplainableReinforcement2022,
  title = {Inherently {{Explainable Reinforcement Learning}} in {{Natural Language}}},
  author = {Peng, Xiangyu and Riedl, Mark O. and Ammanabrolu, Prithviraj},
  date = {2022-10-06},
  number = {arXiv:2112.08907},
  eprint = {2112.08907},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.08907},
  url = {http://arxiv.org/abs/2112.08907},
  urldate = {2022-12-13},
  abstract = {We focus on the task of creating a reinforcement learning agent that is inherently explainable -- with the ability to produce immediate local explanations by thinking out loud while performing a task and analyzing entire trajectories post-hoc to produce causal explanations. This Hierarchically Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive Fictions, text-based game environments in which an agent perceives and acts upon the world using textual natural language. These games are usually structured as puzzles or quests with long-term dependencies in which an agent must complete a sequence of actions to succeed -- providing ideal environments in which to test an agent's ability to explain its actions. Our agent is designed to treat explainability as a first-class citizen, using an extracted symbolic knowledge graph-based state representation coupled with a Hierarchical Graph Attention mechanism that points to the facts in the internal graph representation that most influenced the choice of actions. Experiments show that this agent provides significantly improved explanations over strong baselines, as rated by human participants generally unfamiliar with the environment, while also matching state-of-the-art task performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/alexandre.piche/Zotero/storage/G6DX2C89/Peng et al. - 2022 - Inherently Explainable Reinforcement Learning in N.pdf;/Users/alexandre.piche/Zotero/storage/HPAKJM58/2112.html}
}

@misc{perezDiscoveringLanguageModel2022,
  title = {Discovering {{Language Model Behaviors}} with {{Model-Written Evaluations}}},
  author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
  date = {2022-12-19},
  number = {arXiv:2212.09251},
  eprint = {2212.09251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.09251},
  url = {http://arxiv.org/abs/2212.09251},
  urldate = {2022-12-27},
  abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/NQNSEN33/Perez et al. - 2022 - Discovering Language Model Behaviors with Model-Written Evaluations.pdf}
}

@misc{phuongFormalAlgorithmsTransformers2022,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  date = {2022-07-19},
  number = {arXiv:2207.09238},
  eprint = {2207.09238},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.09238},
  urldate = {2022-12-13},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (not results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/alexandre.piche/Zotero/storage/SP6N6W57/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf}
}

@misc{phuongFormalAlgorithmsTransformers2022a,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  date = {2022-07-19},
  number = {arXiv:2207.09238},
  eprint = {2207.09238},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.09238},
  urldate = {2022-12-13},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (not results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/alexandre.piche/Zotero/storage/35XG9J6H/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf}
}

@misc{phuongFormalAlgorithmsTransformers2022b,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  date = {2022-07-19},
  number = {arXiv:2207.09238},
  eprint = {2207.09238},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.09238},
  urldate = {2022-08-05},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/alexandre.piche/Zotero/storage/LDMP75ZG/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf;/Users/alexandre.piche/Zotero/storage/V2LEHVHF/2207.html}
}

@misc{pillutlaMAUVEMeasuringGap2021,
  title = {{{MAUVE}}: {{Measuring}} the {{Gap Between Neural Text}} and {{Human Text}} Using {{Divergence Frontiers}}},
  shorttitle = {{{MAUVE}}},
  author = {Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  date = {2021-11-23},
  number = {arXiv:2102.01454},
  eprint = {2102.01454},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2102.01454},
  urldate = {2022-11-25},
  abstract = {As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/F9NF5USS/Pillutla et al. - 2021 - MAUVE Measuring the Gap Between Neural Text and H.pdf;/Users/alexandre.piche/Zotero/storage/VGUK9XUI/2102.html}
}

@misc{pokleDeepEquilibriumApproaches2022,
  title = {Deep {{Equilibrium Approaches}} to {{Diffusion Models}}},
  author = {Pokle, Ashwini and Geng, Zhengyang and Kolter, Zico},
  date = {2022-10-23},
  number = {arXiv:2210.12867},
  eprint = {2210.12867},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.12867},
  urldate = {2022-12-05},
  abstract = {Diffusion-based generative models are extremely effective in generating high-quality images, with generated samples often surpassing the quality of those produced by other models under several metrics. One distinguishing feature of these models, however, is that they typically require long sampling chains to produce high-fidelity images. This presents a challenge not only from the lenses of sampling time, but also from the inherent difficulty in backpropagating through these chains in order to accomplish tasks such as model inversion, i.e. approximately finding latent states that generate known images. In this paper, we look at diffusion models through a different perspective, that of a (deep) equilibrium (DEQ) fixed point model. Specifically, we extend the recent denoising diffusion implicit model (DDIM; Song et al. 2020), and model the entire sampling chain as a joint, multivariate fixed point system. This setup provides an elegant unification of diffusion and equilibrium models, and shows benefits in 1) single image sampling, as it replaces the fully-serial typical sampling process with a parallel one; and 2) model inversion, where we can leverage fast gradients in the DEQ setting to much more quickly find the noise that generates a given image. The approach is also orthogonal and thus complementary to other methods used to reduce the sampling time, or improve model inversion. We demonstrate our method's strong performance across several datasets, including CIFAR10, CelebA, and LSUN Bedrooms and Churches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/J53XIJXV/Pokle et al. - 2022 - Deep Equilibrium Approaches to Diffusion Models.pdf;/Users/alexandre.piche/Zotero/storage/IFNRYGM8/2210.html}
}

@misc{pressMeasuringNarrowingCompositionality2022,
  title = {Measuring and {{Narrowing}} the {{Compositionality Gap}} in {{Language Models}}},
  author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
  date = {2022-10-07},
  number = {arXiv:2210.03350},
  eprint = {2210.03350},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03350},
  url = {http://arxiv.org/abs/2210.03350},
  urldate = {2022-12-13},
  abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/7YZY3XMR/Press et al. - 2022 - Measuring and Narrowing the Compositionality Gap i.pdf;/Users/alexandre.piche/Zotero/storage/Y968X3LZ/2210.html}
}

@misc{ramamurthyReinforcementLearningNot2022,
  title = {Is {{Reinforcement Learning}} ({{Not}}) for {{Natural Language Processing}}?: {{Benchmarks}}, {{Baselines}}, and {{Building Blocks}} for {{Natural Language Policy Optimization}}},
  shorttitle = {Is {{Reinforcement Learning}} ({{Not}}) for {{Natural Language Processing}}?},
  author = {Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kianté and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  date = {2022-10-03},
  number = {arXiv:2210.01241},
  eprint = {2210.01241},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.01241},
  urldate = {2022-10-27},
  abstract = {We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization)\vphantom\{\} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/PJVCRHL9/Ramamurthy et al. - 2022 - Is Reinforcement Learning (Not) for Natural Langua.pdf;/Users/alexandre.piche/Zotero/storage/KHHVCX9Q/2210.html}
}

@misc{ramamurthyReinforcementLearningNot2022a,
  title = {Is {{Reinforcement Learning}} ({{Not}}) for {{Natural Language Processing}}?: {{Benchmarks}}, {{Baselines}}, and {{Building Blocks}} for {{Natural Language Policy Optimization}}},
  shorttitle = {Is {{Reinforcement Learning}} ({{Not}}) for {{Natural Language Processing}}?},
  author = {Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kianté and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  date = {2022-10-03},
  number = {arXiv:2210.01241},
  eprint = {2210.01241},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.01241},
  urldate = {2022-11-01},
  abstract = {We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization)\vphantom\{\} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/8SQMVT26/Ramamurthy et al. - 2022 - Is Reinforcement Learning (Not) for Natural Langua.pdf;/Users/alexandre.piche/Zotero/storage/REBZBPZW/2210.html}
}

@article{rayBenchmarkingSafeExploration,
  title = {Benchmarking {{Safe Exploration}} in {{Deep Reinforcement Learning}}},
  author = {Ray, Alex and Achiam, Joshua and Amodei, Dario},
  pages = {25},
  abstract = {Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/U3VC58EZ/Ray et al. - Benchmarking Safe Exploration in Deep Reinforcemen.pdf}
}

@misc{reddyFirstContactUnsupervised2022,
  title = {First {{Contact}}: {{Unsupervised Human-Machine Co-Adaptation}} via {{Mutual Information Maximization}}},
  shorttitle = {First {{Contact}}},
  author = {Reddy, Siddharth and Levine, Sergey and Dragan, Anca D.},
  date = {2022-09-14},
  number = {arXiv:2205.12381},
  eprint = {2205.12381},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2205.12381},
  url = {http://arxiv.org/abs/2205.12381},
  urldate = {2022-12-15},
  abstract = {How can we train an assistive human-machine interface (e.g., an electromyography-based limb prosthesis) to translate a user's raw command signals into the actions of a robot or computer when there is no prior mapping, we cannot ask the user for supervision in the form of action labels or reward feedback, and we do not have prior knowledge of the tasks the user is trying to accomplish? The key idea in this paper is that, regardless of the task, when an interface is more intuitive, the user's commands are less noisy. We formalize this idea as a completely unsupervised objective for optimizing interfaces: the mutual information between the user's command signals and the induced state transitions in the environment. To evaluate whether this mutual information score can distinguish between effective and ineffective interfaces, we conduct an observational study on 540K examples of users operating various keyboard and eye gaze interfaces for typing, controlling simulated robots, and playing video games. The results show that our mutual information scores are predictive of the ground-truth task completion metrics in a variety of domains, with an average Spearman's rank correlation of 0.43. In addition to offline evaluation of existing interfaces, we use our unsupervised objective to learn an interface from scratch: we randomly initialize the interface, have the user attempt to perform their desired tasks using the interface, measure the mutual information score, and update the interface to maximize mutual information through reinforcement learning. We evaluate our method through a user study with 12 participants who perform a 2D cursor control task using a perturbed mouse, and an experiment with one user playing the Lunar Lander game using hand gestures. The results show that we can learn an interface from scratch, without any user supervision or prior knowledge of tasks, in under 30 minutes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/QKGDSKHZ/Reddy et al. - 2022 - First Contact Unsupervised Human-Machine Co-Adaptation via Mutual Information Maximization.pdf}
}

@misc{reddyLearningHumanObjectives2021,
  title = {Learning {{Human Objectives}} by {{Evaluating Hypothetical Behavior}}},
  author = {Reddy, Siddharth and Dragan, Anca D. and Levine, Sergey and Legg, Shane and Leike, Jan},
  date = {2021-03-24},
  number = {arXiv:1912.05652},
  eprint = {1912.05652},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1912.05652},
  urldate = {2022-07-27},
  abstract = {We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/JYRC6KQ5/Reddy et al. - 2021 - Learning Human Objectives by Evaluating Hypothetic.pdf;/Users/alexandre.piche/Zotero/storage/7ZJNH8BT/1912.html}
}

@article{rogersPrimerBERTologyWhat2020,
  title = {A {{Primer}} in {{BERTology}}: {{What We Know About How BERT Works}}},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020-12},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00349},
  url = {https://direct.mit.edu/tacl/article/96482},
  urldate = {2022-07-18},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/E7XINEWA/Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT.pdf}
}

@article{rogersPrimerBERTologyWhat2020a,
  title = {A {{Primer}} in {{BERTology}}: {{What We Know About How BERT Works}}},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020-12},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00349},
  url = {https://direct.mit.edu/tacl/article/96482},
  urldate = {2022-08-31},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/C2ZR7C8U/Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT.pdf}
}

@article{rudnerPathologiesKLRegularizedReinforcement,
  title = {On {{Pathologies}} in {{KL-Regularized Reinforcement Learning}} from {{Expert Demonstrations}}},
  author = {Rudner, Tim G J and Lu, Cong and Osborne, Michael A and Gal, Yarin and Teh, Yee Whye},
  pages = {14},
  abstract = {KL-regularized reinforcement learning from expert demonstrations has proved successful in improving the sample efficiency of deep reinforcement learning algorithms, allowing them to be applied to challenging physical real-world tasks. However, we show that KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations can suffer from pathological training dynamics that can lead to slow, unstable, and suboptimal online learning. We show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. Finally, we show that the pathology can be remedied by non-parametric behavioral reference policies and that this allows KL-regularized reinforcement learning to significantly outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/D6IIKSSB/Rudner et al. - On Pathologies in KL-Regularized Reinforcement Lea.pdf}
}

@inproceedings{sadighActivePreferenceBasedLearning2017,
  title = {Active {{Preference-Based Learning}} of {{Reward Functions}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIII}}},
  author = {Sadigh, Dorsa and Dragan, Anca and Sastry, Shankar and Seshia, Sanjit},
  date = {2017-07-12},
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2017.XIII.053},
  url = {http://www.roboticsproceedings.org/rss13/p53.pdf},
  urldate = {2022-12-15},
  abstract = {Our goal is to efficiently learn reward functions encoding a human’s preferences for how a dynamical system should act. There are two challenges with this. First, in many problems it is difficult for people to provide demonstrations of the desired system trajectory (like a high-DOF robot arm motion or an aggressive driving maneuver), or to even assign how much numerical reward an action or trajectory should get. We build on work in label ranking and propose to learn from preferences (or comparisons) instead: the person provides the system a relative preference between two trajectories. Second, the learned reward function strongly depends on what environments and trajectories were experienced during the training phase. We thus take an active learning approach, in which the system decides on what preference queries to make. A novel aspect of our work is the complexity and continuous nature of the queries: continuous trajectories of a dynamical system in environments with other moving agents (humans or robots). We contribute a method for actively synthesizing queries that satisfy the dynamics of the system. Further, we learn the reward function from a continuous hypothesis space by maximizing the volume removed from the hypothesis space by each query. We assign weights to the hypothesis space in the form of a log-concave distribution and provide a bound on the number of iterations required to converge. We show that our algorithm converges faster to the desired reward compared to approaches that are not active or that do not synthesize queries in an autonomous driving domain. We then run a user study to put our method to the test with real people.},
  eventtitle = {Robotics: {{Science}} and {{Systems}} 2017},
  isbn = {978-0-9923747-3-0},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/TJ7G7KRC/Sadigh et al. - 2017 - Active Preference-Based Learning of Reward Functio.pdf}
}

@online{SecondLawThermodynamics,
  title = {The {{Second Law}} of {{Thermodynamics}}, and {{Engines}} of {{Cognition}} - {{LessWrong}}},
  url = {https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition},
  urldate = {2022-11-04},
  abstract = {The first law of thermodynamics, better known as Conservation of Energy, says that you can't create energy from nothing: it prohibits perpetual motion machines of the first type, which run and run in…}
}

@misc{shahRetrospective2021BASALT2022,
  title = {Retrospective on the 2021 {{BASALT Competition}} on {{Learning}} from {{Human Feedback}}},
  author = {Shah, Rohin and Wang, Steven H. and Wild, Cody and Milani, Stephanie and Kanervisto, Anssi and Goecks, Vinicius G. and Waytowich, Nicholas and Watkins-Valls, David and Prakash, Bharat and Mills, Edmund and Garg, Divyansh and Fries, Alexander and Souly, Alexandra and Shern, Chan Jun and del Castillo, Daniel and Lieberum, Tom},
  options = {useprefix=true},
  date = {2022-04-14},
  number = {arXiv:2204.07123},
  eprint = {2204.07123},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.07123},
  urldate = {2022-07-30},
  abstract = {We held the first-ever MineRL Benchmark for Agents that Solve Almost-Lifelike Tasks (MineRL BASALT) Competition at the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021). The goal of the competition was to promote research towards agents that use learning from human feedback (LfHF) techniques to solve open-world tasks. Rather than mandating the use of LfHF techniques, we described four tasks in natural language to be accomplished in the video game Minecraft, and allowed participants to use any approach they wanted to build agents that could accomplish the tasks. Teams developed a diverse range of LfHF algorithms across a variety of possible human feedback types. The three winning teams implemented significantly different approaches while achieving similar performance. Interestingly, their approaches performed well on different tasks, validating our choice of tasks to include in the competition. While the outcomes validated the design of our competition, we did not get as many participants and submissions as our sister competition, MineRL Diamond. We speculate about the causes of this problem and suggest improvements for future iterations of the competition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/alexandre.piche/Zotero/storage/YS29B8CG/Shah et al. - 2022 - Retrospective on the 2021 BASALT Competition on Le.pdf;/Users/alexandre.piche/Zotero/storage/2Q7EKAUE/2204.html}
}

@misc{shinOfflinePreferenceBasedApprenticeship2022,
  title = {Offline {{Preference-Based Apprenticeship Learning}}},
  author = {Shin, Daniel and Brown, Daniel S. and Dragan, Anca D.},
  date = {2022-02-16},
  number = {arXiv:2107.09251},
  eprint = {2107.09251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2107.09251},
  urldate = {2022-07-29},
  abstract = {Learning a reward function from human preferences is challenging as it typically requires having a high-fidelity simulator or using expensive and potentially unsafe actual physical rollouts in the environment. However, in many tasks the agent might have access to offline data from related tasks in the same target environment. While offline data is increasingly being used to aid policy optimization via offline RL, our observation is that it can be a surprisingly rich source of information for preference learning as well. We propose an approach that uses an offline dataset to craft preference queries via pool-based active learning, learns a distribution over reward functions, and optimizes a corresponding policy via offline RL. Crucially, our proposed approach does not require actual physical rollouts or an accurate simulator for either the reward learning or policy optimization steps. To test our approach, we identify a subset of existing offline RL benchmarks that are well suited for offline reward learning and also propose new offline apprenticeship learning benchmarks which allow for more open-ended behaviors. Our empirical results suggest that combining offline RL with learned human preferences can enable an agent to learn to perform novel tasks that were not explicitly shown in the offline data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/II27864N/Shin et al. - 2022 - Offline Preference-Based Apprenticeship Learning.pdf;/Users/alexandre.piche/Zotero/storage/FBYAZ4NT/2107.html}
}

@misc{shwartz-zivPreTrainYourLoss2022,
  title = {Pre-{{Train Your Loss}}: {{Easy Bayesian Transfer Learning}} with {{Informative Priors}}},
  shorttitle = {Pre-{{Train Your Loss}}},
  author = {Shwartz-Ziv, Ravid and Goldblum, Micah and Souri, Hossein and Kapoor, Sanyam and Zhu, Chen and LeCun, Yann and Wilson, Andrew Gordon},
  date = {2022-05-20},
  number = {arXiv:2205.10279},
  eprint = {2205.10279},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.10279},
  urldate = {2022-12-13},
  abstract = {Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/PYFSYIVY/Shwartz-Ziv et al. - 2022 - Pre-Train Your Loss Easy Bayesian Transfer Learni.pdf}
}

@online{SimulatorsLessWrong,
  title = {Simulators - {{LessWrong}}},
  url = {https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators},
  urldate = {2022-11-13},
  abstract = {Thanks to Adam Shimi, Lee Sharkey, Evan Hubinger, Nicholas Dupuis, Leo Gao, Johannes Treutlein, and Jonathan Low for feedback on drafts. …}
}

@misc{singhOfflineRLRealistic2022,
  title = {Offline {{RL With Realistic Datasets}}: {{Heteroskedasticity}} and {{Support Constraints}}},
  shorttitle = {Offline {{RL With Realistic Datasets}}},
  author = {Singh, Anikait and Kumar, Aviral and Vuong, Quan and Chebotar, Yevgen and Levine, Sergey},
  date = {2022-11-21},
  number = {arXiv:2211.01052},
  eprint = {2211.01052},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01052},
  url = {http://arxiv.org/abs/2211.01052},
  urldate = {2022-12-13},
  abstract = {Offline reinforcement learning (RL) learns policies entirely from static datasets, thereby avoiding the challenges associated with online data collection. Practical applications of offline RL will inevitably require learning from datasets where the variability of demonstrated behaviors changes non-uniformly across the state space. For example, at a red light, nearly all human drivers behave similarly by stopping, but when merging onto a highway, some drivers merge quickly, efficiently, and safely, while many hesitate or merge dangerously. Both theoretically and empirically, we show that typical offline RL methods, which are based on distribution constraints fail to learn from data with such non-uniform variability, due to the requirement to stay close to the behavior policy to the same extent across the state space. Ideally, the learned policy should be free to choose per state how closely to follow the behavior policy to maximize long-term return, as long as the learned policy stays within the support of the behavior policy. To instantiate this principle, we reweight the data distribution in conservative Q-learning (CQL) to obtain an approximate support constraint formulation. The reweighted distribution is a mixture of the current policy and an additional policy trained to mine poor actions that are likely under the behavior policy. Our method, CQL (ReDS), is simple, theoretically motivated, and improves performance across a wide range of offline RL problems in Atari games, navigation, and pixel-based manipulation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/64BXQP5H/Singh et al. - 2022 - Offline RL With Realistic Datasets Heteroskedasti.pdf;/Users/alexandre.piche/Zotero/storage/KSQIKJZH/2211.html}
}

@misc{smithSimplifiedStateSpace2022,
  title = {Simplified {{State Space Layers}} for {{Sequence Modeling}}},
  author = {Smith, Jimmy T. H. and Warrington, Andrew and Linderman, Scott W.},
  date = {2022-10-05},
  number = {arXiv:2208.04933},
  eprint = {2208.04933},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.04933},
  url = {http://arxiv.org/abs/2208.04933},
  urldate = {2022-12-13},
  abstract = {Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4\% on the long range arena benchmark, and 98.5\% on the most difficult Path-X task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/SLDV5KSN/Smith et al. - 2022 - Simplified State Space Layers for Sequence Modelin.pdf;/Users/alexandre.piche/Zotero/storage/B6B97VY4/2208.html}
}

@misc{smithWalkParkLearning2022,
  title = {A {{Walk}} in the {{Park}}: {{Learning}} to {{Walk}} in 20 {{Minutes With Model-Free Reinforcement Learning}}},
  shorttitle = {A {{Walk}} in the {{Park}}},
  author = {Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  date = {2022-08-16},
  number = {arXiv:2208.07860},
  eprint = {2208.07860},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.07860},
  url = {http://arxiv.org/abs/2208.07860},
  urldate = {2022-12-13},
  abstract = {Deep reinforcement learning is a promising approach to learning policies in uncontrolled environments that do not require domain knowledge. Unfortunately, due to sample inefficiency, deep RL applications have primarily focused on simulated environments. In this work, we demonstrate that the recent advancements in machine learning algorithms and libraries combined with a carefully tuned robot controller lead to learning quadruped locomotion in only 20 minutes in the real world. We evaluate our approach on several indoor and outdoor terrains which are known to be challenging for classical model-based controllers. We observe the robot to be able to learn walking gait consistently on all of these terrains. Finally, we evaluate our design decisions in a simulated environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/4VSF9L6J/Smith et al. - 2022 - A Walk in the Park Learning to Walk in 20 Minutes.pdf;/Users/alexandre.piche/Zotero/storage/794ZUVN3/2208.html}
}

@online{Software2022,
  title = {{{Software}}²},
  date = {2022-11-15T08:00:00+00:00},
  url = {https://blog.minch.co/2022/11/15/software-squared.html},
  urldate = {2022-11-17},
  abstract = {We are currently at the cusp of transitioning from “learning from data” to “learning what data to learn from” as the central focus of AI research. State-of-the-art deep learning models, like GPT‑[X] and Stable Diffusion, have been described as data sponges,1 capable of modeling immense amounts of data.2 These large generative models, many based on the transformer architecture, can model massive datasets, learning to produce images, video, audio, code, and data in many other domains at a quality that begins to rival that of samples authored by human experts. Growing evidence suggests the generality of such large models is largely limited by the quality of the training data. Yet, mainstream training practices are not inherently data-seeking. Instead, they ignore the specific quality of information within the training data in favor of maximizing data quantity. This discrepancy hints at a likely, major shift in research focus in the coming years, toward innovating directly on data collection and generation as a principal way to improve model performance. To our knowledge, the term “data sponge” was first coined in Eric Jang’s excellent article, “Just Ask for Generalization.” https://evjang.com/2021/10/23/generalization.html.~↩ The recent Stable Diffusion model effectively compresses approximately 100GB of training data into a mere 2GB of model weights.~↩},
  langid = {english},
  organization = {{Minqi Jiang}},
  file = {/Users/alexandre.piche/Zotero/storage/6WRR8M3F/software-squared.html}
}

@online{sohl-dicksteinTooMuchEfficiency2022,
  title = {Too Much Efficiency Makes Everything Worse: Overfitting and the Strong Version of {{Goodhart}}’s Law},
  shorttitle = {Too Much Efficiency Makes Everything Worse},
  author = {Sohl-Dickstein, Jascha},
  date = {2022-11-06T00:00:00+00:00},
  url = {http://sohl-dickstein.github.io/2022/11/06/strong-Goodhart.html},
  urldate = {2022-11-09},
  abstract = {This blog is intended to be a place to share ideas and results that are too weird, incomplete, or off-topic to turn into an academic paper, but that I think may be important. Let me know what you think! Contact links to the left.}
}

@misc{songHowTrainYour2021,
  title = {How to {{Train Your Energy-Based Models}}},
  author = {Song, Yang and Kingma, Diederik P.},
  date = {2021-02-17},
  number = {arXiv:2101.03288},
  eprint = {2101.03288},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2101.03288},
  urldate = {2022-08-12},
  abstract = {Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/KYJPGREW/Song and Kingma - 2021 - How to Train Your Energy-Based Models.pdf;/Users/alexandre.piche/Zotero/storage/TIV9D9R7/2101.html}
}

@misc{sorscherNeuralScalingLaws2022,
  title = {Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning},
  shorttitle = {Beyond Neural Scaling Laws},
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  date = {2022-11-15},
  number = {arXiv:2206.14486},
  eprint = {2206.14486},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.14486},
  urldate = {2022-12-13},
  abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how both in theory and practice we can break beyond power law scaling and reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this new exponential scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling performance on ResNets trained on CIFAR10, SVHN, and ImageNet. Given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/6UDVT8N2/Sorscher et al. - 2022 - Beyond neural scaling laws beating power law scal.pdf}
}

@misc{stiennonLearningSummarizeHuman2022,
  title = {Learning to Summarize from Human Feedback},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
  date = {2022-02-15},
  number = {arXiv:2009.01325},
  eprint = {2009.01325},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2009.01325},
  urldate = {2022-10-27},
  abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/DZQVWDHV/Stiennon et al. - 2022 - Learning to summarize from human feedback.pdf;/Users/alexandre.piche/Zotero/storage/P8KJ46YT/2009.html}
}

@misc{strayBuildingHumanValues2022,
  title = {Building {{Human Values}} into {{Recommender Systems}}: {{An Interdisciplinary Synthesis}}},
  shorttitle = {Building {{Human Values}} into {{Recommender Systems}}},
  author = {Stray, Jonathan and Halevy, Alon and Assar, Parisa and Hadfield-Menell, Dylan and Boutilier, Craig and Ashar, Amar and Beattie, Lex and Ekstrand, Michael and Leibowicz, Claire and Sehat, Connie Moon and Johansen, Sara and Kerlin, Lianne and Vickrey, David and Singh, Spandana and Vrijenhoek, Sanne and Zhang, Amy and Andrus, McKane and Helberger, Natali and Proutskova, Polina and Mitra, Tanushree and Vasan, Nina},
  date = {2022-07-20},
  number = {arXiv:2207.10192},
  eprint = {2207.10192},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.10192},
  urldate = {2022-08-03},
  abstract = {Recommender systems are the algorithms which select, filter, and personalize content across many of the worlds largest platforms and apps. As such, their positive and negative effects on individuals and on societies have been extensively theorized and studied. Our overarching question is how to ensure that recommender systems enact the values of the individuals and societies that they serve. Addressing this question in a principled fashion requires technical knowledge of recommender design and operation, and also critically depends on insights from diverse fields including social science, ethics, economics, psychology, policy and law. This paper is a multidisciplinary effort to synthesize theory and practice from different perspectives, with the goal of providing a shared language, articulating current design approaches, and identifying open problems. It is not a comprehensive survey of this large space, but a set of highlights identified by our diverse author cohort. We collect a set of values that seem most relevant to recommender systems operating across different domains, then examine them from the perspectives of current industry practice, measurement, product design, and policy approaches. Important open problems include multi-stakeholder processes for defining values and resolving trade-offs, better values-driven measurements, recommender controls that people use, non-behavioral algorithmic feedback, optimization for long-term outcomes, causal inference of recommender effects, academic-industry research collaborations, and interdisciplinary policy-making.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Social and Information Networks,H.3.3,J.4,K.4.2},
  file = {/Users/alexandre.piche/Zotero/storage/HYKIFDG4/Stray et al. - 2022 - Building Human Values into Recommender Systems An.pdf;/Users/alexandre.piche/Zotero/storage/56ANKD8D/2207.html}
}

@misc{strouseCollaboratingHumansHuman2022,
  title = {Collaborating with {{Humans}} without {{Human Data}}},
  author = {Strouse, D. J. and McKee, Kevin R. and Botvinick, Matt and Hughes, Edward and Everett, Richard},
  date = {2022-01-07},
  number = {arXiv:2110.08176},
  eprint = {2110.08176},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.08176},
  urldate = {2022-08-03},
  abstract = {Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train "human-aware" agents ("behavioral cloning play", or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/alexandre.piche/Zotero/storage/7TDSKF3I/Strouse et al. - 2022 - Collaborating with Humans without Human Data.pdf;/Users/alexandre.piche/Zotero/storage/XHLAD7NV/2110.html}
}

@misc{thoppilanLaMDALanguageModels2022,
  title = {{{LaMDA}}: {{Language Models}} for {{Dialog Applications}}},
  shorttitle = {{{LaMDA}}},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  date = {2022-02-10},
  number = {arXiv:2201.08239},
  eprint = {2201.08239},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.08239},
  urldate = {2022-11-18},
  abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/C8GPPFCZ/Thoppilan et al. - 2022 - LaMDA Language Models for Dialog Applications.pdf;/Users/alexandre.piche/Zotero/storage/B5GPKQ4V/2201.html}
}

@misc{tranPlexReliabilityUsing2022,
  title = {Plex: {{Towards Reliability}} Using {{Pretrained Large Model Extensions}}},
  shorttitle = {Plex},
  author = {Tran, Dustin and Liu, Jeremiah and Dusenberry, Michael W. and Phan, Du and Collier, Mark and Ren, Jie and Han, Kehang and Wang, Zi and Mariet, Zelda and Hu, Huiyi and Band, Neil and Rudner, Tim G. J. and Singhal, Karan and Nado, Zachary and van Amersfoort, Joost and Kirsch, Andreas and Jenatton, Rodolphe and Thain, Nithum and Yuan, Honglin and Buchanan, Kelly and Murphy, Kevin and Sculley, D. and Gal, Yarin and Ghahramani, Zoubin and Snoek, Jasper and Lakshminarayanan, Balaji},
  options = {useprefix=true},
  date = {2022-07-15},
  number = {arXiv:2207.07411},
  eprint = {2207.07411},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.07411},
  urldate = {2022-12-13},
  abstract = {A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/IHVUBL6P/Tran et al. - 2022 - Plex Towards Reliability using Pretrained Large M.pdf}
}

@misc{tranPlexReliabilityUsing2022a,
  title = {Plex: {{Towards Reliability}} Using {{Pretrained Large Model Extensions}}},
  shorttitle = {Plex},
  author = {Tran, Dustin and Liu, Jeremiah and Dusenberry, Michael W. and Phan, Du and Collier, Mark and Ren, Jie and Han, Kehang and Wang, Zi and Mariet, Zelda and Hu, Huiyi and Band, Neil and Rudner, Tim G. J. and Singhal, Karan and Nado, Zachary and van Amersfoort, Joost and Kirsch, Andreas and Jenatton, Rodolphe and Thain, Nithum and Yuan, Honglin and Buchanan, Kelly and Murphy, Kevin and Sculley, D. and Gal, Yarin and Ghahramani, Zoubin and Snoek, Jasper and Lakshminarayanan, Balaji},
  options = {useprefix=true},
  date = {2022-07-15},
  number = {arXiv:2207.07411},
  eprint = {2207.07411},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.07411},
  url = {http://arxiv.org/abs/2207.07411},
  urldate = {2022-12-13},
  abstract = {A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/NWQDTJM5/Tran et al. - 2022 - Plex Towards Reliability using Pretrained Large M.pdf;/Users/alexandre.piche/Zotero/storage/XUH9QKUU/2207.html}
}

@online{Tutorial17Transformers,
  title = {Tutorial \#17: {{Transformers III Training}} - {{Borealis AI}}},
  shorttitle = {Tutorial \#17},
  url = {https://www.borealisai.com/research-blogs/tutorial-17-transformers-iii-training/},
  urldate = {2022-11-07}
}

@online{UnderstandLanguageUnderstand2021,
  title = {To {{Understand Language}} Is to {{Understand Generalization}}},
  date = {2021-12-17T00:00:00+00:00},
  url = {https://evjang.com/2021/12/17/lang-generalization.html},
  urldate = {2022-08-13},
  abstract = {In my essay “Just ask for Generalization”, I argued that some optimization capabilities, such as reinforcement learning from sub-optimal trajectories, might be better implemented by generalization than by construction. We have to generalize to unseen situations at deployment time anyway, so why not focus on generalization capability as the first class citizen, and then “just ask for optimality” as an unseen case? A corollary to this design philosophy is that we should discard inductive biases that introduce optimization bottlenecks for “the data sponge”: if an inductive bias turns out to be merely “data in disguise”, it may not only cease to provide a benefit in the high data regime, but actually hinder the model on examples where the inductive bias no longer applies.},
  langid = {english},
  organization = {{Eric Jang}},
  file = {/Users/alexandre.piche/Zotero/storage/BI9XZ4JY/2021 - To Understand Language is to Understand Generaliza.pdf;/Users/alexandre.piche/Zotero/storage/EMHFKLEW/lang-generalization.html}
}

@misc{urainComposableEnergyPolicies2021,
  title = {Composable {{Energy Policies}} for {{Reactive Motion Generation}} and {{Reinforcement Learning}}},
  author = {Urain, Julen and Li, Anqi and Liu, Puze and D'Eramo, Carlo and Peters, Jan},
  date = {2021-05-11},
  number = {arXiv:2105.04962},
  eprint = {2105.04962},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.04962},
  url = {http://arxiv.org/abs/2105.04962},
  urldate = {2022-12-13},
  abstract = {Reactive motion generation problems are usually solved by computing actions as a sum of policies. However, these policies are independent of each other and thus, they can have conflicting behaviors when summing their contributions together. We introduce Composable Energy Policies (CEP), a novel framework for modular reactive motion generation. CEP computes the control action by optimization over the product of a set of stochastic policies. This product of policies will provide a high probability to those actions that satisfy all the components and low probability to the others. Optimizing over the product of the policies avoids the detrimental effect of conflicting behaviors between policies choosing an action that satisfies all the objectives. Besides, we show that CEP naturally adapts to the Reinforcement Learning problem allowing us to integrate, in a hierarchical fashion, any distribution as prior, from multimodal distributions to non-smooth distributions and learn a new policy given them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/U2P57WUU/Urain et al. - 2021 - Composable Energy Policies for Reactive Motion Gen.pdf;/Users/alexandre.piche/Zotero/storage/S38BQFSB/2105.html}
}

@misc{vaitlGradientsShouldStay2022,
  title = {Gradients Should Stay on {{Path}}: {{Better Estimators}} of the {{Reverse-}} and {{Forward KL Divergence}} for {{Normalizing Flows}}},
  shorttitle = {Gradients Should Stay on {{Path}}},
  author = {Vaitl, Lorenz and Nicoli, Kim A. and Nakajima, Shinichi and Kessel, Pan},
  date = {2022-07-17},
  number = {arXiv:2207.08219},
  eprint = {2207.08219},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.08219},
  urldate = {2022-12-13},
  abstract = {We propose an algorithm to estimate the path-gradient of both the reverse and forward Kullback–Leibler divergence for an arbitrary manifestly invertible normalizing flow. The resulting path-gradient estimators are straightforward to implement, have lower variance, and lead not only to faster convergence of training but also to better overall approximation results compared to standard total gradient estimators. We also demonstrate that path-gradient training is less susceptible to mode-collapse. In light of our results, we expect that path-gradient estimators will become the new standard method to train normalizing flows for variational inference.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/8BEK6E86/Vaitl et al. - 2022 - Gradients should stay on Path Better Estimators o.pdf}
}

@misc{wangDiffusionPoliciesExpressive2022,
  title = {Diffusion {{Policies}} as an {{Expressive Policy Class}} for {{Offline Reinforcement Learning}}},
  author = {Wang, Zhendong and Hunt, Jonathan J. and Zhou, Mingyuan},
  date = {2022-08-12},
  number = {arXiv:2208.06193},
  eprint = {2208.06193},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.06193},
  urldate = {2022-08-15},
  abstract = {Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly at this task due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness and sometimes result in substantially suboptimal solutions. In this paper, we propose Diffusion-QL that utilizes a conditional diffusion model as a highly expressive policy class for behavior cloning and policy regularization. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of a conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate our method and prior work in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks for offline RL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/6GP8WFC8/Wang et al. - 2022 - Diffusion Policies as an Expressive Policy Class f.pdf}
}

@misc{wangSelfInstructAligningLanguage2022,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  date = {2022-12-20},
  number = {arXiv:2212.10560},
  eprint = {2212.10560},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.10560},
  url = {http://arxiv.org/abs/2212.10560},
  urldate = {2022-12-21},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/FMFF6MWR/Wang et al. - 2022 - Self-Instruct Aligning Language Model with Self Generated Instructions.pdf}
}

@misc{wangSelfInstructAligningLanguage2022a,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  date = {2022-12-20},
  number = {arXiv:2212.10560},
  eprint = {2212.10560},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.10560},
  url = {http://arxiv.org/abs/2212.10560},
  urldate = {2022-12-26},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/F5Z2KGJ7/Wang et al. - 2022 - Self-Instruct Aligning Language Model with Self Generated Instructions.pdf}
}

@misc{weiChainThoughtPrompting2022,
  title = {Chain of {{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2022-10-10},
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2022-11-23},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/PN9UKNFN/Wei et al. - 2022 - Chain of Thought Prompting Elicits Reasoning in La.pdf;/Users/alexandre.piche/Zotero/storage/QXV467WM/2201.html}
}

@misc{xieExplanationIncontextLearning2022,
  title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian Inference}}},
  author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  date = {2022-07-21},
  number = {arXiv:2111.02080},
  eprint = {2111.02080},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.02080},
  urldate = {2022-12-07},
  abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/R995EXSN/Xie et al. - 2022 - An Explanation of In-context Learning as Implicit .pdf;/Users/alexandre.piche/Zotero/storage/SBMR7W4F/2111.html}
}

@misc{xieExplanationIncontextLearning2022a,
  title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian Inference}}},
  author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  date = {2022-07-21},
  number = {arXiv:2111.02080},
  eprint = {2111.02080},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.02080},
  urldate = {2022-12-13},
  abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning1. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/YXU39759/Xie et al. - 2022 - An Explanation of In-context Learning as Implicit .pdf}
}

@misc{xuLearningNewSkills2022,
  title = {Learning {{New Skills}} after {{Deployment}}: {{Improving}} Open-Domain Internet-Driven Dialogue with Human Feedback},
  shorttitle = {Learning {{New Skills}} after {{Deployment}}},
  author = {Xu, Jing and Ung, Megan and Komeili, Mojtaba and Arora, Kushal and Boureau, Y.-Lan and Weston, Jason},
  date = {2022-08-16},
  number = {arXiv:2208.03270},
  eprint = {2208.03270},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.03270},
  urldate = {2022-11-30},
  abstract = {Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In this work we study how to improve internet-driven conversational skills in such a learning framework. We collect deployment data, which we make publicly available, of human interactions, and collect various types of human feedback -- including binary quality measurements, free-form text feedback, and fine-grained reasons for failure. We then study various algorithms for improving from such feedback, including standard supervised learning, rejection sampling, model-guiding and reward-based learning, in order to make recommendations on which type of feedback and algorithms work best. We find the recently introduced Director model (Arora et al., '22) shows significant improvements over other existing approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/5W7XHBUQ/Xu et al. - 2022 - Learning New Skills after Deployment Improving op.pdf;/Users/alexandre.piche/Zotero/storage/RAAP46AW/2208.html}
}

@misc{yangDichotomyControlSeparating2022,
  title = {Dichotomy of {{Control}}: {{Separating What You Can Control}} from {{What You Cannot}}},
  shorttitle = {Dichotomy of {{Control}}},
  author = {Yang, Mengjiao and Schuurmans, Dale and Abbeel, Pieter and Nachum, Ofir},
  date = {2022-10-24},
  number = {arXiv:2210.13435},
  eprint = {2210.13435},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.13435},
  url = {http://arxiv.org/abs/2210.13435},
  urldate = {2022-12-13},
  abstract = {Future- or return-conditioned supervised learning is an emerging paradigm for offline reinforcement learning (RL), where the future outcome (i.e., return) associated with an observed action sequence is used as input to a policy trained to imitate those same actions. While return-conditioning is at the heart of popular algorithms such as decision transformer (DT), these methods tend to perform poorly in highly stochastic environments, where an occasional high return can arise from randomness in the environment rather than the actions themselves. Such situations can lead to a learned policy that is inconsistent with its conditioning inputs; i.e., using the policy to act in the environment, when conditioning on a specific desired return, leads to a distribution of real returns that is wildly different than desired. In this work, we propose the dichotomy of control (DoC), a future-conditioned supervised learning framework that separates mechanisms within a policy's control (actions) from those beyond a policy's control (environment stochasticity). We achieve this separation by conditioning the policy on a latent variable representation of the future, and designing a mutual information constraint that removes any information from the latent variable associated with randomness in the environment. Theoretically, we show that DoC yields policies that are consistent with their conditioning inputs, ensuring that conditioning a learned policy on a desired high-return future outcome will correctly induce high-return behavior. Empirically, we show that DoC is able to achieve significantly better performance than DT on environments that have highly stochastic rewards and transition},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/7W3WZGBR/Yang et al. - 2022 - Dichotomy of Control Separating What You Can Cont.pdf;/Users/alexandre.piche/Zotero/storage/LQGTWXV7/2210.html}
}

@misc{yangMultiTaskReinforcementLearning2020,
  title = {Multi-{{Task Reinforcement Learning}} with {{Soft Modularization}}},
  author = {Yang, Ruihan and Xu, Huazhe and Wu, Yi and Wang, Xiaolong},
  date = {2020-12-07},
  number = {arXiv:2003.13661},
  eprint = {2003.13661},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.13661},
  url = {http://arxiv.org/abs/2003.13661},
  urldate = {2022-12-13},
  abstract = {Multi-task learning is a very challenging problem in reinforcement learning. While training multiple tasks jointly allow the policies to share parameters across different tasks, the optimization problem becomes non-trivial: It remains unclear what parameters in the network should be reused across tasks, and how the gradients from different tasks may interfere with each other. Thus, instead of naively sharing parameters across tasks, we introduce an explicit modularization technique on policy representation to alleviate this optimization issue. Given a base policy network, we design a routing network which estimates different routing strategies to reconfigure the base network for each task. Instead of directly selecting routes for each task, our task-specific policy uses a method called soft modularization to softly combine all the possible routes, which makes it suitable for sequential tasks. We experiment with various robotics manipulation tasks in simulation and show our method improves both sample efficiency and performance over strong baselines by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/ILAJ2TQU/Yang et al. - 2020 - Multi-Task Reinforcement Learning with Soft Modula.pdf;/Users/alexandre.piche/Zotero/storage/SJJ48KP4/2003.html}
}

@report{yangNowWhatSequence2022,
  type = {preprint},
  title = {Now {{What Sequence}}? {{Pre-trained Ensembles}} for {{Bayesian Optimization}} of {{Protein Sequences}}},
  shorttitle = {Now {{What Sequence}}?},
  author = {Yang, Ziyue and Milas, Katarina A. and White, Andrew D.},
  date = {2022-08-06},
  institution = {{Bioinformatics}},
  doi = {10.1101/2022.08.05.502972},
  url = {http://biorxiv.org/lookup/doi/10.1101/2022.08.05.502972},
  urldate = {2022-12-13},
  abstract = {Pre-trained models have been transformative in natural language, computer vision, and now protein sequences by enabling accuracy with few training examples. We show how to use pre-trained sequence models in Bayesian optimization to design new protein sequences with minimal labels (i.e., few experiments). Pre-trained models give good predictive accuracy at low data and Bayesian optimization guides the choice of which sequences to test. Pre-trained sequence models also remove the common requirement of having a list of possible experiments. Any sequence can be considered. We show significantly fewer labeled sequences are required for three sequence design tasks, including creating novel peptide inhibitors with AlphaFold. These de novo peptide inhibitors require only sequence information, no known protein-protein structures, and we can predict highly-efficient binders with less than 10 AlphaFold calculations.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/ZLSEGC3V/Yang et al. - 2022 - Now What Sequence Pre-trained Ensembles for Bayes.pdf}
}

@misc{yaoReActSynergizingReasoning2022,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  date = {2022-11-27},
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.03629},
  urldate = {2022-12-13},
  abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/HR8D52BP/Yao et al. - 2022 - ReAct Synergizing Reasoning and Acting in Languag.pdf}
}

@misc{yeNeuralStoryPlanning2022,
  title = {Neural {{Story Planning}}},
  author = {Ye, Anbang and Cui, Christopher and Shi, Taiwei and Riedl, Mark O.},
  date = {2022-12-16},
  number = {arXiv:2212.08718},
  eprint = {2212.08718},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.08718},
  url = {http://arxiv.org/abs/2212.08718},
  urldate = {2022-12-22},
  abstract = {Automated plot generation is the challenge of generating a sequence of events that will be perceived by readers as the plot of a coherent story. Traditional symbolic planners plan a story from a goal state and guarantee logical causal plot coherence but rely on a library of hand-crafted actions with their preconditions and effects. This closed world setting limits the length and diversity of what symbolic planners can generate. On the other hand, pre-trained neural language models can generate stories with great diversity, while being generally incapable of ending a story in a specified manner and can have trouble maintaining coherence. In this paper, we present an approach to story plot generation that unifies causal planning with neural language models. We propose to use commonsense knowledge extracted from large language models to recursively expand a story plot in a backward chaining fashion. Specifically, our system infers the preconditions for events in the story and then events that will cause those conditions to become true. We performed automatic evaluation to measure narrative coherence as indicated by the ability to answer questions about whether different events in the story are causally related to other events. Results indicate that our proposed method produces more coherent plotlines than several strong baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/78KP4NNL/Ye et al. - 2022 - Neural Story Planning.pdf;/Users/alexandre.piche/Zotero/storage/9XCFUY3V/2212.html}
}

@misc{zengSocraticModelsComposing2022,
  title = {Socratic {{Models}}: {{Composing Zero-Shot Multimodal Reasoning}} with {{Language}}},
  shorttitle = {Socratic {{Models}}},
  author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  date = {2022-05-27},
  number = {arXiv:2204.00598},
  eprint = {2204.00598},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.00598},
  urldate = {2022-08-05},
  abstract = {Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/DC34S9RK/Zeng et al. - 2022 - Socratic Models Composing Zero-Shot Multimodal Re.pdf;/Users/alexandre.piche/Zotero/storage/XFVU4ZD3/2204.html}
}

@unpublished{zhangPathfinderParallelQuasiNewton2022,
  title = {Pathfinder: {{Parallel}} Quasi-{{Newton}} Variational Inference},
  shorttitle = {Pathfinder},
  author = {Zhang, Lu and Carpenter, Bob and Gelman, Andrew and Vehtari, Aki},
  date = {2022-05-16},
  eprint = {2108.03782},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2108.03782},
  urldate = {2022-10-01},
  abstract = {We propose Pathfinder, a variational method for approximately sampling from differentiable log densities. Starting from a random initialization, Pathfinder locates normal approximations to the target density along a quasi-Newton optimization path, with local covariance estimated using the inverse Hessian estimates produced by the optimizer. Pathfinder returns draws from the approximation with the lowest estimated Kullback-Leibler (KL) divergence to the true posterior. We evaluate Pathfinder on a wide range of posterior distributions, demonstrating that its approximate draws are better than those from automatic differentiation variational inference (ADVI) and comparable to those produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as measured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC runs, Pathfinder requires one to two orders of magnitude fewer log density and gradient evaluations, with greater reductions for more challenging posteriors. Importance resampling over multiple runs of Pathfinder improves the diversity of approximate draws, reducing 1-Wasserstein distance further and providing a measure of robustness to optimization failures on plateaus, saddle points, or in minor modes. The Monte Carlo KL divergence estimates are embarrassingly parallelizable in the core Pathfinder algorithm, as are multiple runs in the resampling version, further increasing Pathfinder's speed advantage with multiple cores.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/NNLR33NF/Zhang et al. - 2022 - Pathfinder Parallel quasi-Newton variational inference.pdf}
}

@misc{zhangTEMPERATestTimePrompting2022,
  title = {{{TEMPERA}}: {{Test-Time Prompting}} via {{Reinforcement Learning}}},
  shorttitle = {{{TEMPERA}}},
  author = {Zhang, Tianjun and Wang, Xuezhi and Zhou, Denny and Schuurmans, Dale and Gonzalez, Joseph E.},
  date = {2022-11-21},
  number = {arXiv:2211.11890},
  eprint = {2211.11890},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.11890},
  url = {http://arxiv.org/abs/2211.11890},
  urldate = {2022-12-13},
  abstract = {Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/5JHYW9XN/Zhang et al. - 2022 - TEMPERA Test-Time Prompting via Reinforcement Lea.pdf;/Users/alexandre.piche/Zotero/storage/IE7C2RJA/2211.html}
}
