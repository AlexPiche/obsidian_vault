
@misc{baiTrainingHelpfulHarmless2022,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  date = {2022-04-12},
  number = {arXiv:2204.05862},
  eprint = {2204.05862},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.05862},
  urldate = {2022-07-27},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/UJI5YEEC/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf;/Users/alexandre.piche/Zotero/storage/9X96A3ML/2204.html}
}

@misc{bavarianEfficientTrainingLanguage2022,
  title = {Efficient {{Training}} of {{Language Models}} to {{Fill}} in the {{Middle}}},
  author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  date = {2022-07-28},
  number = {arXiv:2207.14255},
  eprint = {2207.14255},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.14255},
  urldate = {2022-08-03},
  abstract = {We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/JK89Q438/Bavarian et al. - 2022 - Efficient Training of Language Models to Fill in t.pdf;/Users/alexandre.piche/Zotero/storage/9FS9VVLY/2207.html}
}

@misc{blondelLearningEnergyNetworks2022,
  title = {Learning {{Energy Networks}} with {{Generalized Fenchel-Young Losses}}},
  author = {Blondel, Mathieu and Llinares-López, Felipe and Dadashi, Robert and Hussenot, Léonard and Geist, Matthieu},
  date = {2022-05-19},
  number = {arXiv:2205.09589},
  eprint = {2205.09589},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.09589},
  urldate = {2022-07-18},
  abstract = {Energy-based models, a.k.a. energy networks, perform inference by optimizing an energy function, typically parametrized by a neural network. This allows one to capture potentially complex relationships between inputs and outputs. To learn the parameters of the energy function, the solution to that optimization problem is typically fed into a loss function. The key challenge for training energy networks lies in computing loss gradients, as this typically requires argmin/argmax differentiation. In this paper, building upon a generalized notion of conjugate function, which replaces the usual bilinear pairing with a general energy function, we propose generalized Fenchel-Young losses, a natural loss construction for learning energy networks. Our losses enjoy many desirable properties and their gradients can be computed efficiently without argmin/argmax differentiation. We also prove the calibration of their excess risk in the case of linear-concave energies. We demonstrate our losses on multilabel classification and imitation learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/QQC3AJZA/Blondel et al. - 2022 - Learning Energy Networks with Generalized Fenchel-.pdf;/Users/alexandre.piche/Zotero/storage/LJ7SD4MD/2205.html}
}

@inproceedings{carrollEstimatingPenalizingInduced2022,
  title = {Estimating and {{Penalizing Induced Preference Shifts}} in {{Recommender Systems}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Carroll, Micah D. and Dragan, Anca and Russell, Stuart and Hadfield-Menell, Dylan},
  date = {2022-06-28},
  pages = {2686--2708},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/carroll22a.html},
  urldate = {2022-07-30},
  abstract = {The content that a recommender system (RS) shows to users influences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce specific internal states in users. Even more, systems trained via long-horizon optimization will have direct incentives to manipulate users, e.g. shift their preferences so they are easier to satisfy. We focus on induced preference shifts in users. We argue that \{–\} before deployment \{–\} system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical policies would influence user preferences if deployed \{–\} we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such influences are manipulative or otherwise unwanted \{–\} we use the notion of "safe shifts", that define a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed "safe". In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/33SGG9GL/Carroll et al. - 2022 - Estimating and Penalizing Induced Preference Shift.pdf}
}

@misc{chenLearningUniversalHyperparameter2022,
  title = {Towards {{Learning Universal Hyperparameter Optimizers}} with {{Transformers}}},
  author = {Chen, Yutian and Song, Xingyou and Lee, Chansoo and Wang, Zi and Zhang, Qiuyi and Dohan, David and Kawakami, Kazuya and Kochanski, Greg and Doucet, Arnaud and Ranzato, Marc'aurelio and Perel, Sagi and de Freitas, Nando},
  options = {useprefix=true},
  date = {2022-05-26},
  number = {arXiv:2205.13320},
  eprint = {2205.13320},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.13320},
  urldate = {2022-07-28},
  abstract = {Meta-learning hyperparameter optimization (HPO) algorithms from prior experiments is a promising approach to improve optimization efficiency over objective functions from a similar distribution. However, existing methods are restricted to learning from experiments sharing the same set of hyperparameters. In this paper, we introduce the OptFormer, the first text-based Transformer HPO framework that provides a universal end-to-end interface for jointly learning policy and function prediction when trained on vast tuning data from the wild. Our extensive experiments demonstrate that the OptFormer can imitate at least 7 different HPO algorithms, which can be further improved via its function uncertainty estimates. Compared to a Gaussian Process, the OptFormer also learns a robust prior distribution for hyperparameter response functions, and can thereby provide more accurate and better calibrated predictions. This work paves the path to future extensions for training a Transformer-based model as a general HPO optimizer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/B74YXQY6/Chen et al. - 2022 - Towards Learning Universal Hyperparameter Optimize.pdf;/Users/alexandre.piche/Zotero/storage/CIJXJCMW/2205.html}
}

@misc{chowMixtureofExpertApproachRLbased2022,
  title = {A {{Mixture-of-Expert Approach}} to {{RL-based Dialogue Management}}},
  author = {Chow, Yinlam and Tulepbergenov, Aza and Nachum, Ofir and Ryu, MoonKyung and Ghavamzadeh, Mohammad and Boutilier, Craig},
  date = {2022-05-31},
  number = {arXiv:2206.00059},
  eprint = {2206.00059},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.00059},
  urldate = {2022-08-03},
  abstract = {Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge. We use reinforcement learning (RL) to develop a dialogue agent that avoids being short-sighted (outputting generic utterances) and maximizes overall user satisfaction. Most existing RL approaches to DM train the agent at the word-level, and thus, have to deal with a combinatorially complex action space even for a medium-size vocabulary. As a result, they struggle to produce a successful and engaging dialogue even if they are warm-started with a pre-trained LM. To address this issue, we develop a RL-based DM using a novel mixture of expert language model (MoE-LM) that consists of (i) a LM capable of learning diverse semantics for conversation histories, (ii) a number of \{\textbackslash em specialized\} LMs (or experts) capable of generating utterances corresponding to a particular attribute or personality, and (iii) a RL-based DM that performs dialogue planning with the utterances generated by the experts. Our MoE approach provides greater flexibility to generate sensible utterances with different intents and allows RL to focus on conversational-level DM. We compare it with SOTA baselines on open-domain dialogues and demonstrate its effectiveness both in terms of the diversity and sensibility of the generated utterances and the overall DM performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/5HGIFXUM/Chow et al. - 2022 - A Mixture-of-Expert Approach to RL-based Dialogue .pdf;/Users/alexandre.piche/Zotero/storage/U3AWZCYH/2206.html}
}

@misc{duLearningIterativeReasoning2022,
  title = {Learning {{Iterative Reasoning}} through {{Energy Minimization}}},
  author = {Du, Yilun and Li, Shuang and Tenenbaum, Joshua B. and Mordatch, Igor},
  date = {2022-06-30},
  number = {arXiv:2206.15448},
  eprint = {2206.15448},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.15448},
  urldate = {2022-07-18},
  abstract = {Deep learning has excelled on complex pattern recognition tasks such as image classification and object recognition. However, it struggles with tasks requiring nontrivial reasoning, such as algorithmic computation. Humans are able to solve such tasks through iterative reasoning -- spending more time thinking about harder tasks. Most existing neural networks, however, exhibit a fixed computational budget controlled by the neural network architecture, preventing additional computational processing on harder tasks. In this work, we present a new framework for iterative reasoning with neural networks. We train a neural network to parameterize an energy landscape over all outputs, and implement each step of the iterative reasoning as an energy minimization step to find a minimal energy solution. By formulating reasoning as an energy minimization problem, for harder problems that lead to more complex energy landscapes, we may then adjust our underlying computational budget by running a more complex optimization procedure. We empirically illustrate that our iterative reasoning approach can solve more accurate and generalizable algorithmic reasoning tasks in both graph and continuous domains. Finally, we illustrate that our approach can recursively solve algorithmic problems requiring nested reasoning},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/GNSI85PS/Du et al. - 2022 - Learning Iterative Reasoning through Energy Minimi.pdf;/Users/alexandre.piche/Zotero/storage/82WE25JQ/2206.html}
}

@misc{elazarMeasuringCausalEffects2022,
  title = {Measuring {{Causal Effects}} of {{Data Statistics}} on {{Language Model}}'s `{{Factual}}' {{Predictions}}},
  author = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Feder, Amir and Ravichander, Abhilasha and Mosbach, Marius and Belinkov, Yonatan and Schütze, Hinrich and Goldberg, Yoav},
  date = {2022-07-28},
  number = {arXiv:2207.14251},
  eprint = {2207.14251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.14251},
  urldate = {2022-08-03},
  abstract = {Large amounts of training data are one of the major reasons for the high performance of state-of-the-art NLP models. But what exactly in the training data causes a model to make a certain prediction? We seek to answer this question by providing a language for describing how training data influences predictions, through a causal framework. Importantly, our framework bypasses the need to retrain expensive models and allows us to estimate causal effects based on observational data alone. Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do influence the predictions of PLMs, suggesting that such models rely on shallow heuristics. Our causal framework and our results demonstrate the importance of studying datasets and the benefits of causality for understanding NLP models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/alexandre.piche/Zotero/storage/XTFCWTNX/Elazar et al. - 2022 - Measuring Causal Effects of Data Statistics on Lan.pdf;/Users/alexandre.piche/Zotero/storage/23GDH3HR/2207.html}
}

@misc{faccioGeneralPolicyEvaluation2022,
  title = {General {{Policy Evaluation}} and {{Improvement}} by {{Learning}} to {{Identify Few But Crucial States}}},
  author = {Faccio, Francesco and Ramesh, Aditya and Herrmann, Vincent and Harb, Jean and Schmidhuber, Jürgen},
  date = {2022-07-04},
  number = {arXiv:2207.01566},
  eprint = {2207.01566},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.01566},
  urldate = {2022-08-04},
  abstract = {Learning to evaluate and improve policies is a core problem of Reinforcement Learning (RL). Traditional RL algorithms learn a value function defined for a single policy. A recently explored competitive alternative is to learn a single value function for many policies. Here we combine the actor-critic architecture of Parameter-Based Value Functions and the policy embedding of Policy Evaluation Networks to learn a single value function for evaluating (and thus helping to improve) any policy represented by a deep neural network (NN). The method yields competitive experimental results. In continuous control problems with infinitely many states, our value function minimizes its prediction error by simultaneously learning a small set of `probing states' and a mapping from actions produced in probing states to the policy's return. The method extracts crucial abstract knowledge about the environment in form of very few states sufficient to fully specify the behavior of many policies. A policy improves solely by changing actions in probing states, following the gradient of the value function's predictions. Surprisingly, it is possible to clone the behavior of a near-optimal policy in Swimmer-v3 and Hopper-v3 environments only by knowing how to act in 3 and 5 such learned states, respectively. Remarkably, our value function trained to evaluate NN policies is also invariant to changes of the policy architecture: we show that it allows for zero-shot learning of linear policies competitive with the best policy seen during training. Our code is public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/7Y2S276F/Faccio et al. - 2022 - General Policy Evaluation and Improvement by Learn.pdf;/Users/alexandre.piche/Zotero/storage/BRYDA5LS/2207.html}
}

@misc{faccioGoalConditionedGeneratorsDeep2022,
  title = {Goal-{{Conditioned Generators}} of {{Deep Policies}}},
  author = {Faccio, Francesco and Herrmann, Vincent and Ramesh, Aditya and Kirsch, Louis and Schmidhuber, Jürgen},
  date = {2022-07-04},
  number = {arXiv:2207.01570},
  eprint = {2207.01570},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.01570},
  urldate = {2022-08-04},
  abstract = {Goal-conditioned Reinforcement Learning (RL) aims at learning optimal policies, given goals encoded in special command inputs. Here we study goal-conditioned neural nets (NNs) that learn to generate deep NN policies in form of context-specific weight matrices, similar to Fast Weight Programmers and other methods from the 1990s. Using context commands of the form "generate a policy that achieves a desired expected return," our NN generators combine powerful exploration of parameter space with generalization across commands to iteratively find better and better policies. A form of weight-sharing HyperNetworks and policy embeddings scales our method to generate deep NNs. Experiments show how a single learned policy generator can produce policies that achieve any return seen during training. Finally, we evaluate our algorithm on a set of continuous control tasks where it exhibits competitive performance. Our code is public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/928TQQPM/Faccio et al. - 2022 - Goal-Conditioned Generators of Deep Policies.pdf;/Users/alexandre.piche/Zotero/storage/KT4SWLTG/2207.html}
}

@misc{florenceImplicitBehavioralCloning2021,
  title = {Implicit {{Behavioral Cloning}}},
  author = {Florence, Pete and Lynch, Corey and Zeng, Andy and Ramirez, Oscar and Wahid, Ayzaan and Downs, Laura and Wong, Adrian and Lee, Johnny and Mordatch, Igor and Tompson, Jonathan},
  date = {2021-08-31},
  number = {arXiv:2109.00137},
  eprint = {2109.00137},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.00137},
  urldate = {2022-07-27},
  abstract = {We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models. We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. On robotic policy learning tasks we show that implicit behavioral cloning policies with energy-based models (EBM) often outperform common explicit (Mean Square Error, or Mixture Density) behavioral cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. We find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the D4RL benchmark suite, despite using no reward information. In the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/alexandre.piche/Zotero/storage/JU7737RJ/Florence et al. - 2021 - Implicit Behavioral Cloning.pdf;/Users/alexandre.piche/Zotero/storage/BTVFHVNR/2109.html}
}

@misc{ghoshOfflineRLPolicies2022,
  title = {Offline {{RL Policies Should}} Be {{Trained}} to Be {{Adaptive}}},
  author = {Ghosh, Dibya and Ajay, Anurag and Agrawal, Pulkit and Levine, Sergey},
  date = {2022-07-05},
  number = {arXiv:2207.02200},
  eprint = {2207.02200},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.02200},
  urldate = {2022-07-18},
  abstract = {Offline RL algorithms must account for the fact that the dataset they are provided may leave many facets of the environment unknown. The most common way to approach this challenge is to employ pessimistic or conservative methods, which avoid behaviors that are too dissimilar from those in the training dataset. However, relying exclusively on conservatism has drawbacks: performance is sensitive to the exact degree of conservatism, and conservative objectives can recover highly suboptimal policies. In this work, we propose that offline RL methods should instead be adaptive in the presence of uncertainty. We show that acting optimally in offline RL in a Bayesian sense involves solving an implicit POMDP. As a result, optimal policies for offline RL must be adaptive, depending not just on the current state but rather all the transitions seen so far during evaluation.We present a model-free algorithm for approximating this optimal adaptive policy, and demonstrate the efficacy of learning such adaptive policies in offline RL benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/F6THD3SU/Ghosh et al. - 2022 - Offline RL Policies Should be Trained to be Adapti.pdf;/Users/alexandre.piche/Zotero/storage/VNFGDEJI/2207.html}
}

@misc{ghoshWhyGeneralizationRL2021,
  title = {Why {{Generalization}} in {{RL}} Is {{Difficult}}: {{Epistemic POMDPs}} and {{Implicit Partial Observability}}},
  shorttitle = {Why {{Generalization}} in {{RL}} Is {{Difficult}}},
  author = {Ghosh, Dibya and Rahme, Jad and Kumar, Aviral and Zhang, Amy and Adams, Ryan P. and Levine, Sergey},
  date = {2021-07-13},
  number = {arXiv:2107.06277},
  eprint = {2107.06277},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2107.06277},
  urldate = {2022-07-18},
  abstract = {Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/D2PFZWNX/Ghosh et al. - 2021 - Why Generalization in RL is Difficult Epistemic P.pdf;/Users/alexandre.piche/Zotero/storage/KK2YXVJS/2107.html}
}

@misc{humphreysDatadrivenApproachLearning2022,
  title = {A Data-Driven Approach for Learning to Control Computers},
  author = {Humphreys, Peter C. and Raposo, David and Pohlen, Toby and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Goldin, Alex and Santoro, Adam and Lillicrap, Timothy},
  date = {2022-02-16},
  number = {arXiv:2202.08137},
  eprint = {2202.08137},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.08137},
  urldate = {2022-08-04},
  abstract = {It would be useful for machines to use computers as humans do so that they can aid us in everyday tasks. This is a setting in which there is also the potential to leverage large-scale expert demonstrations and human judgements of interactive behaviour, which are two ingredients that have driven much recent success in AI. Here we investigate the setting of computer control using keyboard and mouse, with goals specified via natural language. Instead of focusing on hand-designed curricula and specialized action spaces, we focus on developing a scalable method centered on reinforcement learning combined with behavioural priors informed by actual human-computer interactions. We achieve state-of-the-art and human-level mean performance across all tasks within the MiniWob++ benchmark, a challenging suite of computer control problems, and find strong evidence of cross-task transfer. These results demonstrate the usefulness of a unified human-agent interface when training machines to use computers. Altogether our results suggest a formula for achieving competency beyond MiniWob++ and towards controlling computers, in general, as a human would.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/KSW5XAE3/Humphreys et al. - 2022 - A data-driven approach for learning to control com.pdf;/Users/alexandre.piche/Zotero/storage/ALBT9E39/2202.html}
}

@misc{irieDualFormNeural2022,
  title = {The {{Dual Form}} of {{Neural Networks Revisited}}: {{Connecting Test Time Predictions}} to {{Training Patterns}} via {{Spotlights}} of {{Attention}}},
  shorttitle = {The {{Dual Form}} of {{Neural Networks Revisited}}},
  author = {Irie, Kazuki and Csordás, Róbert and Schmidhuber, Jürgen},
  date = {2022-06-17},
  number = {arXiv:2202.05798},
  eprint = {2202.05798},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.05798},
  urldate = {2022-08-04},
  abstract = {Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/R74F5Q46/Irie et al. - 2022 - The Dual Form of Neural Networks Revisited Connec.pdf;/Users/alexandre.piche/Zotero/storage/46YM9XJR/2202.html}
}

@misc{jainDataBasedPerspectiveTransfer2022,
  title = {A {{Data-Based Perspective}} on {{Transfer Learning}}},
  author = {Jain, Saachi and Salman, Hadi and Khaddaj, Alaa and Wong, Eric and Park, Sung Min and Madry, Aleksander},
  date = {2022-07-12},
  number = {arXiv:2207.05739},
  eprint = {2207.05739},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.05739},
  urldate = {2022-07-19},
  abstract = {It is commonly believed that in transfer learning including more pre-training data translates into better performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we take a closer look at the role of the source dataset's composition in transfer learning and present a framework for probing its impact on downstream performance. Our framework gives rise to new capabilities such as pinpointing transfer learning brittleness as well as detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset. In particular, we demonstrate that removing detrimental datapoints identified by our framework improves transfer learning performance from ImageNet on a variety of target tasks. Code is available at https://github.com/MadryLab/data-transfer},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/MYKZ2YS9/Jain et al. - 2022 - A Data-Based Perspective on Transfer Learning.pdf;/Users/alexandre.piche/Zotero/storage/CSAETLVK/2207.html}
}

@misc{kaddourCausalMachineLearning2022,
  title = {Causal {{Machine Learning}}: {{A Survey}} and {{Open Problems}}},
  shorttitle = {Causal {{Machine Learning}}},
  author = {Kaddour, Jean and Lynch, Aengus and Liu, Qi and Kusner, Matt J. and Silva, Ricardo},
  date = {2022-07-21},
  number = {arXiv:2206.15475},
  eprint = {2206.15475},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.15475},
  urldate = {2022-07-28},
  abstract = {Causal Machine Learning (CausalML) is an umbrella term for machine learning methods that formalize the data-generation process as a structural causal model (SCM). This perspective enables us to reason about the effects of changes to this process (interventions) and what would have happened in hindsight (counterfactuals). We categorize work in CausalML into five groups according to the problems they address: (1) causal supervised learning, (2) causal generative modeling, (3) causal explanations, (4) causal fairness, and (5) causal reinforcement learning. We systematically compare the methods in each category and point out open problems. Further, we review data-modality-specific applications in computer vision, natural language processing, and graph representation learning. Finally, we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field, including recommendations for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Methodology},
  file = {/Users/alexandre.piche/Zotero/storage/GX5VSE2M/Kaddour et al. - 2022 - Causal Machine Learning A Survey and Open Problem.pdf;/Users/alexandre.piche/Zotero/storage/G6U8WBUM/2206.html}
}

@misc{krishnamoorthyGenerativePretrainingBlackBox2022,
  title = {Generative {{Pretraining}} for {{Black-Box Optimization}}},
  author = {Krishnamoorthy, Siddarth and Mashkaria, Satvik Mehul and Grover, Aditya},
  date = {2022-06-21},
  number = {arXiv:2206.10786},
  eprint = {2206.10786},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.10786},
  urldate = {2022-07-28},
  abstract = {Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose Black-box Optimization Transformer (BOOMER), a generative framework for pretraining black-box optimizers using offline datasets. In BOOMER, we train an autoregressive model to imitate trajectory runs of implicit black-box function optimizers. Since these trajectories are unavailable by default, we develop a simple randomized heuristic to synthesize trajectories by sorting random points from offline data. We show theoretically that this heuristic induces trajectories that mimic transitions from diverse low-fidelity (exploration) to high-fidelity (exploitation) samples. Further, we introduce mechanisms to control the rate at which a trajectory transitions from exploration to exploitation, and use it to generalize outside the offline data at test-time. Empirically, we instantiate BOOMER using a casually masked Transformer and evaluate it on Design-Bench, where we rank the best on average, outperforming state-of-the-art baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/8CIHAMTP/Krishnamoorthy et al. - 2022 - Generative Pretraining for Black-Box Optimization.pdf;/Users/alexandre.piche/Zotero/storage/YIMPNL75/2206.html}
}

@misc{krishnaRankGenImprovingText2022,
  title = {{{RankGen}}: {{Improving Text Generation}} with {{Large Ranking Models}}},
  shorttitle = {{{RankGen}}},
  author = {Krishna, Kalpesh and Chang, Yapei and Wieting, John and Iyyer, Mohit},
  date = {2022-05-19},
  number = {arXiv:2205.09726},
  eprint = {2205.09726},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.09726},
  urldate = {2022-08-03},
  abstract = {Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues, we present RankGen, an encoder model (1.2B parameters) that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and, which discourage topically-similar but irrelevant generations; (2) sequences generated from a large language model conditioned on the prefix, which discourage repetition and hallucination. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5\% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We open source our model checkpoints, code, and human preferences with detailed explanations for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{krishnaRankGenImprovingText2022a,
  title = {{{RankGen}}: {{Improving Text Generation}} with {{Large Ranking Models}}},
  shorttitle = {{{RankGen}}},
  author = {Krishna, Kalpesh and Chang, Yapei and Wieting, John and Iyyer, Mohit},
  date = {2022-05-19},
  number = {arXiv:2205.09726},
  eprint = {2205.09726},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.09726},
  urldate = {2022-08-03},
  abstract = {Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues, we present RankGen, an encoder model (1.2B parameters) that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and, which discourage topically-similar but irrelevant generations; (2) sequences generated from a large language model conditioned on the prefix, which discourage repetition and hallucination. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5\% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We open source our model checkpoints, code, and human preferences with detailed explanations for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{leeBPrefBenchmarkingPreferenceBased2021,
  title = {B-{{Pref}}: {{Benchmarking Preference-Based Reinforcement Learning}}},
  shorttitle = {B-{{Pref}}},
  author = {Lee, Kimin and Smith, Laura and Dragan, Anca and Abbeel, Pieter},
  date = {2021-11-04},
  number = {arXiv:2111.03026},
  eprint = {2111.03026},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.03026},
  urldate = {2022-07-30},
  abstract = {Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/FLVL9EF3/Lee et al. - 2021 - B-Pref Benchmarking Preference-Based Reinforcemen.pdf;/Users/alexandre.piche/Zotero/storage/98JYUWUM/2111.html}
}

@inproceedings{leeGeneralizedLeverageScore2020,
  title = {Generalized {{Leverage Score Sampling}} for {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Jason D and Shen, Ruoqi and Song, Zhao and Wang, Mengdi and Yu, zheng},
  date = {2020},
  volume = {33},
  pages = {10775--10787},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/7a22c0c0a4515485e31f95fd372050c9-Abstract.html},
  urldate = {2022-08-04},
  abstract = {Leverage score sampling is a powerful technique that originates from theoretical computer science, which can be used to speed up a large number of fundamental questions, e.g. linear regression, linear programming, semi-definite programming, cutting plane method, graph sparsification, maximum matching and max-flow. Recently, it has been shown that leverage score sampling helps to accelerate kernel methods [Avron, Kapralov, Musco, Musco, Velingker and Zandieh 17]. In this work, we generalize the results in [Avron, Kapralov, Musco, Musco, Velingker and Zandieh 17] to a broader class of kernels. We further bring the leverage score sampling into the field of deep learning theory.  1. We show the connection between the initialization for neural network training and approximating the neural tangent kernel with random features. 2. We prove the equivalence between regularized neural network and neural tangent kernel ridge regression under the initialization of both classical random Gaussian and leverage score sampling.},
  file = {/Users/alexandre.piche/Zotero/storage/5JNFFGXQ/Lee et al. - 2020 - Generalized Leverage Score Sampling for Neural Net.pdf}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  date = {2022-06-01},
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.09332},
  url = {http://arxiv.org/abs/2112.09332},
  urldate = {2022-07-29},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/SGMGHV95/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf;/Users/alexandre.piche/Zotero/storage/UIRW4SXM/2112.html}
}

@misc{ortegaShakingFoundationsDelusions2021,
  title = {Shaking the Foundations: Delusions in Sequence Models for Interaction and Control},
  shorttitle = {Shaking the Foundations},
  author = {Ortega, Pedro A. and Kunesch, Markus and Delétang, Grégoire and Genewein, Tim and Grau-Moya, Jordi and Veness, Joel and Buchli, Jonas and Degrave, Jonas and Piot, Bilal and Perolat, Julien and Everitt, Tom and Tallec, Corentin and Parisotto, Emilio and Erez, Tom and Chen, Yutian and Reed, Scott and Hutter, Marcus and de Freitas, Nando and Legg, Shane},
  options = {useprefix=true},
  date = {2021-10-20},
  number = {arXiv:2110.10819},
  eprint = {2110.10819},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.10819},
  urldate = {2022-08-04},
  abstract = {The recent phenomenal success of language models has reinvigorated machine learning research, and large sequence models such as transformers are being applied to a variety of domains. One important problem class that has remained relatively elusive however is purposeful adaptive behavior. Currently there is a common perception that sequence models "lack the understanding of the cause and effect of their actions" leading them to draw incorrect inferences due to auto-suggestive delusions. In this report we explain where this mismatch originates, and show that it can be resolved by treating actions as causal interventions. Finally, we show that in supervised learning, one can teach a system to condition or intervene on data by training with factual and counterfactual error signals respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/H99U9UUG/Ortega et al. - 2021 - Shaking the foundations delusions in sequence mod.pdf;/Users/alexandre.piche/Zotero/storage/MUPLS54T/2110.html}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2022-07-29},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/F9QRE2RV/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf;/Users/alexandre.piche/Zotero/storage/K9W3XWSS/2203.html}
}

@misc{reddyLearningHumanObjectives2021,
  title = {Learning {{Human Objectives}} by {{Evaluating Hypothetical Behavior}}},
  author = {Reddy, Siddharth and Dragan, Anca D. and Levine, Sergey and Legg, Shane and Leike, Jan},
  date = {2021-03-24},
  number = {arXiv:1912.05652},
  eprint = {1912.05652},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1912.05652},
  urldate = {2022-07-27},
  abstract = {We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/JYRC6KQ5/Reddy et al. - 2021 - Learning Human Objectives by Evaluating Hypothetic.pdf;/Users/alexandre.piche/Zotero/storage/7ZJNH8BT/1912.html}
}

@article{rogersPrimerBERTologyWhat2020,
  title = {A {{Primer}} in {{BERTology}}: {{What We Know About How BERT Works}}},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020-12},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00349},
  url = {https://direct.mit.edu/tacl/article/96482},
  urldate = {2022-07-18},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
  langid = {english},
  file = {/Users/alexandre.piche/Zotero/storage/E7XINEWA/Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT.pdf}
}

@misc{shahRetrospective2021BASALT2022,
  title = {Retrospective on the 2021 {{BASALT Competition}} on {{Learning}} from {{Human Feedback}}},
  author = {Shah, Rohin and Wang, Steven H. and Wild, Cody and Milani, Stephanie and Kanervisto, Anssi and Goecks, Vinicius G. and Waytowich, Nicholas and Watkins-Valls, David and Prakash, Bharat and Mills, Edmund and Garg, Divyansh and Fries, Alexander and Souly, Alexandra and Shern, Chan Jun and del Castillo, Daniel and Lieberum, Tom},
  options = {useprefix=true},
  date = {2022-04-14},
  number = {arXiv:2204.07123},
  eprint = {2204.07123},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.07123},
  urldate = {2022-07-30},
  abstract = {We held the first-ever MineRL Benchmark for Agents that Solve Almost-Lifelike Tasks (MineRL BASALT) Competition at the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021). The goal of the competition was to promote research towards agents that use learning from human feedback (LfHF) techniques to solve open-world tasks. Rather than mandating the use of LfHF techniques, we described four tasks in natural language to be accomplished in the video game Minecraft, and allowed participants to use any approach they wanted to build agents that could accomplish the tasks. Teams developed a diverse range of LfHF algorithms across a variety of possible human feedback types. The three winning teams implemented significantly different approaches while achieving similar performance. Interestingly, their approaches performed well on different tasks, validating our choice of tasks to include in the competition. While the outcomes validated the design of our competition, we did not get as many participants and submissions as our sister competition, MineRL Diamond. We speculate about the causes of this problem and suggest improvements for future iterations of the competition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/alexandre.piche/Zotero/storage/YS29B8CG/Shah et al. - 2022 - Retrospective on the 2021 BASALT Competition on Le.pdf;/Users/alexandre.piche/Zotero/storage/2Q7EKAUE/2204.html}
}

@misc{shinOfflinePreferenceBasedApprenticeship2022,
  title = {Offline {{Preference-Based Apprenticeship Learning}}},
  author = {Shin, Daniel and Brown, Daniel S. and Dragan, Anca D.},
  date = {2022-02-16},
  number = {arXiv:2107.09251},
  eprint = {2107.09251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2107.09251},
  urldate = {2022-07-29},
  abstract = {Learning a reward function from human preferences is challenging as it typically requires having a high-fidelity simulator or using expensive and potentially unsafe actual physical rollouts in the environment. However, in many tasks the agent might have access to offline data from related tasks in the same target environment. While offline data is increasingly being used to aid policy optimization via offline RL, our observation is that it can be a surprisingly rich source of information for preference learning as well. We propose an approach that uses an offline dataset to craft preference queries via pool-based active learning, learns a distribution over reward functions, and optimizes a corresponding policy via offline RL. Crucially, our proposed approach does not require actual physical rollouts or an accurate simulator for either the reward learning or policy optimization steps. To test our approach, we identify a subset of existing offline RL benchmarks that are well suited for offline reward learning and also propose new offline apprenticeship learning benchmarks which allow for more open-ended behaviors. Our empirical results suggest that combining offline RL with learned human preferences can enable an agent to learn to perform novel tasks that were not explicitly shown in the offline data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/II27864N/Shin et al. - 2022 - Offline Preference-Based Apprenticeship Learning.pdf;/Users/alexandre.piche/Zotero/storage/FBYAZ4NT/2107.html}
}

@misc{strayBuildingHumanValues2022,
  title = {Building {{Human Values}} into {{Recommender Systems}}: {{An Interdisciplinary Synthesis}}},
  shorttitle = {Building {{Human Values}} into {{Recommender Systems}}},
  author = {Stray, Jonathan and Halevy, Alon and Assar, Parisa and Hadfield-Menell, Dylan and Boutilier, Craig and Ashar, Amar and Beattie, Lex and Ekstrand, Michael and Leibowicz, Claire and Sehat, Connie Moon and Johansen, Sara and Kerlin, Lianne and Vickrey, David and Singh, Spandana and Vrijenhoek, Sanne and Zhang, Amy and Andrus, McKane and Helberger, Natali and Proutskova, Polina and Mitra, Tanushree and Vasan, Nina},
  date = {2022-07-20},
  number = {arXiv:2207.10192},
  eprint = {2207.10192},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.10192},
  urldate = {2022-08-03},
  abstract = {Recommender systems are the algorithms which select, filter, and personalize content across many of the worlds largest platforms and apps. As such, their positive and negative effects on individuals and on societies have been extensively theorized and studied. Our overarching question is how to ensure that recommender systems enact the values of the individuals and societies that they serve. Addressing this question in a principled fashion requires technical knowledge of recommender design and operation, and also critically depends on insights from diverse fields including social science, ethics, economics, psychology, policy and law. This paper is a multidisciplinary effort to synthesize theory and practice from different perspectives, with the goal of providing a shared language, articulating current design approaches, and identifying open problems. It is not a comprehensive survey of this large space, but a set of highlights identified by our diverse author cohort. We collect a set of values that seem most relevant to recommender systems operating across different domains, then examine them from the perspectives of current industry practice, measurement, product design, and policy approaches. Important open problems include multi-stakeholder processes for defining values and resolving trade-offs, better values-driven measurements, recommender controls that people use, non-behavioral algorithmic feedback, optimization for long-term outcomes, causal inference of recommender effects, academic-industry research collaborations, and interdisciplinary policy-making.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Social and Information Networks,H.3.3,J.4,K.4.2},
  file = {/Users/alexandre.piche/Zotero/storage/HYKIFDG4/Stray et al. - 2022 - Building Human Values into Recommender Systems An.pdf;/Users/alexandre.piche/Zotero/storage/56ANKD8D/2207.html}
}

@misc{strouseCollaboratingHumansHuman2022,
  title = {Collaborating with {{Humans}} without {{Human Data}}},
  author = {Strouse, D. J. and McKee, Kevin R. and Botvinick, Matt and Hughes, Edward and Everett, Richard},
  date = {2022-01-07},
  number = {arXiv:2110.08176},
  eprint = {2110.08176},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.08176},
  urldate = {2022-08-03},
  abstract = {Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train "human-aware" agents ("behavioral cloning play", or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/alexandre.piche/Zotero/storage/7TDSKF3I/Strouse et al. - 2022 - Collaborating with Humans without Human Data.pdf;/Users/alexandre.piche/Zotero/storage/XHLAD7NV/2110.html}
}

@misc{zengSocraticModelsComposing2022,
  title = {Socratic {{Models}}: {{Composing Zero-Shot Multimodal Reasoning}} with {{Language}}},
  shorttitle = {Socratic {{Models}}},
  author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  date = {2022-05-27},
  number = {arXiv:2204.00598},
  eprint = {2204.00598},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.00598},
  urldate = {2022-08-05},
  abstract = {Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/alexandre.piche/Zotero/storage/DC34S9RK/Zeng et al. - 2022 - Socratic Models Composing Zero-Shot Multimodal Re.pdf;/Users/alexandre.piche/Zotero/storage/XFVU4ZD3/2204.html}
}


